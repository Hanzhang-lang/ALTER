{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN one sample in a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_manager import get_k_shot_with_answer, view_instruction, row_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置全局变量\n",
    "这里会设置任务名和split\n",
    "tabfact：事实判断数据集，这里使用small_test=True，选取测试集合中的2000+条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.01)\n",
    "schema_information = pd.read_csv(f\"result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979 world figure skating championships\n",
      "2-11312764-4.html.csv\n",
      "linda fratianne finish in first place , but anett pötzsch do not manage to get in the top 3\n",
      "+--------+-----------------------------+----------------+-----------+----------+----------+\n",
      "|   rank | name                        | nation         |   sp_+_fs |   points |   places |\n",
      "|--------+-----------------------------+----------------+-----------+----------+----------|\n",
      "|      1 | linda fratianne             | united states  |         1 |   186.92 |       11 |\n",
      "|      2 | anett pötzsch               | east germany   |         3 |   184.36 |       18 |\n",
      "|      3 | emi watanabe                | japan          |         4 |   180.52 |       31 |\n",
      "|      4 | dagmar lurz                 | west germany   |         6 |   179.96 |       33 |\n",
      "|      5 | denise biellmann            | switzerland    |         2 |   177.28 |       49 |\n",
      "|      6 | lisa - marie allen          | united states  |         5 |   176.68 |       54 |\n",
      "|      7 | claudia kristofics - binder | austria        |         7 |   175.44 |       63 |\n",
      "|      8 | susanna driano              | italy          |         9 |   173.46 |       70 |\n",
      "|      9 | carola weißenberg           | east germany   |        11 |   170.54 |       88 |\n",
      "|     10 | kristiina wegelius          | finland        |        15 |   169.26 |       98 |\n",
      "|     11 | carrie rugh                 | united states  |        10 |   169.34 |       97 |\n",
      "|     12 | sanda dubravčić             | yugoslavia     |         8 |   166.96 |      115 |\n",
      "|     13 | natalia strelkova           | soviet union   |        16 |   164.94 |      134 |\n",
      "|     14 | deborah cottrill            | united kingdom |        20 |   164.8  |      136 |\n",
      "|     15 | karin riediger              | west germany   |        17 |   164.5  |      142 |\n",
      "|     16 | renata baierova             | czechoslovakia |        13 |   164    |      144 |\n",
      "|     17 | petra ernert                | west germany   |        14 |   163.24 |      149 |\n",
      "|     18 | kira ivanova                | soviet union   |        12 |   164.02 |      147 |\n",
      "|     19 | janet morrissey             | canada         |        18 |   162.04 |      162 |\n",
      "|     20 | reiko kobayashi             | japan          |        21 |   161.3  |      170 |\n",
      "|     21 | jeanne chapman              | norway         |        19 |   161.8  |      166 |\n",
      "|     22 | anita siegfried             | switzerland    |        26 |   150.34 |      207 |\n",
      "|     23 | astrid jansen in de wal     | netherlands    |        25 |   149.18 |      216 |\n",
      "|     24 | franca bianconi             | italy          |        22 |   149.04 |      218 |\n",
      "|     25 | bodil olsson                | sweden         |        23 |   147.02 |      225 |\n",
      "|     26 | corine wyrsch               | switzerland    |        27 |   146.76 |      233 |\n",
      "|     27 | kim myo sil                 | north korea    |        24 |   145.48 |      237 |\n",
      "|     28 | belinda coulthard           | australia      |        28 |   145.92 |      238 |\n",
      "|     29 | katie symmonds              | new zealand    |        29 |   134.58 |      261 |\n",
      "|     30 | shin hae sook               | south korea    |        30 |   120.44 |      270 |\n",
      "|     31 | gloria mas                  | spain          |        31 |   112.28 |      279 |\n",
      "+--------+-----------------------------+----------------+-----------+----------+----------+\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from executor import SQLManager\n",
    "from sqlalchemy import create_engine\n",
    "from data_loader import TableLoader, TableFormat\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine)\n",
    "#选取数据集对应的index\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "i = 66\n",
    "\n",
    "def show_table(data, execute=False):\n",
    "    formatter = TableFormat(format='none', data=data)\n",
    "    print(data['table']['id'])\n",
    "    print(data['statement'])\n",
    "    print(formatter.format_psql())\n",
    "    # print(preds[i])\n",
    "    # print(SQLs[i])\n",
    "    # test_df = manager.execute_from_df(SQLs[i], formatter.all_data)\n",
    "    # print(test_df)\n",
    "    print(data['label'])\n",
    "print(table_loader.dataset[i]['table']['caption'])\n",
    "show_table(table_loader.dataset[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-back & decompose prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose prompt\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\n",
    "    # \"after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\",\n",
    "                  \"all 12 club play a total of 22 game for the wru division one east\",\n",
    "                #   \"a gamecube game loss the award in each of the first 3 year\",\n",
    "                \"from 1980 to 2011 , apoel bc lose more than 2 time as many game as it win\",\n",
    "                  \"polona hercog 1890partner with alberta brianti after she have stephanie vogt as the partner\",\n",
    "                  ]\n",
    "task_examples = [\"query rewrite\", \"query decompose\", \"query ambiguity resolve\"]\n",
    "new_query_examples = [\n",
    "    # \"Who were the winners of the lifetime achievement award after 2005?;\",\n",
    "                      \"How many clubs play for the wru division one east in total?; How many clubs play 22 game for the wru division one east?;\",\n",
    "                    #   \"a gamecube game loss the award in each of the first 3 year\",\n",
    "                    \"from 1980 to 2011 , how many games did apoel bc lose?; from 1980 to 2011 , how many games did apoel bc win?;\",\n",
    "                      \"When did polona hercog partner with alberta brianti?; When did polona hercog partner with stephanie vogt?\",\n",
    "                      ]\n",
    "num_k = 3\n",
    "inds = [1, 124, 5]\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "examples = [TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_html(table_loader.dataset[inds[i]]['table']['caption']) for i in range(num_k)]\n",
    "\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"table\", \"new_query\"], template=\n",
    "\"\"\"Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"table\": examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "decompose_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are capable of converting complex query into sub-queries. Based on the table, provide at most 2 continuity sub-queries for knowledge that you need. \n",
    "Split the queries with ’;’.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: \n",
    "    \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n",
    "\n",
    "# Sub-questions are separated by semicolons.\n",
    "# answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "#                                     template=\"\"\"\n",
    "# Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "# SQL Excuted: \n",
    "# ```{SQL}```\n",
    "# Sub-table: {table}\n",
    "# Query: {claim}\n",
    "# answer the question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# \"\"\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step-back prompt\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [8, 15]\n",
    "num_k = 2\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "examples = [TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_html(table_loader.dataset[inds[i]]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\n",
    "    # \"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "    \"which college list be public?\",\n",
    "    \"which game be all score over time?\"\n",
    "    ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": table_loader.dataset[inds[i]]['statement'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "step_back_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Based on the table, your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Query: {query}\n",
    "Sub-Table: {table}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单测试prompt的脚本，可以不运行这个block\n",
    "\n",
    "# table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "# i = 90\n",
    "# sample = table_loader.normalize_table(table_loader.dataset[i])\n",
    "# all_queries = []\n",
    "# formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "# llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final inference prompt（deprecated）\n",
    "\n",
    "\n",
    "# def get_k_shot_with_answer(k: int=1):\n",
    "#     sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"]\n",
    "#     thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. Therefore, the claim that 2 is the fewest points they received is false. The output should be 0.\"]\n",
    "#     tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "#     claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "#     # inds from test split\n",
    "#     examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], template=\n",
    "#     \"\"\"\n",
    "#     SQL Excuted: \n",
    "#     ```{SQL}```\n",
    "#     Sub-table: {table}\n",
    "#     ...\n",
    "\n",
    "#     Query: {claim}\n",
    "#     Thought: {thought}\n",
    "#     Output: {output}\n",
    "#     \"\"\")\n",
    "#     examples_dict = dict(zip([\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], claims[0], thoughts[0], '0']))\n",
    "#     prompt_template = FewShotPromptTemplate(\n",
    "#         examples=[examples_dict],\n",
    "#         example_prompt=examples_prompt,\n",
    "#         prefix=\"\"\"Below are some sub-tables generated by excuting the SQL. You need to understand the logic behind the SQL filtering. Complete task using all information below. Please think step by step and return 0 or 1 without any other information at last.\"\"\",\n",
    "#         suffix=\n",
    "#         \"\"\"\n",
    "#     {information}\n",
    "#     Query: {query}\n",
    "#     Thought: \"\"\",\n",
    "#         input_variables=[\"table\", \"query\"],\n",
    "# )\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column sampling prompt\n",
    "\n",
    "def get_k_shot_with_aug(k: int=2):\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "\n",
    "    inds = [3, 6, 260, 33]\n",
    "    Output_examples = [\n",
    "                       'team, goals_for',\n",
    "                       'year, game, platform_s',\n",
    "                       'name, population_density_km_2_, population_2011_census_'\n",
    "                       'leading_scorer, score, date']\n",
    "    linking_examples = ['the team -> team; the most goal for -> goals_for',\n",
    "                        'gamecube -> platform_s; gamecube game -> game; the first 3 year -> year;',\n",
    "                        'alberta -> name; population density -> population_density_km_2_; 4257744 less people -> population_2011_census_; 2011 -> population_2011_census_'\n",
    "                        'jason richardson -> leading_scorer; leading scorer -> score; month -> date; 23 point per game -> leading_scorer'\n",
    "    ]\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\n",
    "    Column linking: {linking}\n",
    "    Columns: {output}\"\"\")\n",
    "    num_k = 3\n",
    "    examples_dict = [{\"table\": TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_nl_sep(table_loader.dataset[inds[i]]['table']['caption']),\n",
    "                                        \"claim\": table_loader.dataset[inds[i]]['statement'],\n",
    "                                        \"linking\": linking_examples[i],\n",
    "                                        # \"summary\": summary_examples[i],\n",
    "                                        \"output\": Output_examples[i]} for i in range(num_k)]\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "    Your task is accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the table.\n",
    "    Approach this task as follows，read the claim thoroughly and list every possible link from query term to column in Table. Then Based on the column linking, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\"\"\",\n",
    "    # You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "    # Given the following table and query, you should output columns related to the query or contain useful information about the query. \n",
    "    # Here are some examples:\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\n",
    "    \"\"\",\n",
    "        input_variables=[\"table\", \"claim\"],\n",
    ")\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义两个函数：\n",
    "\n",
    "**scene_A**: 执行column sampling, SQL生成得到子表，最终返回sql和子表，不回答问题\n",
    "\n",
    "\n",
    "**scene_B**: 执行column sampling, SQL生成执行并直接返回问题回答的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "\n",
    "def scene_A(query, sample, verbose=True):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"\n",
    "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "Extra information: {aug}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        # stage 2: SQL generation\n",
    "        \n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        try: \n",
    "            formatter.data = formatter.data.loc[:, columns]\n",
    "            formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "        except:\n",
    "            pass\n",
    "        extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + extra_information\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "    print(\"total_tokens:\", cb.total_tokens)\n",
    "    print(stage_2_batch_pred)\n",
    "    # stage 3: SQL Excution\n",
    "    try: \n",
    "        formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "    except:\n",
    "        formatter.data = formatter.all_data\n",
    "        stage_2_batch_pred = 'SELECT * from DF;'\n",
    "    if len(formatter.data) == 0:\n",
    "        return query, stage_2_batch_pred, 'No data from database', cb.total_tokens\n",
    "    return query, stage_2_batch_pred, formatter.format_html(), cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: \n",
    "{table}\n",
    "Query: {claim}\n",
    "Please provide a clear, concise statement in response to the question. If you cannot answer the question based on the sub-table, just say 'Cannot get answer from sub-table'\n",
    "\"\"\" )\n",
    "def scene_B(query, sample, verbose=False):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"\n",
    "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "Extra information: {aug}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            # 'aug':  summary_aug + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "        \n",
    "        try: \n",
    "            formatter.data = formatter.data.loc[:, columns]\n",
    "            formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "        except:\n",
    "            pass\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + extra_information\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "    \n",
    "        \n",
    "        print(stage_2_batch_pred)\n",
    "        # stage 3: SQL Excution\n",
    "        try: \n",
    "            formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "        except:\n",
    "            formatter.data = formatter.all_data\n",
    "            stage_2_batch_pred = 'SELECT * from DF;'\n",
    "        llm_chain = LLMChain(llm=model, prompt=answer_instruction, verbose=verbose)\n",
    "        response = llm_chain.batch([dict({'table': formatter.format_html(),\n",
    "                                                'claim': query,\n",
    "                                                'SQL':  stage_2_batch_pred\n",
    "                                                })], return_only_outputs=True)[0]['text']\n",
    "    print(\"total_tokens:\", cb.total_tokens)\n",
    "    return response, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "def save_csv(input_list: List[List], label_list: List, file_path):\n",
    "    import pandas as pd\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    assert len(input_list) == len(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(label_list)):\n",
    "        df[label_list[i]] = pd.Series(input_list[i])\n",
    "    if os.path.exists(file_path) and file_path.endswith('.csv'):\n",
    "        df_origin = pd.read_csv(file_path)\n",
    "        df = pd.concat([df_origin, df], axis=0)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_blury_string(pred_list):\n",
    "    pred_label = []\n",
    "    for pred in pred_list:\n",
    "        predict_ans = pred.split('\\n')[-1]\n",
    "        if '0' in predict_ans:\n",
    "            predict_ans = '0'\n",
    "        elif '1' in predict_ans:\n",
    "            predict_ans = '1'\n",
    "        else:\n",
    "            predict_ans = '2'\n",
    "        pred_label.append(predict_ans)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' who finish in first place and who do not manage to get in the top 3?', 'How did Linda Fratianne finish in the competition?', ' Did Anett Pötzsch manage to get in the top 3 in the competition?']\n",
      "Column linking: Linda Fratianne -> name; finish in the competition -> rank\n",
      "Columns: name, rank\n",
      "Column linking: first place -> rank 1; top 3 -> rank 1, rank 2, rank 3\n",
      "Columns: rank, name\n",
      "SELECT rank\n",
      "FROM DF\n",
      "WHERE name = 'Linda Fratianne';\n",
      "SELECT name\n",
      "FROM DF\n",
      "WHERE rank = 1 OR rank > 3;\n",
      "total_tokens: 1528\n",
      "Column linking: Anett Pötzsch -> name; top 3 -> rank\n",
      "Columns: name, rank\n",
      "total_tokens: 2102\n",
      "SELECT name, rank\n",
      "FROM DF\n",
      "WHERE name = 'Anett Pötzsch' AND rank <= 3;\n",
      "total_tokens: 1567\n",
      "Column linking: linda fratianne -> name; first place -> rank; anett pötzsch -> name; top 3 -> rank\n",
      "Columns: name, rank\n",
      "total_tokens: 1407\n",
      "SELECT name, rank\n",
      "FROM DF\n",
      "WHERE name = 'linda fratianne' OR name = 'anett pötzsch';\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
      "\n",
      "SQL Excuted: \n",
      "```SELECT name, rank\n",
      "FROM DF\n",
      "WHERE name = 'linda fratianne' OR name = 'anett pötzsch';```\n",
      "Sub-table:\n",
      "<table>\n",
      "<thead>\n",
      "<tr><th>           name</th><th>  rank</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>linda fratianne</td><td>1     </td></tr>\n",
      "<tr><td>anett pötzsch  </td><td>2     </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Extra information:\n",
      "Based on the sub-table provided, Linda Fratianne finished in first place, and the following skaters did not manage to get in the top 3: Lisa-Marie Allen, Susanna Driano, Carola Weißenberg, Carrie Rugh, Sanda Dubravčić, Deborah Cottrill, Renata Baierova, Petra Ernert, Janet Morrissey, Reiko Kobayashi, Jeanne Chapman, Anita Siegfried, Astrid Jansen in de Wal, Franca Bianconi, Bodil Olsson, Corine Wyrsch, Kim Myo Sil, Belinda Coulthard, Katie Symmonds, Shin Hae Sook, and Gloria Mas.\n",
      "Based on the sub-table generated by the SQL query, Linda Fratianne's rank in the competition is not provided. Therefore, we cannot determine how Linda Fratianne finished in the competition from the sub-table.\n",
      "Based on the sub-table, Anett Pötzsch did manage to get in the top 3 in the competition as her rank is less than or equal to 3.\n",
      "\n",
      "Query: linda fratianne finish in first place , but anett pötzsch do not manage to get in the top 3\n",
      "Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'text': \"Based on the information provided in the sub-table and the extra information:\\n\\n1. The sub-table shows that Linda Fratianne's rank is 1, indicating that she finished in first place.\\n2. Anett Pötzsch's rank is 2, which means she did not manage to get in the top 3.\\n\\nTherefore, the provided claim/query is true:\\n- Linda Fratianne finished in first place.\\n- Anett Pötzsch did not manage to get in the top 3.\\n\\nTherefore, the answer is 1.\"}\n",
      "ALL TOKENS 7598\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.01)\n",
    "save_path = f\"result/answer/tabfact_zh_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "# template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "template = \"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table:\n",
    "{table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {query}\n",
    "Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\"\"\" )\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "\n",
    "i = 66\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "all_tokens = 0\n",
    "labels.append(sample['label'])\n",
    "all_queries = []\n",
    "formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "with get_openai_callback() as cb:\n",
    "    llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=False)\n",
    "    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)\n",
    "    all_queries.append(batch_pred[0]['text'].split(':')[-1])\n",
    "    llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=False)\n",
    "    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)\n",
    "    all_queries.extend(batch_pred[0]['text'].split(';'))\n",
    "    print(all_queries)\n",
    "all_tokens += cb.total_tokens\n",
    "args_list = [{\"query\": q, \"sample\": sample} for q in all_queries]\n",
    "ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "results = [res[0] for res in ans_from_B if res[0] != 'Cannot get answer from sub-table']\n",
    "all_tokens += sum([res[1] for res in ans_from_B])\n",
    "#With answer\n",
    "with get_openai_callback() as cb:\n",
    "    imp_input = scene_A(sample['query'], sample, False)\n",
    "    llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "    batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "print(batch_pred[0])\n",
    "all_tokens += cb.total_tokens\n",
    "print('ALL TOKENS', all_tokens)\n",
    "outputs.append(batch_pred[0]['text'])\n",
    "# if (i + 1) % 10 == 0:\n",
    "#     save_csv([outputs, labels], ['preds', 'labels'], file_path=save_path)\n",
    "#     outputs = []\n",
    "#     labels = []\n",
    "#With no answer\n",
    "# temp = [f\"\"\"\n",
    "# SQL Excuted for extra information: \n",
    "# ```{res[1]}```\n",
    "# Sub-table for extra information: {res[2]}\"\"\" for res in results if res[2] != 'No data from database']\n",
    "# imp_input = scene_A(sample['query'], sample, False)\n",
    "# llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(temp)}], return_only_outputs=True)\n",
    "# print(batch_pred[0])\n",
    "# outputs.append(batch_pred[0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#前100条我测试的badcase， ind列表\n",
    "[1, 11, 14, 22, 26, 37, 38, 40, 47, 49, 50,  59, 63, 65, 66, 68, 82, 88, 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some sample filtereed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "46: west berkshire brewery 's maggs magnificent mild be its most decorate beer between 1995 and 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '：' (U+FF1A) (3998830562.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    20 ：Based on the information provided, the SQL query filtered for records where the place starts with 't9' and the country is 'united states'. The result of the query shows that there are 3 people who meet these criteria. However, the extra information states that there were actually 4 people who tied for ninth place, and all of them were from the United States.\\n\\nTherefore, the provided claim/query is false. The correct number of people from the United States who tied for ninth place is 4, not 3. \\n\\nFinal answer: 0\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '：' (U+FF1A)\n"
     ]
    }
   ],
   "source": [
    "20：Based on the information provided, the SQL query filtered for records where the place starts with 't9' and the country is 'united states'. The result of the query shows that there are 3 people who meet these criteria. However, the extra information states that there were actually 4 people who tied for ninth place, and all of them were from the United States.\\n\\nTherefore, the provided claim/query is false. The correct number of people from the United States who tied for ninth place is 4, not 3. \\n\\nFinal answer: 0\n",
    "88: To verify whether the term start for Bashkim Fino is after the term start for Vilson Ahmeti, we need to compare the dates mentioned in the sub-table. From the sub-table:\\n- Vilson Ahmeti's term started on December 10, 1991.\\n- Bashkim Fino's term started on March 11, 1997.\\n\\nSince December comes before March in the calendar year, it is evident that Vilson Ahmeti's term started before Bashkim Fino's term. Therefore, the claim that the term start for Bashkim Fino is after the term start for Vilson Ahmeti is FALSE.\\n\\nHence, the answer is 0\n",
    "72: To verify the claim that the gap between the first and last player being a total of 58.04 is true, we need to calculate the difference between the points of the first and last player based on the given information.\\n\\nGiven:\\n- Total gap between the first and last rank is 18.\\n- Rank of the first player is 1.\\n- Rank of the last player is 19.\\n\\nLet's calculate the points difference between the first and last player:\\npoints difference = (points of last player) - (points of first player)\\n\\nSince the total gap between the first and last rank is 18, we can calculate the points difference as follows:\\npoints difference = 18 * (MAX(points) - MIN(points))\\n\\nGiven that the calculated point gap from the sub-table is 58.04, we can substitute this value into the formula:\\n18 * 58.04 = 1044.72\\n\\nTherefore, the claim that the gap between the first and last player being a total of 58.04 is false.\\n\\nFinal answer: 0\n",
    "76: The SQL query filters for competitors from France with a rank less than 5. Since the sub-table generated from the query shows no data, it means that there were no competitors from France who finished better than 5th place. \\n\\nTherefore, the provided claim that France's competitors all finished better than 5th place is TRUE.\\n\\nFinal answer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_normalize(user_input, recognition_types=None):\n",
    "    \"\"\"A string normalizer which recognize and normalize value based on recognizers_suite\"\"\"\n",
    "    user_input = str(user_input)\n",
    "    user_input = user_input.replace(\"\\\\n\", \"; \")\n",
    "\n",
    "    def replace_by_idx_pairs(orig_str, strs_to_replace, idx_pairs):\n",
    "        assert len(strs_to_replace) == len(idx_pairs)\n",
    "        last_end = 0\n",
    "        to_concat = []\n",
    "        for idx_pair, str_to_replace in zip(idx_pairs, strs_to_replace):\n",
    "            to_concat.append(orig_str[last_end : idx_pair[0]])\n",
    "            to_concat.append(str_to_replace)\n",
    "            last_end = idx_pair[1]\n",
    "        to_concat.append(orig_str[last_end:])\n",
    "        return ''.join(to_concat)\n",
    "\n",
    "    if recognition_types is None:\n",
    "        recognition_types = [\n",
    "            \"datetime\",\n",
    "            \"number\",\n",
    "            \"ordinal\",\n",
    "            \"percentage\",\n",
    "            \"age\",\n",
    "            \"currency\",\n",
    "            \"dimension\",\n",
    "            \"temperature\",\n",
    "        ]\n",
    "    culture = Culture.English\n",
    "    for recognition_type in recognition_types:\n",
    "        if re.match(\"\\d+/\\d+\", user_input):\n",
    "            # avoid calculating str as 1991/92\n",
    "            continue\n",
    "        recognized_list = getattr(\n",
    "            recognizers_suite, \"recognize_{}\".format(recognition_type)\n",
    "        )(\n",
    "            user_input, culture\n",
    "        )  # may match multiple parts\n",
    "        strs_to_replace = []\n",
    "        idx_pairs = []\n",
    "        for recognized in recognized_list:\n",
    "            if not recognition_type == 'datetime':\n",
    "                recognized_value = recognized.resolution['value']\n",
    "                if str(recognized_value).startswith(\"P\"):\n",
    "                    # if the datetime is a period:\n",
    "                    continue\n",
    "                else:\n",
    "                    strs_to_replace.append(recognized_value)\n",
    "                    idx_pairs.append((recognized.start, recognized.end + 1))\n",
    "            else:\n",
    "                if recognized.resolution:  # in some cases, this variable could be none.\n",
    "                    if len(recognized.resolution['values']) == 1:\n",
    "                        strs_to_replace.append(\n",
    "                            recognized.resolution['values'][0]['timex']\n",
    "                        )  # We use timex as normalization\n",
    "                        idx_pairs.append((recognized.start, recognized.end + 1))\n",
    "\n",
    "        if len(strs_to_replace) > 0:\n",
    "            user_input = replace_by_idx_pairs(user_input, strs_to_replace, idx_pairs)\n",
    "\n",
    "    if re.match(\"(.*)-(.*)-(.*) 00:00:00\", user_input):\n",
    "        user_input = user_input[: -len(\"00:00:00\") - 1]\n",
    "        # '2008-04-13 00:00:00' -> '2008-04-13'\n",
    "    return user_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
