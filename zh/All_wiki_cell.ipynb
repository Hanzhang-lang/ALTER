{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/tabular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import parse_specific_composition, add_row_number, parse_specific_composition_zh\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name='BAAI/bge-large-en',\n",
    "            model_kwargs={'device': 'cuda:2', 'trust_remote_code': True},\n",
    "            encode_kwargs={'normalize_embeddings': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\n",
    "                  \"what was the time difference between the first place finisher and the eighth place finisher?\",\n",
    "                  \"other than william stuart price, which other businessman was born in tulsa?\",\n",
    "                  \"which canadian city had the most passengers traveling from manzanillo international airport in 2013?\"\n",
    "                  ]\n",
    "new_query_examples = [\n",
    "                      \"what was the time for the first place finisher?; what was the time for the eighth place finisher?\",\n",
    "                      \"was william stuart price born in tulsa?; who was born in tulsa?\",\n",
    "                      \"how many passengers do each airline from canadian city have?; which canadian city had the most passengers?\"\n",
    "                      ]\n",
    "num_k = 3\n",
    "inds = [1, 11, 86]\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True, embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"table\", \"new_query\"], template=\n",
    "\"\"\"Sub-Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"table\": examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "decompose_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are capable of converting complex query into sub-queries. Below is a sub-table with rows randomly sampled from the original table. Based on the sub-table, decompose the original query into 2-3 complete sub-queries that can solve the original query.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Sub-Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'wikitable'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "# model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-WZtqZEeuE0Xb6syVghDgAxdwe0ASWLkQRGxl61UI7B9RqNC4\", temperature=0.7).bind(logprobs=True)\n",
    "schema_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:////media/disk1/chatgpt/zh/tabular_data/db/sqlite/cell.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name=task_name, split='test', use_sample=False, small_test=False)\n",
    "sample = table_loader.normalize_table(table_loader.dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [5, 11, 46]\n",
    "num_k = 3\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True,embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\"was the Sandia Peak Tramway before or after the 3S Aerial Tramway in terms of Year_of_inauguration?\",\n",
    "                      \"other than William Stuart Price, which other businessman's was in Tulsa in terms of Hometown?\",\n",
    "                      \"How many players are G\\\\nF in terms of Position?\"]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"Sub-table: {table}\n",
    "Query: {query}\n",
    "New query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": table_loader.dataset[inds[i]]['question'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "disambiguous_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    # prefix=\"\"\"Based on the given table, your task is to rewrite the query to resolve ambiguity and ensure the question is consistent with the table. \n",
    "    # This requires pinpointing elements of the question to table contents and rewriting the question to ensure a consistent, clear interpretation. \"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Sub-table: {table}\n",
    "Query: {query}\n",
    "New query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [11, 182, 70]\n",
    "num_k = 2\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True,embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\n",
    "    # \"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "    \"which business man was born in tulsa?\",\n",
    "    \"what is the network owned by national polytechnic institute?\",\n",
    "    \"what districts are more populous than haridwar?\"\n",
    "    ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"Sub-table: {table}\n",
    "Query: {query}\n",
    "New query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": table_loader.dataset[inds[i]]['question'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "step_back_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Below is a sub-table with rows randomly sampled from the original table. Based on the sub-table, your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Sub-table: {table}\n",
    "Query: {query}\n",
    "New query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_answer(k: int=1):\n",
    "#     sqls = [\"SELECT COUNT(*) FROM DF WHERE Outcome = 'Runner-up' AND Opponent = 'Roger Federer';\"\n",
    "#             ]\n",
    "#     thoughts = [\"The SQL query filters the data to only include rows where the outcome is 'Runner-up' and the opponent is 'Roger Federer'. The sub-table shows that Roger Federer was a runner-up 2 times.\"]\n",
    "#     tables = [\"\"\" <table>\n",
    "# <thead>\n",
    "# <tr><th>  COUNT(*)</th></tr>\n",
    "# </thead>\n",
    "# <tbody>\n",
    "# <tr><td>2.0000    </td></tr>\n",
    "# </tbody>\n",
    "# </table>\"\"\"]\n",
    "#     tables_pipe = [\"\"\"/*\n",
    "# table caption : turkish cup\n",
    "# col : MAX(winners_c_from_previous_round)\n",
    "# row 1: 54\n",
    "# */\"\"\"]\n",
    "#     claims = [\"how many times was roger federer a runner-up?\"]\n",
    "        sqls = [\"SELECT DISTINCT Type FROM DF WHERE Type != 'audio';\"\n",
    "                ]\n",
    "        thoughts = [\"Based on the SQL query and the extra information provided, the types include audio or video. Therefore, other than audio, the payload type is video.\"]\n",
    "        tables = ['<table>\\n<thead>\\n<tr><th> Type </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>video   </td></tr>\\n<tr><td>audio/video   </td></tr>\\n</tbody>\\n</table>']\n",
    "        claims = [\"other than audio, what type of payload types are there?\"]\n",
    "        extras = [\"The payload types for audio include audio, video, and audio/video.\"]\n",
    "#     sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"\n",
    "#             ]\n",
    "#     thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. 3 is the fewest points they received. \"]\n",
    "#     tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "#     claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "    # inds from test split\n",
    "        examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"information\",  \"claim\", \"thought\", \"output\"], template=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {claim}\n",
    "Thought: {thought}\n",
    "Answer: {output}\n",
    "        \"\"\")\n",
    "        examples_dict = dict(zip([\"SQL\", \"table\", \"information\",  \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], extras[0], claims[0], thoughts[0], 'video']))\n",
    "        prompt_template = FewShotPromptTemplate(\n",
    "                examples=[examples_dict],\n",
    "                example_prompt=examples_prompt,\n",
    "                prefix=\"\"\"Below is a sub-table generated by executing the corresponding SQL. You need to understand the logic behind the SQL filtering. Think step by step and answer the question given in the query.\n",
    "You should output in the following format:\n",
    "Thought: your step by step thought\n",
    "Answer: Only return the concise string instead of other format information. Do not repeat the question.\n",
    "Below is an example.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {query}\"\"\",\n",
    "                input_variables=[\"table\", \"query\", \"SQL\", \"information\"],\n",
    "        )\n",
    "        return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_aug(k: int=2):\n",
    "    table_loader = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "    examples_dict = []\n",
    "    \n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>Hoot Kloot</caption>\\n<thead>\\n<tr><th> Number</th><th> Title</th><th> Directed_by_</th><th> Released_</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>1  </td><td>\"Kloot\\'s Kounty\"           </td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>2  </td><td>\"Apache on the County Seat\"</td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>6  </td><td>\"Mesa Trouble\"       </td><td>Sid Marcus </td><td>1974       </td></tr>\\n</tbody>\\n</table>',\n",
    "                                        \"claim\": table_loader.dataset[95]['question'],\n",
    "                                        \"aug\": \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year. \\n1. Number: The episode number in the series \\n2. Title: The title of the episode \\n3. Directed_by_: The director of the episode \\n4. Released_: The release year of the episode\",\n",
    "                                        \"linking\": \"the last title -> Released_, the last title-> Number, title -> Title, sid marcus -> Directed_by_\",\n",
    "                                        \"output\": \"Released_, Number, Title, Directed_by_\"}])\n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>1943–44 Chicago Black Hawks season</caption>\\n<thead>\\n<tr><th>  num</th><th>       Date</th><th>            Visitor</th><th>  Score</th><th>               Home</th><th>  Record</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>14   </td><td>December 5 </td><td>New York Rangers   </td><td>6–7    </td><td>Chicago Black Hawks</td><td>8–6–0   </td></tr>\\n<tr><td>40   </td><td>February 26</td><td>Chicago Black Hawks</td><td>3–2    </td><td>Toronto Maple Leafs</td><td>18–18–4 </td></tr>\\n<tr><td>31   </td><td>January 29 </td><td>Chicago Black Hawks</td><td>4–3    </td><td>Toronto Maple Leafs</td><td>14–16–1 </td></tr>\\n</tbody>\\n</table>',\n",
    "                                        \"claim\": 'what was the difference in score in the december 19th win?',\n",
    "                                        \"aug\": 'The table contains information about the 1943-44 Chicago Black Hawks season, including the date, visitor, score, home team, record, and points for each game. \\n1. num: The game number in the season \\n2. Date: The date of the game\\n3. Vistor: The visiting team\\n4. Score: The final score, with the visitor score listed first\\n5. Home: The home team\\n6. Record: The team win-loss-overtime loss record at the time of the game',\n",
    "                                        \"linking\": 'difference in score -> Score, december 19th -> Date',\n",
    "                                        \"output\": 'Date, Score'}])\n",
    "    \n",
    "    # \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year.\"\n",
    "    # \"№<The episode number in the series>\\nTitle<The title of the episode>\\nDirected_by_<The director of the episode>\\nReleased_<The release year of the episode>\"\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"aug\",\"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "Table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "Column linking: {linking}\n",
    "Columns: {output}\"\"\")\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "Based on the Table below, your task is to accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "Approach this task as follows:\n",
    "Read the query and extra information thoroughly and list every possible link from query term to column in the Table. \n",
    "Then Based on the column linking, output all useful columns at last. Make sure all columns in the linking step are included and every column is in the Table.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\"\"\",\n",
    "        input_variables=[\"table\", \"claim\", \"aug\"],\n",
    ")\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "# def get_k_shot_with_aug(k: int=2):\n",
    "#     table_loader = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "#     examples_dict = []\n",
    "#     examples_dict.extend([{\"table\": '<table>\\n<caption>Hoot Kloot</caption>\\n<thead>\\n<tr><th> Number</th><th> Title</th><th> Directed_by_</th><th> Released_</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>1  </td><td>\"Kloot\\'s Kounty\"           </td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>2  </td><td>\"Apache on the County Seat\"</td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>6  </td><td>\"Stirrups and Hiccups\"     </td><td>Gerry Chiniquy</td><td>1973       </td></tr>\\n</tbody>\\n</table>',\n",
    "#                                         \"claim\": table_loader.dataset[95]['question'],\n",
    "#                                         \"linking\": \"the last title -> Released_, the last title-> Number, title -> Title, sid marcus -> Directed_by_\",\n",
    "#                                         \"output\": \"Title, Released_, Number, Directed_by_\"}])\n",
    "#     examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "#     \"\"\"\n",
    "#     Table: {table}\n",
    "#     Query: {claim}\n",
    "#     Column linking: {linking}\n",
    "#     Columns: {output}\"\"\")\n",
    "#     prompt_template = FewShotPromptTemplate(\n",
    "#         examples=examples_dict,\n",
    "#         example_prompt=examples_prompt,\n",
    "#         prefix=\n",
    "#         \"\"\"\n",
    "#     Based on the Table below, your task is accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "#     Approach this task as follows:\n",
    "#     Read the query thoroughly and list every possible link from query term to column in the Table. \n",
    "#     Then Based on the column linking, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\"\"\",\n",
    "#     # You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "#     # Given the following table and query, you should output columns related to the query or contain useful information about the query. \n",
    "#     # Here are some examples:\"\"\",\n",
    "#         suffix=\n",
    "#         \"\"\"\n",
    "#     Table: {table}\n",
    "#     Extra information: {aug}\n",
    "    \n",
    "#     Query: {claim}\"\"\",\n",
    "#         input_variables=[\"table\", \"claim\", \"aug\"],\n",
    "# )\n",
    "#     return prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "def scene_A(query, sample, k =3, verbose=True):\n",
    "    # Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "    # Our ultimate goal is to answer query based on the original table. Now we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "#     row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "#                                  template=\"\"\"Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "# sub-table: {table}\n",
    "# Extra information: {aug}\n",
    "\n",
    "# Query: {claim}\n",
    "# SQL: \"\"\")\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                    template=\"\"\"Our ultimate goal is to answer the query based on the original table. Now we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table.  Based on the augmentation information, carefully analyze the query and write an SQLITE3 SELECT SQL statement using table DF that completes the query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    if k == 0:\n",
    "        sample_data = formatter.get_sample_data(sample_type='head', k=k)\n",
    "    else:\n",
    "        sample_data = formatter.get_sample_data(sample_type='embedding', query=query, k=k)\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['table']['id']]['summary'], aug_information.loc[sample['table']['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        \n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug +'\\n'+ '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        if verbose:\n",
    "            print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        extra_cols = formatter.get_sample_column(embeddings, column_aug)\n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')] + extra_cols))\n",
    "        try: \n",
    "            # formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "            sample_data = add_row_number(sample_data.loc[:, columns])\n",
    "        except:\n",
    "            sample_data = add_row_number(sample_data)\n",
    "        extra_information = []\n",
    "        tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        for col, com in tuples:\n",
    "            if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "                extra_information.append(col + ':' + com)\n",
    "            else:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "                extra_information.append(col + ':' + com)\n",
    "        extra_information.append('row_number: row index in the original table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data = sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\nColumn information:\\n' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "        if verbose:\n",
    "            print(stage_2_batch_pred)\n",
    "    # stage 3: SQL Excution\n",
    "    try: \n",
    "        execute_data = manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')\n",
    "    except:\n",
    "        execute_data = formatter.all_data\n",
    "        stage_2_batch_pred = 'SELECT * from DF;'\n",
    "    if len(execute_data) == 0:\n",
    "        return query, stage_2_batch_pred, 'No data from database', cb.total_tokens\n",
    "    return query, stage_2_batch_pred, TableFormat.format_html(data=execute_data), cb.total_tokens\n",
    "    # return query, stage_2_batch_pred, execute_data, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_blury_string(pred_list):\n",
    "    pred_label = []\n",
    "    for pred in pred_list:\n",
    "        predict_ans = pred.split('\\n')[-1]\n",
    "        if '0' in predict_ans:\n",
    "            predict_ans = '0'\n",
    "        elif '1' in predict_ans:\n",
    "            predict_ans = '1'\n",
    "        else:\n",
    "            predict_ans = '2'\n",
    "        pred_label.append(predict_ans)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)\n",
    "\n",
    "def save_csv(input_list: List[List], label_list: List, file_path):\n",
    "    import pandas as pd\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    assert len(input_list) == len(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(label_list)):\n",
    "        df[label_list[i]] = pd.Series(input_list[i])\n",
    "    if os.path.exists(file_path) and file_path.endswith('.csv'):\n",
    "        df_origin = pd.read_csv(file_path)\n",
    "        df = pd.concat([df_origin, df], axis=0)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整extrainformation的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering and answer the query using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: \n",
    "{table}\n",
    "Query: {claim}\n",
    "Please provide a clear, complete statement in response to the query. If you cannot answer the query based on the sub-table, just say 'Cannot get answer from sub-table'.\n",
    "\"\"\" )\n",
    "def scene_B(query, sample, k=3, verbose=False):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"Our ultimate goal is to answer the query based on the original table. Now we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table.  Based on the augmentation information, carefully analyze the query and write an SQLITE3 SELECT SQL statement using table DF that completes the query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    \n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    if k == 0:\n",
    "        sample_data = formatter.get_sample_data(sample_type='head', k=k)\n",
    "    else:\n",
    "        sample_data = formatter.get_sample_data(sample_type='embedding', query=query, k=k)\n",
    "    # get columns\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_aug(), verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['table']['id']]['summary'], aug_information.loc[sample['table']['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n' + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        extra_cols = formatter.get_sample_column(embeddings, column_aug)\n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')] + extra_cols))\n",
    "        try: \n",
    "            sample_data = add_row_number(sample_data.loc[:, columns])\n",
    "        except:\n",
    "            sample_data = add_row_number(sample_data)\n",
    "        extra_information = []\n",
    "        tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        for col, com in tuples:\n",
    "            if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "                extra_information.append(col + ':' + com)\n",
    "            else:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "                extra_information.append(col + ':' + com)\n",
    "        #  sample augmentation\n",
    "        # extra_information = (parse_specific_composition(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns))\n",
    "        extra_information.append('row_number: row index in the table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "    \n",
    "        \n",
    "        # stage 3: SQL Excution\n",
    "        try: \n",
    "            execute_data= manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')\n",
    "        except:\n",
    "            execute_data = formatter.all_data\n",
    "            stage_2_batch_pred = 'SELECT * from DF;'\n",
    "        llm_chain = LLMChain(llm=model, prompt=answer_instruction, verbose=verbose)\n",
    "        response = llm_chain.batch([dict({'table': TableFormat.format_html(execute_data),\n",
    "                                                'claim': query,\n",
    "                                                'SQL':  stage_2_batch_pred\n",
    "                                                })], return_only_outputs=True)[0]['text']\n",
    "    # print(\"total_tokens:\", cb.total_tokens)\n",
    "    return response, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0c75de50975e4f278b882fe90da47f2f\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ces.openai.azure.com\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=0.3,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '跑的是加了选列的shot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m跑的是加了选列的shot\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name '跑的是加了选列的shot' is not defined"
     ]
    }
   ],
   "source": [
    "跑的是加了选列的shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils import generate_random_text, load_courp\n",
    "en = load_courp('../result/data/en_corpus.txt')\n",
    "def replace_sample(sample, col_schema):\n",
    "    np.random.seed(30)\n",
    "    num_col = len(sample['table']['header'])\n",
    "    num_row = len(sample['table']['rows'])\n",
    "    total_cells = num_col * num_row\n",
    "    if total_cells <= 150:\n",
    "        ratio = 0.02\n",
    "    elif 150 <total_cells <= 300:\n",
    "        ratio = 0.05\n",
    "    elif 300 <total_cells <= 450:\n",
    "        ratio = 0.1\n",
    "    elif 450 <total_cells:\n",
    "        ratio = 0.12\n",
    "    num_cells_to_sample = int(total_cells * ratio)\n",
    "    index_pairs = np.random.choice(total_cells, num_cells_to_sample, replace=False)\n",
    "    \n",
    "    selected_cells = []\n",
    "    generate_cells = []\n",
    "    \n",
    "    for index in index_pairs:\n",
    "        i = index // num_col\n",
    "        j = index % num_col\n",
    "        selected_cells.append((i, j))\n",
    "        # print(i, j)\n",
    "        generate_cells.append(generate_random_text(col_schema[j], en))\n",
    "        # print(generate_cells)\n",
    "    for idx, (i, j) in enumerate(selected_cells):\n",
    "        sample['table']['rows'][i][j] = generate_cells[idx]  \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad4655222fa45aa842f42153ef9ea92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Process 3031\n",
      " Process 3032\n",
      " Process 3033\n",
      " Process 3034\n",
      " Process 3035\n",
      " Process 3036\n",
      " Process 3037\n",
      " Process 3038\n",
      " Process 3039\n",
      "saving 3039\n",
      " Process 3040\n",
      " Process 3041\n",
      " Process 3042\n",
      " Process 3043\n",
      " Process 3044\n",
      " Process 3045\n",
      " Process 3046\n",
      " Process 3047\n",
      " Process 3048\n",
      " Process 3049\n",
      "saving 3049\n",
      " Process 3050\n",
      " Process 3051\n",
      " Process 3052\n",
      " Process 3053\n",
      " Process 3054\n",
      " Process 3055\n",
      " Process 3056\n",
      " Process 3057\n",
      " Process 3058\n",
      " Process 3059\n",
      "saving 3059\n",
      " Process 3060\n",
      " Process 3061\n",
      " Process 3062\n",
      " Process 3063\n",
      " Process 3064\n",
      " Process 3065\n",
      " Process 3066\n",
      " Process 3067\n",
      " Process 3068\n",
      " Process 3069\n",
      "saving 3069\n",
      " Process 3070\n",
      " Process 3071\n",
      " Process 3072\n",
      " Process 3073\n",
      " Process 3074\n",
      " Process 3075\n",
      " Process 3076\n",
      " Process 3077\n",
      " Process 3078\n",
      " Process 3079\n",
      "saving 3079\n",
      " Process 3080\n",
      " Process 3081\n",
      " Process 3082\n",
      " Process 3083\n",
      " Process 3084\n",
      " Process 3085\n",
      " Process 3086\n",
      " Process 3087\n",
      " Process 3088\n",
      " Process 3089\n",
      "saving 3089\n",
      " Process 3090\n",
      " Process 3091\n",
      " Process 3092\n",
      " Process 3093\n",
      " Process 3094\n",
      " Process 3095\n",
      " Process 3096\n",
      " Process 3097\n",
      " Process 3098\n",
      " Process 3099\n",
      "saving 3099\n",
      " Process 3100\n",
      " Process 3101\n",
      " Process 3102\n",
      " Process 3103\n",
      " Process 3104\n",
      " Process 3105\n",
      " Process 3106\n",
      " Process 3107\n",
      " Process 3108\n",
      " Process 3109\n",
      "saving 3109\n",
      " Process 3110\n",
      " Process 3111\n",
      " Process 3112\n",
      " Process 3113\n",
      " Process 3114\n",
      " Process 3115\n",
      " Process 3116\n",
      " Process 3117\n",
      " Process 3118\n",
      " Process 3119\n",
      "saving 3119\n",
      " Process 3120\n",
      " Process 3121\n",
      " Process 3122\n",
      " Process 3123\n",
      " Process 3124\n",
      " Process 3125\n",
      " Process 3126\n",
      " Process 3127\n",
      " Process 3128\n",
      " Process 3129\n",
      "saving 3129\n",
      " Process 3130\n",
      " Process 3131\n",
      " Process 3132\n",
      " Process 3133\n",
      " Process 3134\n",
      " Process 3135\n",
      " Process 3136\n",
      " Process 3137\n",
      " Process 3138\n",
      " Process 3139\n",
      "saving 3139\n",
      " Process 3140\n",
      " Process 3141\n",
      " Process 3142\n",
      " Process 3143\n",
      " Process 3144\n",
      " Process 3145\n",
      " Process 3146\n",
      " Process 3147\n",
      " Process 3148\n",
      " Process 3149\n",
      "saving 3149\n",
      " Process 3150\n",
      " Process 3151\n",
      " Process 3152\n",
      " Process 3153\n",
      " Process 3154\n",
      " Process 3155\n",
      " Process 3156\n",
      " Process 3157\n",
      " Process 3158\n",
      " Process 3159\n",
      "saving 3159\n",
      " Process 3160\n",
      " Process 3161\n",
      " Process 3162\n",
      " Process 3163\n",
      " Process 3164\n",
      " Process 3165\n",
      " Process 3166\n",
      " Process 3167\n",
      " Process 3168\n",
      " Process 3169\n",
      "saving 3169\n",
      " Process 3170\n",
      " Process 3171\n",
      " Process 3172\n",
      " Process 3173\n",
      " Process 3174\n",
      " Process 3175\n",
      " Process 3176\n",
      " Process 3177\n",
      " Process 3178\n",
      " Process 3179\n",
      "saving 3179\n",
      " Process 3180\n",
      " Process 3181\n",
      " Process 3182\n",
      " Process 3183\n",
      " Process 3184\n",
      " Process 3185\n",
      " Process 3186\n",
      " Process 3187\n",
      " Process 3188\n",
      " Process 3189\n",
      "saving 3189\n",
      " Process 3190\n",
      " Process 3191\n",
      " Process 3192\n",
      " Process 3193\n",
      " Process 3194\n",
      " Process 3195\n",
      " Process 3196\n",
      " Process 3197\n",
      " Process 3198\n",
      " Process 3199\n",
      "saving 3199\n",
      " Process 3200\n",
      " Process 3201\n",
      " Process 3202\n",
      " Process 3203\n",
      " Process 3204\n",
      " Process 3205\n",
      " Process 3206\n",
      " Process 3207\n",
      " Process 3208\n",
      " Process 3209\n",
      "saving 3209\n",
      " Process 3210\n",
      " Process 3211\n",
      " Process 3212\n",
      " Process 3213\n",
      " Process 3214\n",
      " Process 3215\n",
      " Process 3216\n",
      " Process 3217\n",
      " Process 3218\n",
      " Process 3219\n",
      "saving 3219\n",
      " Process 3220\n",
      " Process 3221\n",
      " Process 3222\n",
      " Process 3223\n",
      " Process 3224\n",
      " Process 3225\n",
      " Process 3226\n",
      " Process 3227\n",
      " Process 3228\n",
      " Process 3229\n",
      "saving 3229\n",
      " Process 3230\n",
      " Process 3231\n",
      " Process 3232\n",
      " Process 3233\n",
      " Process 3234\n",
      " Process 3235\n",
      " Process 3236\n",
      " Process 3237\n",
      " Process 3238\n",
      " Process 3239\n",
      "saving 3239\n",
      " Process 3240\n",
      " Process 3241\n",
      " Process 3242\n",
      " Process 3243\n",
      " Process 3244\n",
      " Process 3245\n",
      " Process 3246\n",
      " Process 3247\n",
      " Process 3248\n",
      " Process 3249\n",
      "saving 3249\n",
      " Process 3250\n",
      " Process 3251\n",
      " Process 3252\n",
      " Process 3253\n",
      " Process 3254\n",
      " Process 3255\n",
      " Process 3256\n",
      " Process 3257\n",
      " Process 3258\n",
      " Process 3259\n",
      "saving 3259\n",
      " Process 3260\n",
      " Process 3261\n",
      " Process 3262\n",
      " Process 3263\n",
      " Process 3264\n",
      " Process 3265\n",
      " Process 3266\n",
      " Process 3267\n",
      " Process 3268\n",
      " Process 3269\n",
      "saving 3269\n",
      " Process 3270\n",
      " Process 3271\n",
      " Process 3272\n",
      " Process 3273\n",
      " Process 3274\n",
      " Process 3275\n",
      " Process 3276\n",
      " Process 3277\n",
      " Process 3278\n",
      " Process 3279\n",
      "saving 3279\n",
      " Process 3280\n",
      " Process 3281\n",
      " Process 3282\n",
      " Process 3283\n",
      " Process 3284\n",
      " Process 3285\n",
      " Process 3286\n",
      " Process 3287\n",
      " Process 3288\n",
      " Process 3289\n",
      "saving 3289\n",
      " Process 3290\n",
      " Process 3291\n",
      " Process 3292\n",
      " Process 3293\n",
      " Process 3294\n",
      " Process 3295\n",
      " Process 3296\n",
      " Process 3297\n",
      " Process 3298\n",
      " Process 3299\n",
      "saving 3299\n",
      " Process 3300\n",
      " Process 3301\n",
      " Process 3302\n",
      " Process 3303\n",
      " Process 3304\n",
      " Process 3305\n",
      " Process 3306\n",
      " Process 3307\n",
      " Process 3308\n",
      " Process 3309\n",
      "saving 3309\n",
      " Process 3310\n",
      " Process 3311\n",
      " Process 3312\n",
      " Process 3313\n",
      " Process 3314\n",
      " Process 3315\n",
      " Process 3316\n",
      " Process 3317\n",
      " Process 3318\n",
      " Process 3319\n",
      "saving 3319\n",
      " Process 3320\n",
      " Process 3321\n",
      " Process 3322\n",
      " Process 3323\n",
      " Process 3324\n",
      " Process 3325\n",
      " Process 3326\n",
      " Process 3327\n",
      " Process 3328\n",
      " Process 3329\n",
      "saving 3329\n",
      " Process 3330\n",
      " Process 3331\n",
      " Process 3332\n",
      " Process 3333\n",
      " Process 3334\n",
      " Process 3335\n",
      " Process 3336\n",
      " Process 3337\n",
      " Process 3338\n",
      " Process 3339\n",
      "saving 3339\n",
      " Process 3340\n",
      " Process 3341\n",
      " Process 3342\n",
      " Process 3343\n",
      " Process 3344\n",
      " Process 3345\n",
      " Process 3346\n",
      " Process 3347\n",
      " Process 3348\n",
      " Process 3349\n",
      "saving 3349\n",
      " Process 3350\n",
      " Process 3351\n",
      " Process 3352\n",
      " Process 3353\n",
      " Process 3354\n",
      " Process 3355\n",
      " Process 3356\n",
      " Process 3357\n",
      " Process 3358\n",
      " Process 3359\n",
      "saving 3359\n",
      " Process 3360\n",
      " Process 3361\n",
      " Process 3362\n",
      " Process 3363\n",
      " Process 3364\n",
      " Process 3365\n",
      " Process 3366\n",
      " Process 3367\n",
      " Process 3368\n",
      " Process 3369\n",
      "saving 3369\n",
      " Process 3370\n",
      " Process 3371\n",
      " Process 3372\n",
      " Process 3373\n",
      " Process 3374\n",
      " Process 3375\n",
      " Process 3376\n",
      " Process 3377\n",
      " Process 3378\n",
      " Process 3379\n",
      "saving 3379\n",
      " Process 3380\n",
      " Process 3381\n",
      " Process 3382\n",
      " Process 3383\n",
      " Process 3384\n",
      " Process 3385\n",
      " Process 3386\n",
      " Process 3387\n",
      " Process 3388\n",
      " Process 3389\n",
      "saving 3389\n",
      " Process 3390\n",
      " Process 3391\n",
      " Process 3392\n",
      " Process 3393\n",
      " Process 3394\n",
      " Process 3395\n",
      " Process 3396\n",
      " Process 3397\n",
      " Process 3398\n",
      " Process 3399\n",
      "saving 3399\n",
      " Process 3400\n",
      " Process 3401\n",
      " Process 3402\n",
      " Process 3403\n",
      " Process 3404\n",
      " Process 3405\n",
      " Process 3406\n",
      " Process 3407\n",
      " Process 3408\n",
      " Process 3409\n",
      "saving 3409\n",
      " Process 3410\n",
      " Process 3411\n",
      " Process 3412\n",
      " Process 3413\n",
      " Process 3414\n",
      " Process 3415\n",
      " Process 3416\n",
      " Process 3417\n",
      " Process 3418\n",
      " Process 3419\n",
      "saving 3419\n",
      " Process 3420\n",
      " Process 3421\n",
      " Process 3422\n",
      " Process 3423\n",
      " Process 3424\n",
      " Process 3425\n",
      " Process 3426\n",
      " Process 3427\n",
      " Process 3428\n",
      " Process 3429\n",
      "saving 3429\n",
      " Process 3430\n",
      " Process 3431\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError\n",
    "from tqdm.notebook import tqdm\n",
    "# table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "table_loader = TableLoader(table_name='wikitable', split='test', use_sample=False, small_test=False)\n",
    "# model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-bLZSHx4pKfPRZkYyIyyvUHSEjrlqj5sh2QIsxOM23yJnyoGD\", temperature=0.01)\n",
    "# save_path = f\"../result/final_answer/wikitable_cell_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "save_path = f\"../result/final_answer/wikitable_cell_06-06_14-37-05.csv\"\n",
    "\n",
    "# reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "# template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "template = \"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table:\n",
    "{table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {query}\n",
    "Think step by step and answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "\"\"\" )\n",
    "sample_k = 3\n",
    "# Task: answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\n",
    "\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "# for sample_n in range(3):\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i = 3030\n",
    "with tqdm(total=len(table_loader.dataset) - 3030, desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        try:\n",
    "            sample = table_loader.normalize_table(\n",
    "                                table_loader.dataset[i])\n",
    "            col_n, col_s = parse_output(schema_information.loc[sample['table']['id']]['schema'])\n",
    "            sample = replace_sample(sample, col_s)\n",
    "            all_tokens = 0\n",
    "            all_queries = []\n",
    "            formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "            sample_data = formatter.get_sample_data(sample_type='random', k=sample_k, query=sample['query'])\n",
    "            with get_openai_callback() as cb:\n",
    "                llm_chain = LLMChain(llm=model, prompt=step_back_prompt_wiki, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                    all_queries.append(batch_pred[0]['text'].strip())\n",
    "                # llm_chain = LLMChain(llm=model, prompt=disambiguous_prompt_wiki, verbose=False)\n",
    "                # batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                # all_queries.append(batch_pred[0]['text'].strip())\n",
    "                llm_chain = LLMChain(llm=model, prompt=decompose_prompt_wiki, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "                # print(all_queries)\n",
    "            all_tokens += cb.total_tokens\n",
    "            all_queries = list(set(all_queries))\n",
    "            args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "            # print(args_list)\n",
    "            ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "            results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0] ]\n",
    "            all_tokens += sum([res[1] for res in ans_from_B])\n",
    "            #With answer\n",
    "            # results= []\n",
    "            with get_openai_callback() as cb:\n",
    "                imp_input = scene_A(sample['query'], sample, sample_k, False)\n",
    "                llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "            # print(batch_pred[0])\n",
    "            all_tokens += cb.total_tokens\n",
    "            # print('ALL TOKENS', all_tokens)\n",
    "            ids.append(sample['id'])\n",
    "            labels.append(sample['query'])\n",
    "            outputs.append(batch_pred[0]['text'])\n",
    "            tokens.append(all_tokens)\n",
    "            extra_quries.append(';'.join(all_queries))\n",
    "            if (i + 1) % 10 == 0:\n",
    "                    print(f'saving {i}')\n",
    "                    save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "                    outputs = []\n",
    "                    labels = []\n",
    "                    ids = []\n",
    "                    tokens = []\n",
    "                    extra_quries = []\n",
    "            i += 1\n",
    "            print(f' Process {i}')\n",
    "            pbar.update(1)\n",
    "        \n",
    "        except BadRequestError as e:\n",
    "            print('*************************Bad Request**************')\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "        except ValueError as e:\n",
    "            print(f'******************Value Error {i}****************************')\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the total number of Škoda cars sold in 2005?',\n",
       " 'what is the number of Škoda Octavia cars sold in 2005?',\n",
       " 'what is the number of Škoda Yeti cars sold in 2005?']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Thought: The SQL query filters the data where the Description_Losses is 'Murdered' and then sums up the values in the c_1940_41 column. The result of this sum is 100,000.\\nAnswer: 100000\"]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the total number of robots in the middle east?',\n",
       " 'what is the total number of robots in the Middle East?',\n",
       " 'what are the robots in the middle east?']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thought: The BBC Three weekly ranking for episode 9 was 6, which is higher than the ranking for episode 8, which was 5. Therefore, episode 9 had a better BBC Three weekly ranking than episode 8.\\nAnswer: episode 9']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932fd7781a054b68887dddff557dbb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 3012\n",
      "process 3516\n",
      "process 4145\n",
      "process 4213\n"
     ]
    }
   ],
   "source": [
    "##### add residual \n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "# from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError, RateLimitError\n",
    "from tqdm.notebook import tqdm\n",
    "table_loader = TableLoader(table_name='wikitable', split='test', use_sample=False, small_test=False)\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-bLZSHx4pKfPRZkYyIyyvUHSEjrlqj5sh2QIsxOM23yJnyoGD\", temperature=0.3)\n",
    "save_path = f\"../result/final_answer/wikitable_06-05_02-40-29.csv\"\n",
    "sample_k = 3\n",
    "data = pd.read_csv(save_path)\n",
    "\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i = 0\n",
    "with tqdm(total=len(table_loader.dataset), desc=f\"Processing\",ncols=150) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        if table_loader.dataset[i]['id'] in list(data['ids']):\n",
    "            i += 1\n",
    "        else:\n",
    "            try:\n",
    "                sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "                all_tokens = 0\n",
    "                all_queries = []\n",
    "                formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "                sample_data = formatter.get_sample_data(sample_type='random', k=sample_k, query=sample['query'])\n",
    "                with get_openai_callback() as cb:\n",
    "                    llm_chain = LLMChain(llm=model, prompt=step_back_prompt_wiki, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                        all_queries.append(batch_pred[0]['text'].strip())\n",
    "                    # llm_chain = LLMChain(llm=model, prompt=disambiguous_prompt_wiki, verbose=False)\n",
    "                    # batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    # all_queries.append(batch_pred[0]['text'].strip())\n",
    "                    llm_chain = LLMChain(llm=model, prompt=decompose_prompt_wiki, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "                    # print(all_queries)\n",
    "                all_tokens += cb.total_tokens\n",
    "                all_queries = list(set(all_queries))\n",
    "                args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "                # print(args_list)\n",
    "                ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "                results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0] ]\n",
    "                all_tokens += sum([res[1] for res in ans_from_B])\n",
    "                #With answer\n",
    "                # results= []\n",
    "                with get_openai_callback() as cb:\n",
    "                    imp_input = scene_A(sample['query'], sample, sample_k, False)\n",
    "                    llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "                # print(batch_pred[0])\n",
    "                all_tokens += cb.total_tokens\n",
    "                # print('ALL TOKENS', all_tokens)\n",
    "                ids.append(sample['id'])\n",
    "                labels.append(sample['query'])\n",
    "                outputs.append(batch_pred[0]['text'])\n",
    "                tokens.append(all_tokens)\n",
    "                extra_quries.append(';'.join(all_queries))\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "                print(f'process {i}')\n",
    "            except RateLimitError as e:\n",
    "                print('*************************Rate limit**************')\n",
    "                pass\n",
    "        pbar.update(1)\n",
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_extra_info(summary_aug, column_aug, composition_aug, columns):\n",
    "    col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "    items, crackets = parse_output(composition_aug, pattern = r'\\d. (.+?): (.+)')\n",
    "    assert len(items) == len(col_names)\n",
    "    extra_col_info = []\n",
    "    for i_c in range(len(col_names)):\n",
    "        if col_names[i_c] in columns:\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]} {crackets[i_c]}')\n",
    "            \n",
    "    extra_col_info.append('row_number: row number in the original table')\n",
    "    return summary_aug + '\\n'.join(extra_col_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
