{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 5.24早修改table format 空格\\xa\n",
    "- 5.24 下午修改str_normalize \n",
    "- 5.24 晚 72需要复查\n",
    "- 5.25 早list query strip() \n",
    "- 5.27 修改total，尝试disambiguous/  改成PIPE  / 晚上发现列名重复修改的问题， 发现shot来自test的问题\n",
    "- 5.29 在shot中添加extra\n",
    "- 5.31 在选列aug的shot中添加extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/tabular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import parse_specific_composition, add_row_number, parse_specific_composition_zh, parse_output,str_normalize, normalize_number\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "import numpy as np\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name='BAAI/bge-large-en',\n",
    "            model_kwargs={'device': 'cuda:0', 'trust_remote_code': True},\n",
    "            encode_kwargs={'normalize_embeddings': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output, load_courp, generate_random_text, composite_frame, set_random_cells_to_empty\n",
    "import random\n",
    "\n",
    "en = load_courp('../result/data/en_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "def composite_frame(df1, df2, reorder=False):\n",
    "    df2 = pd.concat([df1, df2])\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "    return df2\n",
    "def add_random_rows(sample, col_schema, times=1):\n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "    df = formatter.all_data\n",
    "    num_col = df.shape[1]\n",
    "    num_row = df.shape[0]\n",
    "    total_cells = num_col * num_row\n",
    "    if total_cells <= 150:\n",
    "        n = 1 * times\n",
    "    elif 150 <total_cells <= 300:\n",
    "        n = 2 * times\n",
    "    elif 300 <total_cells <= 450:\n",
    "        n = 5 * times\n",
    "    elif 450 <total_cells:\n",
    "        n = 8 * times\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        row = []\n",
    "        for i in range(num_col):\n",
    "            if col_schema[i] == 'Numerical':\n",
    "                try:\n",
    "                    num_column = pd.to_numeric(df.iloc[: , i].apply(lambda x: normalize_number(x)))\n",
    "                    random_number = random.uniform(min(num_column), max(num_column))\n",
    "                    row.append(str(random_number))\n",
    "                except:\n",
    "                    row.append(generate_random_text(col_schema[i], en))\n",
    "            elif col_schema[i] == 'Date':\n",
    "                start_date = datetime.datetime.strptime('1890-01-01', '%Y-%m-%d')\n",
    "                end_date = datetime.datetime.strptime('1990-12-31', '%Y-%m-%d')\n",
    "                delta = (end_date - start_date).days\n",
    "                \n",
    "                try:\n",
    "                    data_column = pd.to_datetime(df.iloc[: , i].apply(lambda x: str_normalize(x)), format='%Y-%m-%d', errors='coerce')\n",
    "                    if not pd.isna(data_column.min()) and not pd.isna(data_column.max()):\n",
    "                        start_date = data_column.min()\n",
    "                        end_date = data_column.max()\n",
    "                        delta = (end_date - start_date).days\n",
    "                except:\n",
    "                    pass\n",
    "                if delta > 0:\n",
    "                    random_day = random.randrange(delta)\n",
    "                    # 加上开始日期得到随机日期\n",
    "                    random_date = start_date + timedelta(days=random_day)\n",
    "                # 格式化输出\n",
    "                    out = random_date.strftime('%Y-%m-%d')\n",
    "                else:\n",
    "                    out = ''\n",
    "                row.append(out)\n",
    "                \n",
    "            elif col_schema[i] == 'Char':\n",
    "                row.append(generate_random_text('Char', en))\n",
    "            else:\n",
    "                row.append('')\n",
    "        rows.append(row)\n",
    "    sample['table']['rows'] += rows\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\n",
    "                  \"what was the time difference between the first place finisher and the eighth place finisher?\",\n",
    "                  \"other than william stuart price, which other businessman was born in tulsa?\",\n",
    "                  \"which canadian city had the most passengers traveling from manzanillo international airport in 2013?\"\n",
    "                  ]\n",
    "new_query_examples = [\n",
    "                      \"what was the time for the first place finisher?; what was the time for the eighth place finisher?\",\n",
    "                      \"was william stuart price born in tulsa?; who was born in tulsa?\",\n",
    "                      \"how many passengers do each airline from canadian city have?; which canadian city had the most passengers?\"\n",
    "                      ]\n",
    "num_k = 3\n",
    "inds = [1, 11, 86]\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True, embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"table\", \"new_query\"], template=\n",
    "\"\"\"Sub-Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"table\": examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "decompose_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are capable of converting complex query into sub queries. Based on the table, decompose original query into at most 2 complete sub queries which can solve original query. Output new query directly.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Sub-Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'wikitable'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "# model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-WZtqZEeuE0Xb6syVghDgAxdwe0ASWLkQRGxl61UI7B9RqNC4\", temperature=0.7).bind(logprobs=True)\n",
    "schema_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:////media/disk1/chatgpt/zh/tabular_data/db/sqlite/add_row.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name=task_name, split='test', use_sample=False, small_test=False)\n",
    "sample = table_loader.normalize_table(table_loader.dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = TableFormat(format='none', data=sample, save_embedding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按比例抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [5, 11, 46]\n",
    "num_k = 3\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True,embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\"was the Sandia Peak Tramway before or after the 3S Aerial Tramway in terms of Year_of_inauguration?\",\n",
    "                      \"other than William Stuart Price, which other businessman's was in Tulsa in terms of Hometown?\",\n",
    "                      \"How many players are G\\\\nF in terms of Position?\"]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "Table: {table}\n",
    "New query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": table_loader.dataset[inds[i]]['question'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "disambiguous_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    # prefix=\"\"\"Based on the given table, your task is to rewrite the query to resolve ambiguity and ensure the question is consistent with the table. \n",
    "    # This requires pinpointing elements of the question to table contents and rewriting the question to ensure a consistent, clear interpretation. \"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Query: {query}\n",
    "Table: {table}\n",
    "New query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [11, 182, 70]\n",
    "num_k = 2\n",
    "table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=True,embeddings=embeddings).get_sample_data(sample_type='embedding', query=normalised_data[i]['query']) for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\n",
    "    # \"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "    \"which business man was born in tulsa?\",\n",
    "    \"what is the network owned by national polytechnic institute?\",\n",
    "    \"what districts are more populous than haridwar?\"\n",
    "    ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "New query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": table_loader.dataset[inds[i]]['question'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "step_back_prompt_wiki = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Based on the table, your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "New query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_answer(k: int=1):\n",
    "#     sqls = [\"SELECT COUNT(*) FROM DF WHERE Outcome = 'Runner-up' AND Opponent = 'Roger Federer';\"\n",
    "#             ]\n",
    "#     thoughts = [\"The SQL query filters the data to only include rows where the outcome is 'Runner-up' and the opponent is 'Roger Federer'. The sub-table shows that Roger Federer was a runner-up 2 times.\"]\n",
    "#     tables = [\"\"\" <table>\n",
    "# <thead>\n",
    "# <tr><th>  COUNT(*)</th></tr>\n",
    "# </thead>\n",
    "# <tbody>\n",
    "# <tr><td>2.0000    </td></tr>\n",
    "# </tbody>\n",
    "# </table>\"\"\"]\n",
    "#     tables_pipe = [\"\"\"/*\n",
    "# table caption : turkish cup\n",
    "# col : MAX(winners_c_from_previous_round)\n",
    "# row 1: 54\n",
    "# */\"\"\"]\n",
    "#     claims = [\"how many times was roger federer a runner-up?\"]\n",
    "        sqls = [\"SELECT DISTINCT Type FROM DF WHERE Type != 'audio';\"\n",
    "                ]\n",
    "        thoughts = [\"Based on the SQL query and the extra information provided, the types include audio or video. Therefore, other than audio, the payload type is video.\"]\n",
    "        tables = ['<table>\\n<thead>\\n<tr><th> Type </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>video   </td></tr>\\n<tr><td>audio/video   </td></tr>\\n</tbody>\\n</table>']\n",
    "        claims = [\"other than audio, what type of payload types are there?\"]\n",
    "        extras = [\"The payload types for audio include audio, video, and audio/video.\"]\n",
    "#     sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"\n",
    "#             ]\n",
    "#     thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. 3 is the fewest points they received. \"]\n",
    "#     tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "#     claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "    # inds from test split\n",
    "        examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"information\",  \"claim\", \"thought\", \"output\"], template=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {claim}\n",
    "Thought: {thought}\n",
    "Answer: {output}\n",
    "        \"\"\")\n",
    "        examples_dict = dict(zip([\"SQL\", \"table\", \"information\",  \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], extras[0], claims[0], thoughts[0], 'video']))\n",
    "        prompt_template = FewShotPromptTemplate(\n",
    "                examples=[examples_dict],\n",
    "                example_prompt=examples_prompt,\n",
    "                prefix=\"\"\"Below is a sub-table generated by excuting the corresponding SQL and some extra information may be useful. You need to understand the logic behind the SQL filtering. Think step by step and answer the question given in the query.\n",
    "You should output in the following format:\n",
    "Thought: your step by step thought\n",
    "Answer: Only return the concise string instead of other format information. Do not repeat the question.\n",
    "Below is an example.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {query}\"\"\",\n",
    "                input_variables=[\"table\", \"query\", \"SQL\", \"information\"],\n",
    "        )\n",
    "        return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_aug(k: int=2):\n",
    "    table_loader = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "    examples_dict = []\n",
    "    \n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>Hoot Kloot</caption>\\n<thead>\\n<tr><th> Number</th><th> Title</th><th> Directed_by_</th><th> Released_</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>1  </td><td>\"Kloot\\'s Kounty\"           </td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>2  </td><td>\"Apache on the County Seat\"</td><td>Hawley Pratt  </td><td>1973       </td></tr>\\n<tr><td>6  </td><td>\"Mesa Trouble\"       </td><td>Sid Marcus </td><td>1974       </td></tr>\\n</tbody>\\n</table>',\n",
    "                                        \"claim\": table_loader.dataset[95]['question'],\n",
    "                                        \"aug\": \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year. \\n1. Number: The episode number in the series \\n2. Title: The title of the episode \\n3. Directed_by_: The director of the episode \\n4. Released_: The release year of the episode\",\n",
    "                                        \"linking\": \"the last title -> Released_, the last title-> Number, title -> Title, sid marcus -> Directed_by_\",\n",
    "                                        \"output\": \"Released_, Number, Title, Directed_by_\"}])\n",
    "    # examples_dict.extend([{\"table\": '<table>\\n<caption>1943–44 Chicago Black Hawks season</caption>\\n<thead>\\n<tr><th>  num</th><th>       Date</th><th>            Visitor</th><th>  Score</th><th>               Home</th><th>  Record</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>14   </td><td>December 5 </td><td>New York Rangers   </td><td>6–7    </td><td>Chicago Black Hawks</td><td>8–6–0   </td></tr>\\n<tr><td>40   </td><td>February 26</td><td>Chicago Black Hawks</td><td>3–2    </td><td>Toronto Maple Leafs</td><td>18–18–4 </td></tr>\\n<tr><td>31   </td><td>January 29 </td><td>Chicago Black Hawks</td><td>4–3    </td><td>Toronto Maple Leafs</td><td>14–16–1 </td></tr>\\n</tbody>\\n</table>',\n",
    "    #                                     \"claim\": 'what was the difference in score in the december 19th win?',\n",
    "    #                                     \"aug\": 'The table contains information about the 1943-44 Chicago Black Hawks season, including the date, visitor, score, home team, record, and points for each game. \\n1. num: The game number in the season \\n2. Date: The date of the game\\n3. Vistor: The visiting team\\n4. Score: The final score, with the visitor score listed first\\n5. Home: The home team\\n6. Record: The team win-loss-overtime loss record at the time of the game',\n",
    "    #                                     \"linking\": 'difference in score -> Score, december 19th -> Date',\n",
    "    #                                     \"output\": 'Date, Score'}])\n",
    "    \n",
    "    # \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year.\"\n",
    "    # \"№<The episode number in the series>\\nTitle<The title of the episode>\\nDirected_by_<The director of the episode>\\nReleased_<The release year of the episode>\"\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"aug\",\"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "Table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "Column linking: {linking}\n",
    "Columns: {output}\"\"\")\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "Based on the Table below, your task is to accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "Approach this task as follows:\n",
    "Read the query and extra information thoroughly and list every possible link from query term to column in the Table. \n",
    "Then based on the column linking, output all useful columns at last. Make sure all columns in the linking step are included and every column is in the Table.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "Table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\"\"\",\n",
    "        input_variables=[\"table\", \"claim\", \"aug\"],\n",
    ")\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "# from utils import parse_output\n",
    "# table_loader = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "# summary_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "# schema_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "# composition_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "\n",
    "\n",
    "# sample = table_loader.normalize_table(table_loader.dataset[110])\n",
    "\n",
    "# # summary_aug, column_aug = summary_information.loc[sample['table']['id']]['summary'], summary_information.loc[sample['table']['id']]['column_description'] \n",
    "# # col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "# # extra_col_info = []\n",
    "# # for i_c in range(len(col_names)):\n",
    "# #     extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "# formatter = TableFormat(format='none', data=sample)\n",
    "\n",
    "\n",
    "\n",
    "# llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_aug(), verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\n",
    "#     \"claim\": sample['query'],\n",
    "#     'aug':  '',\n",
    "#     # \"query\": \"how many of the seasons games were played in the gold coast convention centre?\",\n",
    "#                             \"table\": TableFormat.format_html(data= formatter.get_sample_data(sample_type='random',k=5, query=sample['query'],),table_caption=sample['table']['caption'])}],)\n",
    "# print(batch_pred[0]['text'])\n",
    "#     #                             \"table\": \"\"\"table caption : 2008 - 09 nbl season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "def scene_A(query, sample, k =3, verbose=True):\n",
    "    # Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "    # Our ultimate goal is to answer query based on the original table. Now we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    \n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    # print(len(formatter.all_data))\n",
    "    if k == 0:\n",
    "        sample_data = formatter.get_sample_data(sample_type='head', k=k)\n",
    "    else:\n",
    "        sample_data = formatter.get_sample_data(sample_type='embedding', query=query, k=k)\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['table']['id']]['summary'], aug_information.loc[sample['table']['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        \n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug +'\\n'+ '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        if verbose:\n",
    "            print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        extra_cols = formatter.get_sample_column(embeddings, column_aug)\n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')] + extra_cols))\n",
    "        \n",
    "        try: \n",
    "            # formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "            sample_data = sample_data.loc[:, columns]\n",
    "        except:\n",
    "            sample_data = sample_data\n",
    "        extra_information = []\n",
    "        tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        for col, com in tuples:\n",
    "            if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "                extra_information.append(col + ':' + com)\n",
    "            else:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "                extra_information.append(col + ':' + com)\n",
    "        # extra_information.append('row_number: row index in the original table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data = sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\nColumn information:\\n' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "        if verbose:\n",
    "            print(stage_2_batch_pred)\n",
    "    # stage 3: SQL Excution\n",
    "    try: \n",
    "        execute_data = manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')[:10]\n",
    "    except:\n",
    "        execute_data = formatter.all_data[:10]\n",
    "        stage_2_batch_pred = 'SELECT * from DF;'\n",
    "    if len(execute_data) == 0:\n",
    "        return query, stage_2_batch_pred, 'No data from database', cb.total_tokens\n",
    "    return query, stage_2_batch_pred, TableFormat.format_html(data=execute_data), cb.total_tokens\n",
    "    # return query, stage_2_batch_pred, execute_data, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_blury_string(pred_list):\n",
    "    pred_label = []\n",
    "    for pred in pred_list:\n",
    "        predict_ans = pred.split('\\n')[-1]\n",
    "        if '0' in predict_ans:\n",
    "            predict_ans = '0'\n",
    "        elif '1' in predict_ans:\n",
    "            predict_ans = '1'\n",
    "        else:\n",
    "            predict_ans = '2'\n",
    "        pred_label.append(predict_ans)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)\n",
    "\n",
    "def save_csv(input_list: List[List], label_list: List, file_path):\n",
    "    import pandas as pd\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    assert len(input_list) == len(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(label_list)):\n",
    "        df[label_list[i]] = pd.Series(input_list[i])\n",
    "    if os.path.exists(file_path) and file_path.endswith('.csv'):\n",
    "        df_origin = pd.read_csv(file_path)\n",
    "        df = pd.concat([df_origin, df], axis=0)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整extrainformation的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering and answer the query using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: \n",
    "{table}\n",
    "Query: {claim}\n",
    "Please provide a clear, complete statement in response to the query. If you cannot answer the query based on the sub-table, just say 'Cannot get answer from sub-table'.\n",
    "\"\"\" )\n",
    "def scene_B(query, sample, k=3, verbose=False):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"Our ultimate goal is to answer query based on the original table. Now we have a sub-table with rows sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Extra information: {aug}\n",
    "\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    \n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    if k == 0:\n",
    "        sample_data = formatter.get_sample_data(sample_type='head', k=k)\n",
    "    else:\n",
    "        sample_data = formatter.get_sample_data(sample_type='embedding', query=query, k=k)\n",
    "    # get columns\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_aug(), verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['table']['id']]['summary'], aug_information.loc[sample['table']['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n' + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        extra_cols = formatter.get_sample_column(embeddings, column_aug)\n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')] + extra_cols))\n",
    "        try: \n",
    "            sample_data = sample_data.loc[:, columns]\n",
    "        except:\n",
    "            sample_data = sample_data\n",
    "        extra_information = []\n",
    "        tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        for col, com in tuples:\n",
    "            if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "                extra_information.append(col + ':' + com)\n",
    "            else:\n",
    "                com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "                extra_information.append(col + ':' + com)\n",
    "        #  sample augmentation\n",
    "        # extra_information = (parse_specific_composition(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns))\n",
    "        # extra_information.append('row_number: row index in the table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "    \n",
    "        \n",
    "        # stage 3: SQL Excution\n",
    "        try: \n",
    "            execute_data= manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')[:10]\n",
    "        except:\n",
    "            execute_data = formatter.all_data[:10]\n",
    "            stage_2_batch_pred = 'SELECT * from DF;'\n",
    "        llm_chain = LLMChain(llm=model, prompt=answer_instruction, verbose=verbose)\n",
    "        response = llm_chain.batch([dict({'table': TableFormat.format_html(execute_data),\n",
    "                                                'claim': query,\n",
    "                                                'SQL':  stage_2_batch_pred\n",
    "                                                })], return_only_outputs=True)[0]['text']\n",
    "    # print(\"total_tokens:\", cb.total_tokens)\n",
    "    return response, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=0.01,\n",
    "    max_retries=5, request_timeout=600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ea9b4908794355a89fa3c28009da8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Process 1\n",
      " Process 2\n",
      " Process 3\n",
      " Process 4\n",
      " Process 5\n",
      " Process 6\n",
      " Process 7\n",
      " Process 8\n",
      " Process 9\n",
      "saving 9\n",
      " Process 10\n",
      "******************Value Error 10****************************\n",
      " Process 12\n",
      " Process 13\n",
      " Process 14\n",
      " Process 15\n",
      " Process 16\n",
      " Process 17\n",
      " Process 18\n",
      " Process 19\n",
      "saving 19\n",
      " Process 20\n",
      " Process 21\n",
      " Process 22\n",
      " Process 23\n",
      " Process 24\n",
      " Process 25\n",
      " Process 26\n",
      " Process 27\n",
      " Process 28\n",
      " Process 29\n",
      "saving 29\n",
      " Process 30\n",
      " Process 31\n",
      " Process 32\n",
      " Process 33\n",
      " Process 34\n",
      " Process 35\n",
      " Process 36\n",
      " Process 37\n",
      " Process 38\n",
      " Process 39\n",
      "saving 39\n",
      " Process 40\n",
      " Process 41\n",
      " Process 42\n",
      " Process 43\n",
      " Process 44\n",
      " Process 45\n",
      " Process 46\n",
      " Process 47\n",
      " Process 48\n",
      " Process 49\n",
      "saving 49\n",
      " Process 50\n",
      " Process 51\n",
      " Process 52\n",
      " Process 53\n",
      " Process 54\n",
      " Process 55\n",
      " Process 56\n",
      " Process 57\n",
      " Process 58\n",
      " Process 59\n",
      "saving 59\n",
      " Process 60\n",
      " Process 61\n",
      " Process 62\n",
      " Process 63\n",
      " Process 64\n",
      " Process 65\n",
      " Process 66\n",
      " Process 67\n",
      " Process 68\n",
      " Process 69\n",
      "saving 69\n",
      " Process 70\n",
      " Process 71\n",
      " Process 72\n",
      " Process 73\n",
      " Process 74\n",
      " Process 75\n",
      " Process 76\n",
      " Process 77\n",
      " Process 78\n",
      " Process 79\n",
      "saving 79\n",
      " Process 80\n",
      " Process 81\n",
      " Process 82\n",
      " Process 83\n",
      " Process 84\n",
      " Process 85\n",
      " Process 86\n",
      " Process 87\n",
      " Process 88\n",
      " Process 89\n",
      "saving 89\n",
      " Process 90\n",
      " Process 91\n",
      " Process 92\n",
      " Process 93\n",
      " Process 94\n",
      " Process 95\n",
      " Process 96\n",
      " Process 97\n",
      " Process 98\n",
      " Process 99\n",
      "saving 99\n",
      " Process 100\n",
      " Process 101\n",
      " Process 102\n",
      " Process 103\n",
      " Process 104\n",
      " Process 105\n",
      " Process 106\n",
      " Process 107\n",
      " Process 108\n",
      " Process 109\n",
      "saving 109\n",
      " Process 110\n",
      " Process 111\n",
      " Process 112\n",
      " Process 113\n",
      " Process 114\n",
      " Process 115\n",
      " Process 116\n",
      " Process 117\n",
      " Process 118\n",
      " Process 119\n",
      "saving 119\n",
      " Process 120\n",
      " Process 121\n",
      " Process 122\n",
      " Process 123\n",
      " Process 124\n",
      " Process 125\n",
      " Process 126\n",
      " Process 127\n",
      " Process 128\n",
      " Process 129\n",
      "saving 129\n",
      " Process 130\n",
      " Process 131\n",
      " Process 132\n",
      " Process 133\n",
      " Process 134\n",
      " Process 135\n",
      " Process 136\n",
      " Process 137\n",
      " Process 138\n",
      " Process 139\n",
      "saving 139\n",
      " Process 140\n",
      " Process 141\n",
      " Process 142\n",
      " Process 143\n",
      " Process 144\n",
      " Process 145\n",
      " Process 146\n",
      " Process 147\n",
      " Process 148\n",
      " Process 149\n",
      "saving 149\n",
      " Process 150\n",
      " Process 151\n",
      " Process 152\n",
      " Process 153\n",
      " Process 154\n",
      " Process 155\n",
      " Process 156\n",
      " Process 157\n",
      " Process 158\n",
      " Process 159\n",
      "saving 159\n",
      " Process 160\n",
      " Process 161\n",
      " Process 162\n",
      " Process 163\n",
      " Process 164\n",
      " Process 165\n",
      " Process 166\n",
      " Process 167\n",
      " Process 168\n",
      " Process 169\n",
      "saving 169\n",
      " Process 170\n",
      " Process 171\n",
      " Process 172\n",
      " Process 173\n",
      " Process 174\n",
      " Process 175\n",
      " Process 176\n",
      " Process 177\n",
      " Process 178\n",
      " Process 179\n",
      "saving 179\n",
      " Process 180\n",
      " Process 181\n",
      " Process 182\n",
      " Process 183\n",
      " Process 184\n",
      " Process 185\n",
      " Process 186\n",
      " Process 187\n",
      " Process 188\n",
      " Process 189\n",
      "saving 189\n",
      " Process 190\n",
      " Process 191\n",
      " Process 192\n",
      " Process 193\n",
      " Process 194\n",
      " Process 195\n",
      " Process 196\n",
      " Process 197\n",
      " Process 198\n",
      " Process 199\n",
      "saving 199\n",
      " Process 200\n",
      " Process 201\n",
      " Process 202\n",
      " Process 203\n",
      " Process 204\n",
      " Process 205\n",
      " Process 206\n",
      " Process 207\n",
      " Process 208\n",
      " Process 209\n",
      "saving 209\n",
      " Process 210\n",
      " Process 211\n",
      " Process 212\n",
      " Process 213\n",
      " Process 214\n",
      " Process 215\n",
      " Process 216\n",
      " Process 217\n",
      " Process 218\n",
      " Process 219\n",
      "saving 219\n",
      " Process 220\n",
      " Process 221\n",
      " Process 222\n",
      " Process 223\n",
      " Process 224\n",
      " Process 225\n",
      " Process 226\n",
      " Process 227\n",
      " Process 228\n",
      " Process 229\n",
      "saving 229\n",
      " Process 230\n",
      " Process 231\n",
      " Process 232\n",
      " Process 233\n",
      " Process 234\n",
      " Process 235\n",
      " Process 236\n",
      " Process 237\n",
      " Process 238\n",
      " Process 239\n",
      "saving 239\n",
      " Process 240\n",
      " Process 241\n",
      " Process 242\n",
      " Process 243\n",
      " Process 244\n",
      " Process 245\n",
      " Process 246\n",
      " Process 247\n",
      " Process 248\n",
      " Process 249\n",
      "saving 249\n",
      " Process 250\n",
      " Process 251\n",
      " Process 252\n",
      " Process 253\n",
      " Process 254\n",
      " Process 255\n",
      " Process 256\n",
      " Process 257\n",
      " Process 258\n",
      " Process 259\n",
      "saving 259\n",
      " Process 260\n",
      " Process 261\n",
      " Process 262\n",
      " Process 263\n",
      " Process 264\n",
      " Process 265\n",
      " Process 266\n",
      " Process 267\n",
      " Process 268\n",
      " Process 269\n",
      "saving 269\n",
      " Process 270\n",
      " Process 271\n",
      " Process 272\n",
      " Process 273\n",
      " Process 274\n",
      " Process 275\n",
      " Process 276\n",
      " Process 277\n",
      " Process 278\n",
      " Process 279\n",
      "saving 279\n",
      " Process 280\n",
      " Process 281\n",
      " Process 282\n",
      " Process 283\n",
      " Process 284\n",
      " Process 285\n",
      " Process 286\n",
      " Process 287\n",
      " Process 288\n",
      " Process 289\n",
      "saving 289\n",
      " Process 290\n",
      " Process 291\n",
      " Process 292\n",
      " Process 293\n",
      " Process 294\n",
      " Process 295\n",
      " Process 296\n",
      " Process 297\n",
      " Process 298\n",
      " Process 299\n",
      "saving 299\n",
      " Process 300\n",
      " Process 301\n",
      " Process 302\n",
      " Process 303\n",
      " Process 304\n",
      " Process 305\n",
      " Process 306\n",
      " Process 307\n",
      " Process 308\n",
      " Process 309\n",
      "saving 309\n",
      " Process 310\n",
      " Process 311\n",
      " Process 312\n",
      " Process 313\n",
      " Process 314\n",
      " Process 315\n",
      " Process 316\n",
      " Process 317\n",
      " Process 318\n",
      " Process 319\n",
      "saving 319\n",
      " Process 320\n",
      " Process 321\n",
      " Process 322\n",
      " Process 323\n",
      " Process 324\n",
      " Process 325\n",
      " Process 326\n",
      " Process 327\n",
      " Process 328\n",
      " Process 329\n",
      "saving 329\n",
      " Process 330\n",
      " Process 331\n",
      " Process 332\n",
      " Process 333\n",
      " Process 334\n",
      " Process 335\n",
      " Process 336\n",
      " Process 337\n",
      " Process 338\n",
      " Process 339\n",
      "saving 339\n",
      " Process 340\n",
      " Process 341\n",
      " Process 342\n",
      " Process 343\n",
      " Process 344\n",
      " Process 345\n",
      " Process 346\n",
      " Process 347\n",
      " Process 348\n",
      " Process 349\n",
      "saving 349\n",
      " Process 350\n",
      " Process 351\n",
      " Process 352\n",
      " Process 353\n",
      " Process 354\n",
      " Process 355\n",
      " Process 356\n",
      " Process 357\n",
      " Process 358\n",
      " Process 359\n",
      "saving 359\n",
      " Process 360\n",
      "*************************Bad Request**************\n",
      " Process 362\n",
      " Process 363\n",
      " Process 364\n",
      " Process 365\n",
      " Process 366\n",
      " Process 367\n",
      " Process 368\n",
      " Process 369\n",
      "saving 369\n",
      " Process 370\n",
      " Process 371\n",
      " Process 372\n",
      " Process 373\n",
      " Process 374\n",
      " Process 375\n",
      " Process 376\n",
      " Process 377\n",
      " Process 378\n",
      " Process 379\n",
      "saving 379\n",
      " Process 380\n",
      " Process 381\n",
      " Process 382\n",
      " Process 383\n",
      " Process 384\n",
      " Process 385\n",
      " Process 386\n",
      " Process 387\n",
      " Process 388\n",
      " Process 389\n",
      "saving 389\n",
      " Process 390\n",
      " Process 391\n",
      " Process 392\n",
      " Process 393\n",
      " Process 394\n",
      " Process 395\n",
      " Process 396\n",
      " Process 397\n",
      " Process 398\n",
      " Process 399\n",
      "saving 399\n",
      " Process 400\n",
      " Process 401\n",
      " Process 402\n",
      " Process 403\n",
      " Process 404\n",
      " Process 405\n",
      " Process 406\n",
      " Process 407\n",
      " Process 408\n",
      " Process 409\n",
      "saving 409\n",
      " Process 410\n",
      " Process 411\n",
      " Process 412\n",
      " Process 413\n",
      " Process 414\n",
      " Process 415\n",
      " Process 416\n",
      " Process 417\n",
      " Process 418\n",
      " Process 419\n",
      "saving 419\n",
      " Process 420\n",
      " Process 421\n",
      " Process 422\n",
      " Process 423\n",
      " Process 424\n",
      " Process 425\n",
      " Process 426\n",
      " Process 427\n",
      " Process 428\n",
      " Process 429\n",
      "saving 429\n",
      " Process 430\n",
      " Process 431\n",
      " Process 432\n",
      " Process 433\n",
      " Process 434\n",
      " Process 435\n",
      " Process 436\n",
      " Process 437\n",
      " Process 438\n",
      " Process 439\n",
      "saving 439\n",
      " Process 440\n",
      " Process 441\n",
      " Process 442\n",
      " Process 443\n",
      " Process 444\n",
      " Process 445\n",
      " Process 446\n",
      " Process 447\n",
      " Process 448\n",
      " Process 449\n",
      "saving 449\n",
      " Process 450\n",
      " Process 451\n",
      " Process 452\n",
      " Process 453\n",
      " Process 454\n",
      " Process 455\n",
      " Process 456\n",
      " Process 457\n",
      " Process 458\n",
      " Process 459\n",
      "saving 459\n",
      " Process 460\n",
      " Process 461\n",
      " Process 462\n",
      " Process 463\n",
      " Process 464\n",
      " Process 465\n",
      " Process 466\n",
      " Process 467\n",
      " Process 468\n",
      " Process 469\n",
      "saving 469\n",
      " Process 470\n",
      " Process 471\n",
      " Process 472\n",
      " Process 473\n",
      " Process 474\n",
      " Process 475\n",
      " Process 476\n",
      " Process 477\n",
      " Process 478\n",
      " Process 479\n",
      "saving 479\n",
      " Process 480\n",
      " Process 481\n",
      " Process 482\n",
      " Process 483\n",
      " Process 484\n",
      " Process 485\n",
      " Process 486\n",
      " Process 487\n",
      " Process 488\n",
      " Process 489\n",
      "saving 489\n",
      " Process 490\n",
      " Process 491\n",
      " Process 492\n",
      " Process 493\n",
      " Process 494\n",
      " Process 495\n",
      " Process 496\n",
      " Process 497\n",
      " Process 498\n",
      " Process 499\n",
      "saving 499\n",
      " Process 500\n",
      " Process 501\n",
      " Process 502\n",
      " Process 503\n",
      " Process 504\n",
      " Process 505\n",
      " Process 506\n",
      " Process 507\n",
      " Process 508\n",
      " Process 509\n",
      "saving 509\n",
      " Process 510\n",
      " Process 511\n",
      " Process 512\n",
      " Process 513\n",
      " Process 514\n",
      " Process 515\n",
      " Process 516\n",
      " Process 517\n",
      " Process 518\n",
      " Process 519\n",
      "saving 519\n",
      " Process 520\n",
      " Process 521\n",
      " Process 522\n",
      " Process 523\n",
      " Process 524\n",
      " Process 525\n",
      " Process 526\n",
      " Process 527\n",
      " Process 528\n",
      " Process 529\n",
      "saving 529\n",
      " Process 530\n",
      " Process 531\n",
      " Process 532\n",
      " Process 533\n",
      " Process 534\n",
      " Process 535\n",
      " Process 536\n",
      " Process 537\n",
      " Process 538\n",
      " Process 539\n",
      "saving 539\n",
      " Process 540\n",
      " Process 541\n",
      " Process 542\n",
      " Process 543\n",
      " Process 544\n",
      " Process 545\n",
      " Process 546\n",
      " Process 547\n",
      " Process 548\n",
      " Process 549\n",
      "saving 549\n",
      " Process 550\n",
      " Process 551\n",
      " Process 552\n",
      " Process 553\n",
      " Process 554\n",
      " Process 555\n",
      " Process 556\n",
      " Process 557\n",
      " Process 558\n",
      " Process 559\n",
      "saving 559\n",
      " Process 560\n",
      " Process 561\n",
      " Process 562\n",
      " Process 563\n",
      " Process 564\n",
      " Process 565\n",
      " Process 566\n",
      " Process 567\n",
      " Process 568\n",
      " Process 569\n",
      "saving 569\n",
      " Process 570\n",
      " Process 571\n",
      " Process 572\n",
      " Process 573\n",
      " Process 574\n",
      " Process 575\n",
      " Process 576\n",
      " Process 577\n",
      " Process 578\n",
      " Process 579\n",
      "saving 579\n",
      " Process 580\n",
      " Process 581\n",
      " Process 582\n",
      " Process 583\n",
      " Process 584\n",
      " Process 585\n",
      " Process 586\n",
      " Process 587\n",
      " Process 588\n",
      " Process 589\n",
      "saving 589\n",
      " Process 590\n",
      " Process 591\n",
      " Process 592\n",
      " Process 593\n",
      " Process 594\n",
      " Process 595\n",
      " Process 596\n",
      " Process 597\n",
      " Process 598\n",
      " Process 599\n",
      "saving 599\n",
      " Process 600\n",
      " Process 601\n",
      " Process 602\n",
      " Process 603\n",
      " Process 604\n",
      " Process 605\n",
      " Process 606\n",
      " Process 607\n",
      " Process 608\n",
      " Process 609\n",
      "saving 609\n",
      " Process 610\n",
      " Process 611\n",
      " Process 612\n",
      " Process 613\n",
      " Process 614\n",
      " Process 615\n",
      " Process 616\n",
      " Process 617\n",
      " Process 618\n",
      " Process 619\n",
      "saving 619\n",
      " Process 620\n",
      " Process 621\n",
      " Process 622\n",
      " Process 623\n",
      " Process 624\n",
      " Process 625\n",
      " Process 626\n",
      " Process 627\n",
      " Process 628\n",
      " Process 629\n",
      "saving 629\n",
      " Process 630\n",
      " Process 631\n",
      " Process 632\n",
      " Process 633\n",
      " Process 634\n",
      " Process 635\n",
      " Process 636\n",
      " Process 637\n",
      " Process 638\n",
      " Process 639\n",
      "saving 639\n",
      " Process 640\n",
      " Process 641\n",
      " Process 642\n",
      " Process 643\n",
      " Process 644\n",
      " Process 645\n",
      " Process 646\n",
      " Process 647\n",
      " Process 648\n",
      " Process 649\n",
      "saving 649\n",
      " Process 650\n",
      " Process 651\n",
      " Process 652\n",
      " Process 653\n",
      " Process 654\n",
      " Process 655\n",
      " Process 656\n",
      " Process 657\n",
      " Process 658\n",
      " Process 659\n",
      "saving 659\n",
      " Process 660\n",
      " Process 661\n",
      " Process 662\n",
      " Process 663\n",
      " Process 664\n",
      " Process 665\n",
      " Process 666\n",
      " Process 667\n",
      " Process 668\n",
      " Process 669\n",
      "saving 669\n",
      " Process 670\n",
      " Process 671\n",
      " Process 672\n",
      " Process 673\n",
      " Process 674\n",
      " Process 675\n",
      " Process 676\n",
      " Process 677\n",
      " Process 678\n",
      " Process 679\n",
      "saving 679\n",
      " Process 680\n",
      " Process 681\n",
      " Process 682\n",
      " Process 683\n",
      " Process 684\n",
      " Process 685\n",
      " Process 686\n",
      " Process 687\n",
      " Process 688\n",
      " Process 689\n",
      "saving 689\n",
      " Process 690\n",
      " Process 691\n",
      " Process 692\n",
      " Process 693\n",
      " Process 694\n",
      " Process 695\n",
      " Process 696\n",
      " Process 697\n",
      " Process 698\n",
      " Process 699\n",
      "saving 699\n",
      " Process 700\n",
      " Process 701\n",
      " Process 702\n",
      " Process 703\n",
      "******************Value Error 703****************************\n",
      " Process 705\n",
      " Process 706\n",
      " Process 707\n",
      " Process 708\n",
      " Process 709\n",
      "saving 709\n",
      " Process 710\n",
      " Process 711\n",
      "Unknown Date format 0    February 1795\n",
      "1     October 1875\n",
      "2       March 1947\n",
      "3    November 1852\n",
      "4    November 2000\n",
      "Name: Date_n, dtype: object\n",
      "Unknown Date format 0    February 1795\n",
      "1     October 1875\n",
      "2       March 1947\n",
      "3    November 1852\n",
      "4    November 2000\n",
      "Name: Date_n, dtype: object\n",
      "Unknown Date format 0    February 1795\n",
      "1     October 1875\n",
      "2       March 1947\n",
      "3    November 1852\n",
      "4    November 2000\n",
      "Name: Date_n, dtype: object\n",
      "Unknown Date format 0    February 1795\n",
      "1     October 1875\n",
      "2       March 1947\n",
      "3    November 1852\n",
      "4    November 2000\n",
      "Name: Date_n, dtype: object\n",
      " Process 712\n",
      " Process 713\n",
      " Process 714\n",
      " Process 715\n",
      " Process 716\n",
      " Process 717\n",
      " Process 718\n",
      " Process 719\n",
      "saving 719\n",
      " Process 720\n",
      " Process 721\n",
      " Process 722\n",
      " Process 723\n",
      " Process 724\n",
      " Process 725\n",
      " Process 726\n",
      " Process 727\n",
      " Process 728\n",
      " Process 729\n",
      "saving 729\n",
      " Process 730\n",
      " Process 731\n",
      " Process 732\n",
      " Process 733\n",
      " Process 734\n",
      " Process 735\n",
      " Process 736\n",
      " Process 737\n",
      " Process 738\n",
      " Process 739\n",
      "saving 739\n",
      " Process 740\n",
      " Process 741\n",
      " Process 742\n",
      " Process 743\n",
      " Process 744\n",
      " Process 745\n",
      " Process 746\n",
      " Process 747\n",
      " Process 748\n",
      " Process 749\n",
      "saving 749\n",
      " Process 750\n",
      " Process 751\n",
      " Process 752\n",
      " Process 753\n",
      " Process 754\n",
      " Process 755\n",
      " Process 756\n",
      " Process 757\n",
      " Process 758\n",
      " Process 759\n",
      "saving 759\n",
      " Process 760\n",
      " Process 761\n",
      " Process 762\n",
      " Process 763\n",
      " Process 764\n",
      " Process 765\n",
      " Process 766\n",
      " Process 767\n",
      " Process 768\n",
      " Process 769\n",
      "saving 769\n",
      " Process 770\n",
      " Process 771\n",
      " Process 772\n",
      " Process 773\n",
      " Process 774\n",
      " Process 775\n",
      " Process 776\n",
      " Process 777\n",
      " Process 778\n",
      " Process 779\n",
      "saving 779\n",
      " Process 780\n",
      " Process 781\n",
      " Process 782\n",
      " Process 783\n",
      " Process 784\n",
      " Process 785\n",
      " Process 786\n",
      " Process 787\n",
      " Process 788\n",
      " Process 789\n",
      "saving 789\n",
      " Process 790\n",
      " Process 791\n",
      " Process 792\n",
      " Process 793\n",
      " Process 794\n",
      " Process 795\n",
      " Process 796\n",
      " Process 797\n",
      " Process 798\n",
      " Process 799\n",
      "saving 799\n",
      " Process 800\n",
      " Process 801\n",
      " Process 802\n",
      " Process 803\n",
      " Process 804\n",
      " Process 805\n",
      " Process 806\n",
      " Process 807\n",
      " Process 808\n",
      " Process 809\n",
      "saving 809\n",
      " Process 810\n",
      " Process 811\n",
      " Process 812\n",
      " Process 813\n",
      " Process 814\n",
      " Process 815\n",
      " Process 816\n",
      " Process 817\n",
      " Process 818\n",
      " Process 819\n",
      "saving 819\n",
      " Process 820\n",
      " Process 821\n",
      " Process 822\n",
      " Process 823\n",
      " Process 824\n",
      " Process 825\n",
      " Process 826\n",
      " Process 827\n",
      " Process 828\n",
      " Process 829\n",
      "saving 829\n",
      " Process 830\n",
      " Process 831\n",
      " Process 832\n",
      " Process 833\n",
      " Process 834\n",
      " Process 835\n",
      " Process 836\n",
      " Process 837\n",
      " Process 838\n",
      " Process 839\n",
      "saving 839\n",
      " Process 840\n",
      " Process 841\n",
      " Process 842\n",
      " Process 843\n",
      " Process 844\n",
      " Process 845\n",
      " Process 846\n",
      " Process 847\n",
      " Process 848\n",
      " Process 849\n",
      "saving 849\n",
      " Process 850\n",
      " Process 851\n",
      " Process 852\n",
      " Process 853\n",
      " Process 854\n",
      " Process 855\n",
      " Process 856\n",
      " Process 857\n",
      " Process 858\n",
      " Process 859\n",
      "saving 859\n",
      " Process 860\n",
      " Process 861\n",
      " Process 862\n",
      " Process 863\n",
      " Process 864\n",
      " Process 865\n",
      " Process 866\n",
      " Process 867\n",
      " Process 868\n",
      " Process 869\n",
      "saving 869\n",
      " Process 870\n",
      " Process 871\n",
      " Process 872\n",
      " Process 873\n",
      " Process 874\n",
      " Process 875\n",
      " Process 876\n",
      " Process 877\n",
      " Process 878\n",
      " Process 879\n",
      "saving 879\n",
      " Process 880\n",
      " Process 881\n",
      " Process 882\n",
      " Process 883\n",
      " Process 884\n",
      " Process 885\n",
      " Process 886\n",
      " Process 887\n",
      " Process 888\n",
      " Process 889\n",
      "saving 889\n",
      " Process 890\n",
      " Process 891\n",
      " Process 892\n",
      " Process 893\n",
      " Process 894\n",
      " Process 895\n",
      " Process 896\n",
      " Process 897\n",
      " Process 898\n",
      " Process 899\n",
      "saving 899\n",
      " Process 900\n",
      " Process 901\n",
      " Process 902\n",
      " Process 903\n",
      " Process 904\n",
      " Process 905\n",
      " Process 906\n",
      " Process 907\n",
      " Process 908\n",
      " Process 909\n",
      "saving 909\n",
      " Process 910\n",
      " Process 911\n",
      " Process 912\n",
      " Process 913\n",
      " Process 914\n",
      " Process 915\n",
      " Process 916\n",
      " Process 917\n",
      " Process 918\n",
      " Process 919\n",
      "saving 919\n",
      " Process 920\n",
      " Process 921\n",
      " Process 922\n",
      " Process 923\n",
      " Process 924\n",
      " Process 925\n",
      " Process 926\n",
      " Process 927\n",
      " Process 928\n",
      " Process 929\n",
      "saving 929\n",
      " Process 930\n",
      " Process 931\n",
      " Process 932\n",
      " Process 933\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError\n",
    "from tqdm.notebook import tqdm\n",
    "# table_loader = TableLoader(table_name='wikitable', split='validation', use_sample=True, small_test=False)\n",
    "table_loader = TableLoader(table_name='wikitable', split='test', use_sample=False, small_test=False)\n",
    "# model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-bLZSHx4pKfPRZkYyIyyvUHSEjrlqj5sh2QIsxOM23yJnyoGD\", temperature=0.01)\n",
    "# save_path = f\"../result/final_answer/wikitable_row_add_2times_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "save_path = f\"../result/final_answer/wikitable_row_add_2times_06-12_01-45-00.csv\"\n",
    "\n",
    "# reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "# template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "template = \"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table:\n",
    "{table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Query: {query}\n",
    "Think step by step and answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "\"\"\" )\n",
    "sample_k = 3\n",
    "# Task: answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\n",
    "\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "# for sample_n in range(3):\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i =  1000\n",
    "with tqdm(total=len(table_loader.dataset) - 1000, desc=f\"Processing\",ncols=1500) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        try:\n",
    "            sample = table_loader.normalize_table(\n",
    "                                table_loader.dataset[i])\n",
    "            col_name, col_schema = parse_output(schema_information.loc[sample['table']['id']]['schema'])\n",
    "            sample = add_random_rows(sample, col_schema, times=2)\n",
    "            all_tokens = 0\n",
    "            all_queries = []\n",
    "            formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "            sample_data = formatter.get_sample_data(sample_type='random', k=sample_k, query=sample['query'])\n",
    "            with get_openai_callback() as cb:\n",
    "                llm_chain = LLMChain(llm=model, prompt=step_back_prompt_wiki, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                    all_queries.append(batch_pred[0]['text'].strip())\n",
    "                # llm_chain = LLMChain(llm=model, prompt=disambiguous_prompt_wiki, verbose=False)\n",
    "                # batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                # all_queries.append(batch_pred[0]['text'].strip())\n",
    "                llm_chain = LLMChain(llm=model, prompt=decompose_prompt_wiki, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "                # print(all_queries)\n",
    "            all_tokens += cb.total_tokens\n",
    "            all_queries = list(set(all_queries))\n",
    "            args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "            # print(args_list)\n",
    "            ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "            results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0] ]\n",
    "            all_tokens += sum([res[1] for res in ans_from_B])\n",
    "            #With answer\n",
    "            # results= []\n",
    "            with get_openai_callback() as cb:\n",
    "                imp_input = scene_A(sample['query'], sample, sample_k, False)\n",
    "                llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "            # print(batch_pred[0])\n",
    "            all_tokens += cb.total_tokens\n",
    "            # print('ALL TOKENS', all_tokens)\n",
    "            ids.append(sample['id'])\n",
    "            labels.append(sample['query'])\n",
    "            outputs.append(batch_pred[0]['text'])\n",
    "            tokens.append(all_tokens)\n",
    "            extra_quries.append(';'.join(all_queries))\n",
    "            if (i + 1) % 10 == 0:\n",
    "                    print(f'saving {i}')\n",
    "                    save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "                    outputs = []\n",
    "                    labels = []\n",
    "                    ids = []\n",
    "                    tokens = []\n",
    "                    extra_quries = []\n",
    "            i += 1\n",
    "            print(f' Process {i}')\n",
    "            pbar.update(1)\n",
    "        \n",
    "        except BadRequestError as e:\n",
    "            print('*************************Bad Request**************')\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "        except ValueError as e:\n",
    "            print(f'******************Value Error {i}****************************')\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241m.\u001b[39mall_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatter' is not defined"
     ]
    }
   ],
   "source": [
    "formatter.all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[87])\n",
    "sample = add_random_rows(sample, col_schema)\n",
    "col_name, col_schema = parse_output(schema_information.loc[sample['table']['id']]['schema'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thought: The player won on Grass surface only once.\\nAnswer: 1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'what is the average percentage of each selection in the polls?'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=model, prompt=step_back_prompt_wiki, verbose=False)\n",
    "llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the box office for Italy in 2013?',\n",
       " 'what is the box office for Australia in 2012?',\n",
       " 'how many countries had at least $1 billion in box office?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thought: The query is looking for the minimum season premiere date that is after June 14, 2010. The next date listed after June 14, 2010 is September 6, 2010.\\nAnswer: 2010-09-06']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03b9f5c42fe4af48ab63cc8f2fdfc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 2901\n"
     ]
    }
   ],
   "source": [
    "##### add residual \n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "# from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError, RateLimitError\n",
    "from tqdm.notebook import tqdm\n",
    "table_loader = TableLoader(table_name='wikitable', split='test', use_sample=False, small_test=False)\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-bLZSHx4pKfPRZkYyIyyvUHSEjrlqj5sh2QIsxOM23yJnyoGD\", temperature=0.01)\n",
    "# save_path = f\"../result/final_answer/wikitable_row_add_06-03_01-43-05.csv\"\n",
    "sample_k = 3\n",
    "data = pd.read_csv(save_path)\n",
    "\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i = 0\n",
    "with tqdm(total=len(table_loader.dataset) - 0, desc=f\"Processing\",ncols=150) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        if table_loader.dataset[i]['id'] in list(data['ids']):\n",
    "            i += 1\n",
    "        else:\n",
    "            try:\n",
    "                sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "                col_name, col_schema = parse_output(schema_information.loc[sample['table']['id']]['schema'])\n",
    "                sample = add_random_rows(sample, col_schema)\n",
    "                all_tokens = 0\n",
    "                all_queries = []\n",
    "                formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "                sample_data = formatter.get_sample_data(sample_type='random', k=sample_k, query=sample['query'])\n",
    "                with get_openai_callback() as cb:\n",
    "                    llm_chain = LLMChain(llm=model, prompt=step_back_prompt_wiki, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                        all_queries.append(batch_pred[0]['text'].strip())\n",
    "                    # llm_chain = LLMChain(llm=model, prompt=disambiguous_prompt_wiki, verbose=False)\n",
    "                    # batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    # all_queries.append(batch_pred[0]['text'].strip())\n",
    "                    llm_chain = LLMChain(llm=model, prompt=decompose_prompt_wiki, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "                    # print(all_queries)\n",
    "                all_tokens += cb.total_tokens\n",
    "                all_queries = list(set(all_queries))\n",
    "                args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "                # print(args_list)\n",
    "                ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "                results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0] ]\n",
    "                all_tokens += sum([res[1] for res in ans_from_B])\n",
    "                                \n",
    "                \n",
    "                with get_openai_callback() as cb:\n",
    "                    imp_input = scene_A(sample['query'], sample, sample_k, False)\n",
    "                    llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "                # print(batch_pred[0])\n",
    "                all_tokens += cb.total_tokens\n",
    "                # print('ALL TOKENS', all_tokens)\n",
    "                ids.append(sample['id'])\n",
    "                labels.append(sample['query'])\n",
    "                outputs.append(batch_pred[0]['text'])\n",
    "                tokens.append(all_tokens)\n",
    "                extra_quries.append(';'.join(all_queries))\n",
    "                i += 1\n",
    "                print(f'process {i}')\n",
    "                if len(outputs) % 10 == 0:\n",
    "                    print(f'saving')\n",
    "                    save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "                    outputs = []\n",
    "                    labels = []\n",
    "                    ids = []\n",
    "                    tokens = []\n",
    "                    extra_quries = []\n",
    "            except RateLimitError as e:\n",
    "                print('*************************Rate limit**************')\n",
    "                pass\n",
    "            # except BadRequestError as e:\n",
    "            #     print('*************************Bad Request**************')\n",
    "            #     i += 1\n",
    "            # except ValueError as e:\n",
    "            #     print(f'******************Value Error {i}****************************')\n",
    "            #     i += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2388"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_extra_info(summary_aug, column_aug, composition_aug, columns):\n",
    "    col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "    items, crackets = parse_output(composition_aug, pattern = r'\\d. (.+?): (.+)')\n",
    "    assert len(items) == len(col_names)\n",
    "    extra_col_info = []\n",
    "    for i_c in range(len(col_names)):\n",
    "        if col_names[i_c] in columns:\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]} {crackets[i_c]}')\n",
    "            \n",
    "    extra_col_info.append('row_number: row number in the original table')\n",
    "    return summary_aug + '\\n'.join(extra_col_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
