{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, re, argparse\n",
    "import unicodedata\n",
    "from codecs import open\n",
    "from math import isnan, isinf\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "################ String Normalization ################\n",
    "\n",
    "def normalize(x):\n",
    "    if not isinstance(x, str):\n",
    "        x = x.decode('utf8', errors='ignore')\n",
    "    # Remove diacritics\n",
    "    x = ''.join(c for c in unicodedata.normalize('NFKD', x)\n",
    "                if unicodedata.category(c) != 'Mn')\n",
    "    # Normalize quotes and dashes\n",
    "    x = re.sub(r\"[‘’´`]\", \"'\", x)\n",
    "    x = re.sub(r\"[“”]\", \"\\\"\", x)\n",
    "    x = re.sub(r\"[‐‑‒–—−]\", \"-\", x)\n",
    "    while True:\n",
    "        old_x = x\n",
    "        # Remove citations\n",
    "        x = re.sub(r\"((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[•♦†‡*#+])*$\", \"\", x.strip())\n",
    "        # Remove details in parenthesis\n",
    "        x = re.sub(r\"(?<!^)( \\([^)]*\\))*$\", \"\", x.strip())\n",
    "        # Remove outermost quotation mark\n",
    "        x = re.sub(r'^\"([^\"]*)\"$', r'\\1', x.strip())\n",
    "        if x == old_x:\n",
    "            break\n",
    "    # Remove final '.'\n",
    "    if x and x[-1] == '.':\n",
    "        x = x[:-1]\n",
    "    # Collapse whitespaces and convert to lower case\n",
    "    x = re.sub(r'\\s+', ' ', x, flags=re.U).lower().strip()\n",
    "    return x\n",
    "\n",
    "\n",
    "################ Value Types ################\n",
    "\n",
    "class Value(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Should be populated with the normalized string\n",
    "    _normalized = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def match(self, other):\n",
    "        \"\"\"Return True if the value matches the other value.\n",
    "        Args:\n",
    "            other (Value)\n",
    "        Returns:\n",
    "            a boolean\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def normalized(self):\n",
    "        return self._normalized\n",
    "\n",
    "\n",
    "class StringValue(Value):\n",
    "\n",
    "    def __init__(self, content):\n",
    "        assert isinstance(content, str)\n",
    "        self._normalized = normalize(content)\n",
    "        self._hash = hash(self._normalized)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, StringValue) and self.normalized == other.normalized\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'S' + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        return self.normalized == other.normalized\n",
    "\n",
    "\n",
    "class NumberValue(Value):\n",
    "\n",
    "    def __init__(self, amount, original_string=None):\n",
    "        assert isinstance(amount, (int, float))\n",
    "        if abs(amount - round(amount)) < 1e-6:\n",
    "            self._amount = int(amount)\n",
    "        else:\n",
    "            self._amount = float(amount)\n",
    "        if not original_string:\n",
    "            self._normalized = str(self._amount)\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        self._hash = hash(self._amount)\n",
    "\n",
    "    @property\n",
    "    def amount(self):\n",
    "        return self._amount\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, NumberValue) and self.amount == other.amount\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('N(%f)' % self.amount) + str([self.normalized])\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        if isinstance(other, NumberValue):\n",
    "            return abs(self.amount - other.amount) < 1e-6\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a number.\n",
    "        Return:\n",
    "            the number (int or float) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(text)\n",
    "        except:\n",
    "            try:\n",
    "                amount = float(text)\n",
    "                assert not isnan(amount) and not isinf(amount)\n",
    "                return amount\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "class DateValue(Value):\n",
    "\n",
    "    def __init__(self, year, month, day, original_string=None):\n",
    "        \"\"\"Create a new DateValue. Placeholders are marked as -1.\"\"\"\n",
    "        assert isinstance(year, int)\n",
    "        assert isinstance(month, int) and (month == -1 or 1 <= month <= 12)\n",
    "        assert isinstance(day, int) and (day == -1 or 1 <= day <= 31)\n",
    "        assert not (year == month == day == -1)\n",
    "        self._year = year\n",
    "        self._month = month\n",
    "        self._day = day\n",
    "        if not original_string:\n",
    "            self._normalized = '{}-{}-{}'.format(\n",
    "                year if year != -1 else 'xx',\n",
    "                month if month != -1 else 'xx',\n",
    "                day if day != '-1' else 'xx')\n",
    "        else:\n",
    "            self._normalized = normalize(original_string)\n",
    "        self._hash = hash((self._year, self._month, self._day))\n",
    "\n",
    "    @property\n",
    "    def ymd(self):\n",
    "        return (self._year, self._month, self._day)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, DateValue) and self.ymd == other.ymd\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self._hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return (('D(%d,%d,%d)' % (self._year, self._month, self._day))\n",
    "                + str([self._normalized]))\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def match(self, other):\n",
    "        assert isinstance(other, Value)\n",
    "        if self.normalized == other.normalized:\n",
    "            return True\n",
    "        if isinstance(other, DateValue):\n",
    "            return self.ymd == other.ymd\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        \"\"\"Try to parse into a date.\n",
    "        Return:\n",
    "            tuple (year, month, date) if successful; otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ymd = text.lower().split('-')\n",
    "            assert len(ymd) == 3\n",
    "            year = -1 if ymd[0] in ('xx', 'xxxx') else int(ymd[0])\n",
    "            month = -1 if ymd[1] == 'xx' else int(ymd[1])\n",
    "            day = -1 if ymd[2] == 'xx' else int(ymd[2])\n",
    "            assert not (year == month == day == -1)\n",
    "            assert month == -1 or 1 <= month <= 12\n",
    "            assert day == -1 or 1 <= day <= 31\n",
    "            return (year, month, day)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "################ Value Instantiation ################\n",
    "\n",
    "def to_value(original_string, corenlp_value=None):\n",
    "    \"\"\"Convert the string to Value object.\n",
    "    Args:\n",
    "        original_string (basestring): Original string\n",
    "        corenlp_value (basestring): Optional value returned from CoreNLP\n",
    "    Returns:\n",
    "        Value\n",
    "    \"\"\"\n",
    "    if isinstance(original_string, Value):\n",
    "        # Already a Value\n",
    "        return original_string\n",
    "    if not corenlp_value:\n",
    "        corenlp_value = original_string\n",
    "    # Number?\n",
    "    amount = NumberValue.parse(corenlp_value)\n",
    "    if amount is not None:\n",
    "        return NumberValue(amount, original_string)\n",
    "    # Date?\n",
    "    ymd = DateValue.parse(corenlp_value)\n",
    "    if ymd is not None:\n",
    "        if ymd[1] == ymd[2] == -1:\n",
    "            return NumberValue(ymd[0], original_string)\n",
    "        else:\n",
    "            return DateValue(ymd[0], ymd[1], ymd[2], original_string)\n",
    "    # String.\n",
    "    return StringValue(original_string)\n",
    "\n",
    "\n",
    "def to_value_list(original_strings, corenlp_values=None):\n",
    "    \"\"\"Convert a list of strings to a list of Values\n",
    "    Args:\n",
    "        original_strings (list[basestring])\n",
    "        corenlp_values (list[basestring or None])\n",
    "    Returns:\n",
    "        list[Value]\n",
    "    \"\"\"\n",
    "    assert isinstance(original_strings, (list, tuple, set))\n",
    "    if corenlp_values is not None:\n",
    "        assert isinstance(corenlp_values, (list, tuple, set))\n",
    "        assert len(original_strings) == len(corenlp_values)\n",
    "        return list(set(to_value(x, y) for (x, y)\n",
    "                        in zip(original_strings, corenlp_values)))\n",
    "    else:\n",
    "        return list(set(to_value(x) for x in original_strings))\n",
    "\n",
    "\n",
    "################ Check the Predicted Denotations ################\n",
    "\n",
    "def check_denotation(target_values, predicted_values):\n",
    "    \"\"\"Return True if the predicted denotation is correct.\n",
    "\n",
    "    Args:\n",
    "        target_values (list[Value])\n",
    "        predicted_values (list[Value])\n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "    # Check size\n",
    "    if len(target_values) != len(predicted_values):\n",
    "        return False\n",
    "    # Check items\n",
    "    for target in target_values:\n",
    "        if not any(target.match(pred) for pred in predicted_values):\n",
    "            return False\n",
    "    return True\n",
    "def tsv_unescape(x):\n",
    "    \"\"\"Unescape strings in the TSV file.\n",
    "    Escaped characters include:\n",
    "        newline (0x10) -> backslash + n\n",
    "        vertical bar (0x7C) -> backslash + p\n",
    "        backslash (0x5C) -> backslash + backslash\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a unicode\n",
    "    \"\"\"\n",
    "    return x.replace(r'\\n', '\\n').replace(r'\\p', '|').replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "def tsv_unescape_list(x):\n",
    "    \"\"\"Unescape a list in the TSV file.\n",
    "    List items are joined with vertical bars (0x5C)\n",
    "\n",
    "    Args:\n",
    "        x (str or unicode)\n",
    "    Returns:\n",
    "        a list of unicodes\n",
    "    \"\"\"\n",
    "    return [tsv_unescape(y) for y in x.split('|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query 改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\n",
      "Query: the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from data_loader import TableLoader\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "from data_loader import TableFormat\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from utils import normalize_schema\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_base=\"https://api.chatanywhere.com.cn/v1\", openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\")\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[130])\n",
    "print(sample['query'])\n",
    "formatter = TableFormat(format='none', data=sample)\n",
    "# schema_information = pd.read_csv(f\"result/aug/tabfact_test_schema.csv\", index_col='table_id')\n",
    "# formatter.data = normalize_schema(formatter.data, schema_information.loc[sample['id']]['schema'])\n",
    "q = \"\"\"making question in cloze style and decompse question into continuity smaller question.\"\"\"\n",
    "amb_instruction = PromptTemplate(input_variables=['query'], template= q + \n",
    "\"\"\"\n",
    "Q : {query}\n",
    "\"\"\"\n",
    ")\n",
    "[\"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\"]\n",
    "\n",
    "step_back_instruction = PromptTemplate(input_variables=['query'], template= \"\"\"Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\n",
    "Query: {query}\"\"\")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "llm_chain = LLMChain(llm=model, prompt=step_back_instruction, verbose=True)\n",
    "pre_instruction = PromptTemplate(input_variables=[\"query\"], template=\n",
    "\"\"\"\n",
    "You are an expert at converting user questions into sub-questions. \n",
    "Think step by step to answer this question， and provide sub-questions for knowledge that you need. Split the queries with ’;’ and end the queries with ’**’\n",
    "Question: {query}\n",
    "Answer: \"\"\")\n",
    "# \n",
    "\n",
    "output = llm_chain.invoke(input = {\"query\" : sample['query']}, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back question: Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\"the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\",]\n",
    "task_examples = [\"query rewrite\", \"query decompose\", \"query ambiguity resolve\"]\n",
    "new_query_examples = [\"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "                    #   \"what is the number of listings from barrington?; what is the number of listings from farmington?; what is the number of listings from rochester combined?\",\n",
    "                      ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"task\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "new_query: {new_query}\"\"\")\n",
    "num_k = 2\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Query: {query}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert at converting user questions into database queries. \n",
      "Your task is to effectively decompose complex, multihop questions into simpler, manageable/abstract sub-questions or tasks. This process involves breaking down a question that requires information from multiple sources or steps into broader, more abstract questions that can be answered individually. \n",
      "\n",
      "\n",
      "Query: Is the following query true or false? after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\n",
      "new_query: Who were the winners of the lifetime achievement award after 2005?; Are the winners andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle?\n",
      "\n",
      "\n",
      "Query: what is the number of listings from barrington, farmington, and rochester combined?\n",
      "new_query: What is the number of listings from barrington?; What is the number of listings from farmington?; What is the number of listings from rochester combined?\n",
      "\n",
      "Query: the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "llm_chain = LLMChain(llm=model, prompt=stage_0_prompt, verbose=True)\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[130])\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query']}], return_only_outputs=True)\n",
    "# \"Is the following query true or false?\" +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\n",
      "new_query: Which country uses the US dollar (USD) as its currency?; Which central bank is associated with the US dollar (USD)?\n"
     ]
    }
   ],
   "source": [
    "print(sample['query'])\n",
    "print(batch_pred[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\"Is the following query true or false? after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\",\n",
    "                  \"what is the number of listings from barrington, farmington, and rochester combined?\"]\n",
    "task_examples = [\"query rewrite\", \"query decompose\", \"query ambiguity resolve\"]\n",
    "new_query_examples = [\"Who were the winners of the lifetime achievement award after 2005?; Are the winners andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle?\",\n",
    "                      \"What is the number of listings from barrington?; What is the number of listings from farmington?; What is the number of listings from rochester combined?\",\n",
    "                      ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"summary\", \"subtable\"], template=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "new_query: {new_query}\"\"\")\n",
    "num_k = 2\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"task\": task_examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "stage_0_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are an expert at converting user questions into database queries. \n",
    "Your task is to effectively decompose complex, multihop questions into simpler, manageable/abstract sub-questions or tasks. This process involves breaking down a question that requires information from multiple sources or steps into broader, more abstract questions that can be answered individually. \"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Query: {query}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "# Sub-questions are separated by semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "#                                  template=\"\"\"\n",
    "# Our ultimate goal is to answer query based on the table. Below is a sub-table from the table. Choose from generating a SQLITE3 SELECT SQL code, or directly answering the question. \n",
    "# When generating SQL, you are required to infer the data distribution and format from the data of the sub-table. When answering question, you are required to use information from history.\n",
    "# sub-table: {table}\n",
    "# Query: {claim}\n",
    "# Extra information: {aug}\n",
    "# Output: #output the SQL/answer directly\"\"\")\n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Query: {claim}\n",
    "answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "\"\"\" )\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below are some sub-tables generated by excuting the SQL. You need to understand the logic behind the SQL filtering and the result sub-table. Complete task using the final sub-table. \n",
    "{information}\n",
    "Query: {claim}\n",
    "verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and only return 0 or 1 without any other information at last.\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prompt_manager import get_k_shot_with_aug, get_k_shot_with_answer, view_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "schema_information = pd.read_csv(f\"result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)\n",
    "def end2end(sample):\n",
    "    # stage 0: query augmentation\n",
    "    llm_chain = LLMChain(llm=model, prompt=stage_0_prompt, verbose=True)\n",
    "    stage_0_batch_pred = llm_chain.batch([{\"query\": sample['query']}], return_only_outputs=True)[0]['text'].split(':')[-1]\n",
    "    print(stage_0_batch_pred)\n",
    "    sub_queries = stage_0_batch_pred.split(';')\n",
    "    sub_queries[-1] += \"verify whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\" \n",
    "    # stage 1: column pick\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    for query in sub_queries:\n",
    "        k_shot_prompt = get_k_shot_with_aug()\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=False)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        # stage 2: SQL generation\n",
    "        # k_shot_prompt = row_instruction\n",
    "        # llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=True)\n",
    "        columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "\n",
    "        #     # formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        formatter.data = formatter.data.loc[:, columns]\n",
    "        \n",
    "        extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        # stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "        #                                     'claim': query,\n",
    "        #                                     'aug':  extra_information\n",
    "        #                                     })], return_only_outputs=True)[0]['text']\n",
    "        # print(stage_2_batch_pred)\n",
    "        # stage 3: SQL Excution\n",
    "        # k_shot_prompt = answer_instruction\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "        chain = prompt | model\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        # formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "        demo_chat_history = ChatMessageHistory()\n",
    "        input1 = row_instruction.format(**dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                        'claim': query,\n",
    "                                        'aug':  extra_information,\n",
    "                                        }))\n",
    "        chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "    response = chain_with_message_history.invoke(\n",
    "    {\"input\": input1},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")   \n",
    "    print(response)\n",
    "    sql_type = sqlparse.parse(response.content)[0]\n",
    "    if sql_type == \"SELECT\":\n",
    "        formatter.data = manager.execute_from_df(response.content, formatter.all_data, table_name='DF')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "parsed = sqlparse.parse('the answer is False')[0]\n",
    "sql_type = parsed.get_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert at converting user questions into database queries. \n",
      "Your task is to effectively decompose complex, multihop questions into simpler, manageable/abstract sub-questions or tasks. This process involves breaking down a question that requires information from multiple sources or steps into broader, more abstract questions that can be answered individually. \n",
      "\n",
      "\n",
      "Query: Is the following query true or false? after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\n",
      "new_query: Who were the winners of the lifetime achievement award after 2005?; Are the winners andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle?\n",
      "\n",
      "\n",
      "Query: what is the number of listings from barrington, farmington, and rochester combined?\n",
      "new_query: What is the number of listings from barrington?; What is the number of listings from farmington?; What is the number of listings from rochester combined?\n",
      "\n",
      "Query: after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Who were the winners of the lifetime achievement award after 2005?; Are the winners Andrew Rule, John Silvester, Sandra Harvey, Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
      "    Given the following table and query, you should output columns related to the query or contain useful information about the query.\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1986 - 87 north west counties football league</caption>\n",
      "<thead>\n",
      "<tr><th>  position</th><th>               team</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  goals_for</th><th>  goals_against</th><th>  goal_difference</th><th>  points_1</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>12        </td><td>ashton town        </td><td>24      </td><td>9      </td><td>11    </td><td>29         </td><td>39             </td><td>10               </td><td>15 2      </td></tr>\n",
      "<tr><td>10        </td><td>padiham            </td><td>24      </td><td>8      </td><td>12    </td><td>35         </td><td>39             </td><td>4                </td><td>16        </td></tr>\n",
      "<tr><td>1         </td><td>atherton collieries</td><td>24      </td><td>4      </td><td>4     </td><td>46         </td><td>22             </td><td>+ 24             </td><td>36        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: the team with the most goal for in the 1986 - 87 north west county football league be flixton\n",
      "    Columns: team, goals for\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>list of game of the year awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>                               game</th><th>                                  genre</th><th>                        platform_s</th><th>                 developer_s</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2011  </td><td>the legend of zelda : skyward sword</td><td>action - adventure                     </td><td>wii                               </td><td>nintendo ead , monolith soft</td></tr>\n",
      "<tr><td>2010  </td><td>mass effect 2                      </td><td>action rpg : ( third - person ) shooter</td><td>xbox 360 , windows , playstation 3</td><td>bioware                     </td></tr>\n",
      "<tr><td>2001  </td><td>super smash bros melee             </td><td>fighting                               </td><td>gamecube                          </td><td>hal laboratory , inc        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: a gamecube game loss the award in each of the first 3 year\n",
      "    Columns: year, game, platform (s)\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>ned kelly awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>  best_teenage_young_adult</th><th>                   reader_s_vote</th><th>                       best_non_fiction</th><th>          lifetime_achievement</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1996  </td><td>na                        </td><td>na                              </td><td>na                                     </td><td>jon cleary                    </td></tr>\n",
      "<tr><td>1997  </td><td>na                        </td><td>na                              </td><td>how to write crime edited by marele day</td><td>alan yates (aka carter brown )</td></tr>\n",
      "<tr><td>2001  </td><td>na                        </td><td>bleeding hearts by lindy cameron</td><td>na                                     </td><td>professor stephen knight      </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim:  Who were the winners of the lifetime achievement award after 2005?\n",
      "    Extra information: The table shows the recipients of different categories of the Ned Kelly Awards for the years 1996, 1997, and 2001.\n",
      "    Columns: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, lifetime achievement\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
      "    Given the following table and query, you should output columns related to the query or contain useful information about the query.\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1986 - 87 north west counties football league</caption>\n",
      "<thead>\n",
      "<tr><th>  position</th><th>               team</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  goals_for</th><th>  goals_against</th><th>  goal_difference</th><th>  points_1</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>12        </td><td>ashton town        </td><td>24      </td><td>9      </td><td>11    </td><td>29         </td><td>39             </td><td>10               </td><td>15 2      </td></tr>\n",
      "<tr><td>10        </td><td>padiham            </td><td>24      </td><td>8      </td><td>12    </td><td>35         </td><td>39             </td><td>4                </td><td>16        </td></tr>\n",
      "<tr><td>1         </td><td>atherton collieries</td><td>24      </td><td>4      </td><td>4     </td><td>46         </td><td>22             </td><td>+ 24             </td><td>36        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: the team with the most goal for in the 1986 - 87 north west county football league be flixton\n",
      "    Columns: team, goals for\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>list of game of the year awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>                               game</th><th>                                  genre</th><th>                        platform_s</th><th>                 developer_s</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2011  </td><td>the legend of zelda : skyward sword</td><td>action - adventure                     </td><td>wii                               </td><td>nintendo ead , monolith soft</td></tr>\n",
      "<tr><td>2010  </td><td>mass effect 2                      </td><td>action rpg : ( third - person ) shooter</td><td>xbox 360 , windows , playstation 3</td><td>bioware                     </td></tr>\n",
      "<tr><td>2001  </td><td>super smash bros melee             </td><td>fighting                               </td><td>gamecube                          </td><td>hal laboratory , inc        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: a gamecube game loss the award in each of the first 3 year\n",
      "    Columns: year, game, platform (s)\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>ned kelly awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>  best_teenage_young_adult</th><th>                   reader_s_vote</th><th>                       best_non_fiction</th><th>          lifetime_achievement</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1996  </td><td>na                        </td><td>na                              </td><td>na                                     </td><td>jon cleary                    </td></tr>\n",
      "<tr><td>1997  </td><td>na                        </td><td>na                              </td><td>how to write crime edited by marele day</td><td>alan yates (aka carter brown )</td></tr>\n",
      "<tr><td>2001  </td><td>na                        </td><td>bleeding hearts by lindy cameron</td><td>na                                     </td><td>professor stephen knight      </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim:  Are the winners Andrew Rule, John Silvester, Sandra Harvey, Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle?verify whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "    Extra information: The table shows the recipients of different categories of the Ned Kelly Awards for the years 1996, 1997, and 2001.\n",
      "    Columns: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, best_teenage_young_adult, reader's_vote, best_non_fiction, lifetime_achievement\n",
      "content=\"SELECT \\n    CASE\\n        WHEN lifetime_achievement LIKE '%Andrew Rule%' THEN 1\\n        WHEN lifetime_achievement LIKE '%John Silvester%' THEN 1\\n        WHEN lifetime_achievement LIKE '%Sandra Harvey%' THEN 1\\n        WHEN lifetime_achievement LIKE '%Lindsay Simpson%' THEN 1\\n        WHEN best_non_fiction LIKE '%Marele Day%' THEN 1\\n        WHEN lifetime_achievement LIKE '%Shane Maloney%' THEN 1\\n        WHEN lifetime_achievement LIKE '%Peter Doyle%' THEN 1\\n        ELSE 0\\n    END\\nFROM DF;\" response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 465, 'total_tokens': 587}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=True, small_test=True)\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[2])\n",
    "end2end(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=\"true\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__83cde5e136ad42dc857aeb7bf791dd3a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "chat = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"\"\"Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Query: {claim}\n",
    "answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "\"\"\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RunnableWithMessageHistory\ninput_messages_key\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[1;32m      3\u001b[0m demo_ephemeral_chat_history_for_chain \u001b[38;5;241m=\u001b[39m ChatMessageHistory()\n\u001b[0;32m----> 5\u001b[0m chain_with_message_history \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableWithMessageHistory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_ephemeral_chat_history_for_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSQL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/langchain_core/runnables/history.py:325\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory.__init__\u001b[0;34m(self, runnable, get_session_history, input_messages_key, output_messages_key, history_messages_key, history_factory_config, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# If not provided, then we'll use the default session_id field\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     _config_specs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    315\u001b[0m         ConfigurableFieldSpec(\n\u001b[1;32m    316\u001b[0m             \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m         ),\n\u001b[1;32m    323\u001b[0m     ]\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_session_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_messages_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_messages_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_messages_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_factory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_config_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/langchain_core/runnables/base.py:4246\u001b[0m, in \u001b[0;36mRunnableBindingBase.__init__\u001b[0;34m(self, bound, kwargs, config, config_factories, custom_input_type, custom_output_type, **other_kwargs)\u001b[0m\n\u001b[1;32m   4218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   4219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4220\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4229\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_kwargs: Any,\n\u001b[1;32m   4230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a RunnableBinding from a runnable and kwargs.\u001b[39;00m\n\u001b[1;32m   4232\u001b[0m \n\u001b[1;32m   4233\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4244\u001b[0m \u001b[38;5;124;03m        **other_kwargs: Unpacked into the base class.\u001b[39;00m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4246\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[1;32m   4247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_factories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_factories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_input_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_input_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_output_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_output_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;66;03m# if we don't explicitly set config to the TypedDict here,\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;66;03m# the pydantic init above will strip out any of the \"extra\"\u001b[39;00m\n\u001b[1;32m   4257\u001b[0m     \u001b[38;5;66;03m# fields even though total=False on the typed dict.\u001b[39;00m\n\u001b[1;32m   4258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RunnableWithMessageHistory\ninput_messages_key\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
    "    input_messages_key=[\"table\", \"claim\", \"SQL\"],\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" in French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 39, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"table\": \"test-table\", \"claim\": \"test-claim\", \"SQL\": \"test-SQL\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 74, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prompt_manager import get_k_shot_with_aug, get_k_shot_with_answer, view_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "schema_information = pd.read_csv(f\"result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)\n",
    "def end2end(sample):\n",
    "    # stage 0: query augmentation\n",
    "    llm_chain = LLMChain(llm=model, prompt=stage_0_prompt, verbose=True)\n",
    "    stage_0_batch_pred = llm_chain.batch([{\"query\": sample['query']}], return_only_outputs=True)[0]['text'].split(':')[-1]\n",
    "    print(stage_0_batch_pred)\n",
    "    sub_queries = stage_0_batch_pred.split(';')\n",
    "    sub_queries[-1] += \"verify whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\" \n",
    "    # stage 1: column pick\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    for query in sub_queries:\n",
    "        k_shot_prompt = get_k_shot_with_aug()\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=True)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        # stage 2: SQL generation\n",
    "        # k_shot_prompt = row_instruction\n",
    "        # llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=True)\n",
    "        # columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "\n",
    "        #     # formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        # formatter.data = formatter.data.loc[:, columns]\n",
    "        \n",
    "        # extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        # stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "        #                                     'claim': query,\n",
    "        #                                     'aug':  extra_information\n",
    "        #                                     })], return_only_outputs=True)[0]['text']\n",
    "        # print(stage_2_batch_pred)\n",
    "        # stage 3: SQL Excution\n",
    "        # k_shot_prompt = answer_instruction\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "        chain = prompt | model\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "        demo_chat_history = ChatMessageHistory()\n",
    "        input1 = row_instruction.format(**dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                        'claim': query,\n",
    "                                        'aug':  extra_information,\n",
    "                                        }))\n",
    "        chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "    chain_with_message_history.invoke(\n",
    "    {\"input\": input1},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL composition\n",
    "\n",
    "Logic:\n",
    "Transform the input to the grammarly correct and executable SQL\n",
    "\n",
    "- 1. RULE based\n",
    "- 2. LLM based \n",
    "\n",
    "```\n",
    "\n",
    "kanji<DELETE>, name<KEEP>, builder<GROUP BY>, laid down<DELETE>, launched<DELETE>, completed<DELETE>\n",
    "\n",
    "```\n",
    "\n",
    "TODO: where部分如何操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04-02_06-08-10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "import string\n",
    "import datetime\n",
    "random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k=20)\n",
    "datetime.datetime.now().strftime('%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SELECT 'SELECT a', ()>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlbuilder.query import SELECT, C, T\n",
    "from sqlbuilder.dummy import dummy_connection, dummy_context\n",
    "str(SELECT(C.column_name, C.another_column, 123, 'abc').FROM(T.table_name)._as_sql(dummy_connection, dummy_context))\n",
    "a='test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round<KEEP>\n",
      "round KEEP\n",
      " clubs remaining<KEEP>\n",
      " clubs remaining KEEP\n",
      " clubs involved<KEEP>\n",
      " clubs involved KEEP\n",
      " winners from previous round<KEEP>\n",
      " winners from previous round KEEP\n",
      " new entries this round<DELETE>\n",
      " new entries this round DELETE\n",
      " leagues entering at this round<KEEP>\n",
      " leagues entering at this round KEEP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_sample = 'round<KEEP> clubs remaining<KEEP> clubs involved<KEEP> winners from previous round<KEEP> new entries this round<DELETE> leagues entering at this round<KEEP>'\n",
    "matches = re.finditer(r'([^<]*)<([^\\s>]*)>', test_sample)\n",
    "for match in matches:\n",
    "    print(match.group(0))\n",
    "    # print(match.group[0])\n",
    "    print(match.group(1), match.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name builder\n",
      "GROUP BY builder\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sqlparse\n",
    "def format_sql(output:str):\n",
    "    \"\"\"\n",
    "    Format subtable output into SQL\n",
    "    \"\"\"\n",
    "    items = output.split(',')\n",
    "    AGG = ['COUNT', 'AVG', 'SUM', 'MAX', 'MIN', 'KEEP']\n",
    "    complex = ['GROUP BY', 'ORDER BY']\n",
    "    select_content = []\n",
    "    complex_content = []\n",
    "    for text in items:\n",
    "        match = re.search(r'(\\w+)\\s*<([^>]*)>', text)\n",
    "        item = match.group(1).strip()\n",
    "        dml = match.group(2).strip()\n",
    "        if dml in AGG:\n",
    "            if dml == 'KEEP':\n",
    "                select_content.append(f'{item}')\n",
    "            else:\n",
    "                select_content.append(dml + f'({item})')\n",
    "        if dml in complex:\n",
    "            # if dml == 'GROUP BY':\n",
    "            #     complex_content.append(dml + f' {item}')\n",
    "            #     select_content.append(f'{item}')\n",
    "            complex_content.append(dml + f' {item}')\n",
    "            select_content.append(f'{item}')\n",
    "    return sqlparse.format('SELECT ' + ' '.join(select_content) + ' '+' '.join(complex_content), keyword_case='upper', reindent=True)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT name builder\\nGROUP BY builder'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.format_sql(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine \n",
    "from sqlalchemy.orm import Session,sessionmaker\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=True)\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DML 'select' at 0x7FAB15075240>, <Whitespace ' ' at 0x7FAB15074C40>, <Wildcard '*' at 0x7FAB15074D00>, <Whitespace ' ' at 0x7FAB15074F40>, <Keyword 'from' at 0x7FAB15074EE0>, <Whitespace ' ' at 0x7FAB15074160>, <Identifier 'DF' at 0x7FAB14CDCCF0>, <Punctuation ';' at 0x7FAB150751E0>]\n"
     ]
    }
   ],
   "source": [
    "for i in sqlparse.parse('select * from DF;'):\n",
    "    print(i.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "from typing import List, Any\n",
    "from sqlalchemy import text\n",
    "def __sql_parse(sql):\n",
    "        sql = sql.strip()\n",
    "        parsed = sqlparse.parse(sql)[0]\n",
    "        sql_type = parsed.get_type()\n",
    "\n",
    "        table_name = parsed.get_name()\n",
    "\n",
    "        first_token = parsed.token_first(skip_ws=True, skip_cm=False)\n",
    "        ttype = first_token.ttype\n",
    "        print(\n",
    "            f\"SQL:{sql}, ttype:{ttype}, sql_type:{sql_type}, table:{table_name}\"\n",
    "        )\n",
    "        return parsed, ttype, sql_type, table_name \n",
    "    \n",
    "def _query(query: str, session, fetch: str = \"all\"):\n",
    "        \"\"\"Run a SQL query and return the results as a list of tuples.\n",
    "\n",
    "        Args:\n",
    "            query (str): SQL query to run\n",
    "            fetch (str): fetch type\n",
    "        \"\"\"\n",
    "        result: List[Any] = []\n",
    "\n",
    "        print(f\"Query[{query}]\")\n",
    "        if not query:\n",
    "            return result\n",
    "        cursor = session.execute(text(query))\n",
    "        if cursor.returns_rows:\n",
    "            if fetch == \"all\":\n",
    "                result = cursor.fetchall()\n",
    "            elif fetch == \"one\":\n",
    "                result = [cursor.fetchone()]\n",
    "            else:\n",
    "                raise ValueError(\"Fetch parameter must be either 'one' or 'all'\")\n",
    "            field_names = tuple(i[0:] for i in cursor.keys())\n",
    "\n",
    "            result.insert(0, field_names)\n",
    "            return result\n",
    "        \n",
    "def get_simple_fields(self, table_name):\n",
    "        \"\"\"Get column fields about specified table.\"\"\"\n",
    "        return _query(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "    \n",
    "def run(command: str, session, fetch: str = \"all\") -> List:\n",
    "    \"\"\"Execute a SQL command and return a string representing the results.\"\"\"\n",
    "    if not command or len(command) < 0:\n",
    "        return []\n",
    "    parsed, ttype, sql_type, table_name = __sql_parse(command)\n",
    "    if ttype == sqlparse.tokens.DML:\n",
    "        if sql_type == \"SELECT\":\n",
    "            return _query(command, session, fetch)\n",
    "        else:\n",
    "            return get_simple_fields(table_name)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "import json\n",
    "with open('./result/data/tabfact_test_04-07_08-05-52.json', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        preds.append(json.loads(l)['pred'])\n",
    "    print(len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coposition augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_manager import get_k_shot_with_answer, view_instruction, row_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "schema_information = pd.read_csv(f\"result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>venue</th>\n",
       "      <th>score</th>\n",
       "      <th>result</th>\n",
       "      <th>competition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>september 4 , 2001</td>\n",
       "      <td>estadio nacional de chile , santiago , chile</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>0 - 2</td>\n",
       "      <td>2002 world cup qualification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>november 20 , 2002</td>\n",
       "      <td>brígido iriarte , caracas , venezuela</td>\n",
       "      <td>1 - 0</td>\n",
       "      <td>1 - 0</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>june 26 , 2007</td>\n",
       "      <td>pueblo nuevo , san cristóbal , venezuela</td>\n",
       "      <td>2 - 1</td>\n",
       "      <td>2 - 2</td>\n",
       "      <td>2007 copa américa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index goal                date  \\\n",
       "0      0    1  september 4 , 2001   \n",
       "1      1    2  november 20 , 2002   \n",
       "2      5    6      june 26 , 2007   \n",
       "\n",
       "                                          venue  score result  \\\n",
       "0  estadio nacional de chile , santiago , chile  0 - 1  0 - 2   \n",
       "1         brígido iriarte , caracas , venezuela  1 - 0  1 - 0   \n",
       "2      pueblo nuevo , san cristóbal , venezuela  2 - 1  2 - 2   \n",
       "\n",
       "                    competition  \n",
       "0  2002 world cup qualification  \n",
       "1                      friendly  \n",
       "2             2007 copa américa  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TableFormat(format='none', data=table_loader.dataset[20], use_sampling=True).data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False)\n",
    "example_data = TableFormat(format='none', data=table_loader.dataset[20], use_sampling=True).data.iloc[:, [0,1,2,3,5]].reset_index(drop=True)\n",
    "example_data['nation'] = TableFormat(format='none', data=table_loader.dataset[130], use_sampling=True).data.iloc[:, 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data['nation'] = TableFormat(format='none', data=table_loader.dataset[130], use_sampling=True).data.iloc[:, 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>venue</th>\n",
       "      <th>score</th>\n",
       "      <th>competition</th>\n",
       "      <th>nation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>september 4 , 2001</td>\n",
       "      <td>estadio nacional de chile , santiago , chile</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>2002 world cup qualification</td>\n",
       "      <td>hungary (hun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>november 20 , 2002</td>\n",
       "      <td>brígido iriarte , caracas , venezuela</td>\n",
       "      <td>1 - 0</td>\n",
       "      <td>friendly</td>\n",
       "      <td>east germany (gdr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>june 26 , 2007</td>\n",
       "      <td>pueblo nuevo , san cristóbal , venezuela</td>\n",
       "      <td>2 - 1</td>\n",
       "      <td>2007 copa américa</td>\n",
       "      <td>poland (pol)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  goal                date                                         venue  \\\n",
       "0    1  september 4 , 2001  estadio nacional de chile , santiago , chile   \n",
       "1    2  november 20 , 2002         brígido iriarte , caracas , venezuela   \n",
       "2    6      june 26 , 2007      pueblo nuevo , san cristóbal , venezuela   \n",
       "\n",
       "   score                   competition              nation  \n",
       "0  0 - 1  2002 world cup qualification       hungary (hun)  \n",
       "1  1 - 0                      friendly  east germany (gdr)  \n",
       "2  2 - 1             2007 copa américa        poland (pol)  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>venue</th>\n",
       "      <th>score</th>\n",
       "      <th>competition</th>\n",
       "      <th>nation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>september 4 , 2001</td>\n",
       "      <td>estadio nacional de chile , santiago , chile</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>2002 world cup qualification</td>\n",
       "      <td>east germany (gdr)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>november 20 , 2002</td>\n",
       "      <td>brígido iriarte , caracas , venezuela</td>\n",
       "      <td>1 - 0</td>\n",
       "      <td>friendly</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>june 26 , 2007</td>\n",
       "      <td>pueblo nuevo , san cristóbal , venezuela</td>\n",
       "      <td>2 - 1</td>\n",
       "      <td>2007 copa américa</td>\n",
       "      <td>hungary (hun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>poland (pol)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  goal                date                                         venue  \\\n",
       "0    1  september 4 , 2001  estadio nacional de chile , santiago , chile   \n",
       "1    2  november 20 , 2002         brígido iriarte , caracas , venezuela   \n",
       "5    6      june 26 , 2007      pueblo nuevo , san cristóbal , venezuela   \n",
       "9  NaN                 NaN                                           NaN   \n",
       "\n",
       "   score                   competition              nation  \n",
       "0  0 - 1  2002 world cup qualification  east germany (gdr)  \n",
       "1  1 - 0                      friendly                 NaN  \n",
       "5  2 - 1             2007 copa américa       hungary (hun)  \n",
       "9    NaN                           NaN        poland (pol)  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([example_data, TableFormat(format='none', data=table_loader.dataset[130], use_sampling=True).data.iloc[:, 1]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_content(k: int = 2):\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False)\n",
    "    Output_examples = [\"\"\"\n",
    "                       \n",
    "                       \"\"\"]\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"summary\", \"output\"], template=\"\"\"\n",
    "    Table: {table}\n",
    "    Output: {example}\"\"\")\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False)\n",
    "    example_data = TableFormat(format='none', data=table_loader.dataset[20], use_sampling=True).data.iloc[:, [0,1,2,3,5]].reset_index(drop=True)\n",
    "    example_data['nation'] = TableFormat(format='none', data=table_loader.dataset[130], use_sampling=True).data.iloc[:, 1].reset_index(drop=True)\n",
    "    examples_dict = [{\"table\": TableFormat(format='none', data=example_data, use_sampling=True).format_html(),\n",
    "                      \"example\": Output_examples[i]} for i in range(k)]\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "    \"\"\"Below is a subtable with rows sampled, your task is to summarize the content and find commonalities in each column.\n",
    "    Refine commonalities about the contents within each table column. The example is below:\"\"\",\n",
    "        suffix=\"\"\"Table: {table}\"\"\",\n",
    "        input_variables=[\"table\"],\n",
    "    )\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_string(k: int = 2):\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False)\n",
    "    Output_examples = [\n",
    "        # \"\"\"leagues_entering_at_this_round: different league name joint with '&' or None value\"\"\",\n",
    "                       \"\"\"\n",
    "    goal: sequential number like 1, 2, 3...\n",
    "    date: date in the format of Y-M-D\n",
    "    venue: venue in the format of location, city, country\n",
    "    score: score number in the format of X-Y\n",
    "    result: result number in the format of X-Y\n",
    "    competition: competition name or friendly\n",
    "    nation: nation name with abbreviation within parentheses\"\"\"]\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"summary\", \"output\"], template=\"\"\"\n",
    "    Table: {table}\n",
    "    Output: {example}\"\"\")\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False)\n",
    "    example_data = TableFormat(format='none', data=table_loader.dataset[20], use_sampling=True).data.iloc[:, [0,1,2,3,5]].reset_index(drop=True)\n",
    "    example_data['nation'] = TableFormat(format='none', data=table_loader.dataset[130], use_sampling=True).data.iloc[:, 1].reset_index(drop=True)\n",
    "    examples_dict = [{\"table\": TableFormat(format='none', data=example_data, use_sampling=True).format_html(),\n",
    "                      \"example\": Output_examples[i]} for i in range(k)]\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "    \"\"\"Below is a subtable with rows sampled, your task is to summarize and synthesize each column in the table, identifying commonalities in the string representations, and ultimately output string format commanalities for each column.\n",
    "    The example is below:\"\"\",\n",
    "        suffix=\"\"\"Table: {table}\"\"\",\n",
    "        input_variables=[\"table\"],\n",
    "    )\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is a subtable with rows sampled, your task is to summarize and synthesize each column in the table, identifying commonalities in the string representations, and ultimately output string format commanalities for each column.\n",
      "    The example is below:\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<thead>\n",
      "<tr><th>  goal</th><th>              date</th><th>                                       venue</th><th>  score</th><th>                 competition</th><th>            nation</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1     </td><td>september 4 , 2001</td><td>estadio nacional de chile , santiago , chile</td><td>0 - 1  </td><td>2002 world cup qualification</td><td>hungary (hun)     </td></tr>\n",
      "<tr><td>2     </td><td>november 20 , 2002</td><td>brígido iriarte , caracas , venezuela       </td><td>1 - 0  </td><td>friendly                    </td><td>east germany (gdr)</td></tr>\n",
      "<tr><td>6     </td><td>june 26 , 2007    </td><td>pueblo nuevo , san cristóbal , venezuela    </td><td>2 - 1  </td><td>2007 copa américa           </td><td>poland (pol)      </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Output: \n",
      "    goal: sequential number like 1, 2, 3...\n",
      "    date: date in the format of Y-M-D\n",
      "    venue: venue in the format of location, city, country\n",
      "    score: score number in the format of X-Y\n",
      "    result: result number in the format of X-Y\n",
      "    competition: competition name or friendly\n",
      "    nation: nation name with abbreviation within parentheses\n",
      "\n",
      "Table: <table>\n",
      "<thead>\n",
      "<tr><th>  rank</th><th>          rider</th><th>               team</th><th>      speed</th><th>      time</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>9     </td><td>mark herbertson</td><td>499cc matchless g50</td><td>95.272 mph </td><td>1:11.17.05</td></tr>\n",
      "<tr><td>2     </td><td>alan oversby   </td><td>500cc norton manx  </td><td>101.863 mph</td><td>1:06.40.30</td></tr>\n",
      "<tr><td>6     </td><td>bob price      </td><td>500cc seeley g50   </td><td>96.890 mph </td><td>1:10.05.64</td></tr>\n",
      "</tbody>\n",
      "</table>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=False, small_test=False)\n",
    "sample = table_loader.normalize_table(table_loader.dataset[38])\n",
    "formatter = TableFormat(format='none',data = sample, use_sampling=True)\n",
    "pre_instruction_com = PromptTemplate(input_variables=[\"table\"], template=\"\"\"\n",
    "Below is a subtable with rows sampled, your task is to summarize and synthesize each column in the table, identifying commonalities in the string representations, and ultimately abstracting a text template.\n",
    "Table: {table}\n",
    "\"\"\")\n",
    "temp = get_k_shot_with_string(k = 1)\n",
    "chain = LLMChain(llm=model, prompt=temp, verbose=True)\n",
    "output  = chain.batch([formatter.format_html()],return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "rank: sequential number like 1, 2, 3...\n",
      "rider: rider's name\n",
      "team: team name\n",
      "speed: speed in mph\n",
      "time: time in the format of H:M:S.SS\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_composition_aug(self, formatter: TableFormat, batch_data, batch_size: int, output_token=False, schema_information=None):\n",
    "        \"\"\"\n",
    "        batch composition augmentation\n",
    "        \"\"\"\n",
    "        pre_instruction_com = PromptTemplate(input_variables=[\"table\"], template=\"\"\"\n",
    "        Below is a subtable with rows sampled, your task is to summarize and synthesize each column in the table, identifying commonalities at the character level, and ultimately abstracting a text template.\n",
    "        You need to output the template as a row in the table. The format should be as followed:\n",
    "        COLUMN_NAME1 | COLUMN_NAME2 ...\n",
    "        COLUMN_FORMAT1 | COLUMN_FORMAT2 ... \n",
    "        \n",
    "        Table: {table}\n",
    "        \"\"\")\n",
    "        com_list = []\n",
    "        llm_chain = LLMChain(\n",
    "            llm=self.llm, prompt=pre_instruction_com, verbose=False)\n",
    "        with get_openai_callback() as cb:\n",
    "            # add schema augmentaion info first\n",
    "            if schema_information is not None:\n",
    "                batch_pred = llm_chain.batch([formatter.load_data_from_dic(batch_data[i], schema_information=schema_information.loc[batch_data[i]['id']]['schema']).format_html(\n",
    "                    batch_data[i]['caption']) for i in range(batch_size)], return_only_outputs=True)\n",
    "            else:\n",
    "                batch_pred = llm_chain.batch([formatter.load_data_from_dic(batch_data[i]).format_html(\n",
    "                    batch_data[i]['caption']) for i in range(batch_size)], return_only_outputs=True)\n",
    "        for i in range(len(batch_pred)):\n",
    "            parts = batch_pred[i]['text']\n",
    "            com_list.append(parts)\n",
    "        if output_token:\n",
    "            logger.info(\n",
    "                f\"Batch Composition Augmentaion  All Tokens: {cb.total_tokens}\")\n",
    "            logger.info(\n",
    "                f\"Batch Composition Augmentaion Tokens Average: {cb.total_tokens / batch_size if batch_size > 0 else 0}\" )\n",
    "        return com_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decomposition 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_manager import get_k_shot_with_answer, view_instruction, row_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name='BAAI/bge-large-en',\n",
    "            model_kwargs={'device': 'cuda:3', 'trust_remote_code': True},\n",
    "            encode_kwargs={'normalize_embeddings': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name='wikitable', split='train', use_sample=False)\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-WZtqZEeuE0Xb6syVghDgAxdwe0ASWLkQRGxl61UI7B9RqNC4\", temperature=0.1)\n",
    "\n",
    "pre_instruction = PromptTemplate(input_variables=[\"query\"], template=\n",
    "\"\"\"\n",
    "Given the question and the answer based on the table, you need to answer whether the information is helpful to answer the original question. Think step by step and only return True/False at last.\n",
    "\"\"\")\n",
    "# llm_chain = LLMChain(llm=model, prompt=pre_instruction, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, answer the question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
      "sub-table: <table>\n",
      "<caption>Portland Timbers (2001–10)</caption>\n",
      "<thead>\n",
      "<tr><th>  Year</th><th>  Division</th><th>            League</th><th>  Regular_Season</th><th>       Playoffs</th><th>       Open_Cup</th><th>  Avg_Attendance</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2002  </td><td>2         </td><td>USL A-League      </td><td>2nd, Pacific    </td><td>1st Round      </td><td>Did not qualify</td><td>6,260           </td></tr>\n",
      "<tr><td>2001  </td><td>2         </td><td>USL A-League      </td><td>4th, Western    </td><td>Quarterfinals  </td><td>Did not qualify</td><td>7,169           </td></tr>\n",
      "<tr><td>2003  </td><td>2         </td><td>USL A-League      </td><td>3rd, Pacific    </td><td>Did not qualify</td><td>Did not qualify</td><td>5,871           </td></tr>\n",
      "<tr><td>2004  </td><td>2         </td><td>USL A-League      </td><td>1st, Western    </td><td>Quarterfinals  </td><td>4th Round      </td><td>5,628           </td></tr>\n",
      "<tr><td>2006  </td><td>2         </td><td>USL First Division</td><td>11th            </td><td>Did not qualify</td><td>3rd Round      </td><td>5,575           </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query: what was the last year where this team was a part of the usl a-league?\n",
      "Output: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'table': '<table>\\n<caption>Portland Timbers (2001–10)</caption>\\n<thead>\\n<tr><th>  Year</th><th>  Division</th><th>            League</th><th>  Regular_Season</th><th>       Playoffs</th><th>       Open_Cup</th><th>  Avg_Attendance</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>2002  </td><td>2         </td><td>USL A-League      </td><td>2nd, Pacific    </td><td>1st Round      </td><td>Did not qualify</td><td>6,260           </td></tr>\\n<tr><td>2001  </td><td>2         </td><td>USL A-League      </td><td>4th, Western    </td><td>Quarterfinals  </td><td>Did not qualify</td><td>7,169           </td></tr>\\n<tr><td>2003  </td><td>2         </td><td>USL A-League      </td><td>3rd, Pacific    </td><td>Did not qualify</td><td>Did not qualify</td><td>5,871           </td></tr>\\n<tr><td>2004  </td><td>2         </td><td>USL A-League      </td><td>1st, Western    </td><td>Quarterfinals  </td><td>4th Round      </td><td>5,628           </td></tr>\\n<tr><td>2006  </td><td>2         </td><td>USL First Division</td><td>11th            </td><td>Did not qualify</td><td>3rd Round      </td><td>5,575           </td></tr>\\n</tbody>\\n</table>',\n",
       "  'claim': 'what was the last year where this team was a part of the usl a-league?',\n",
       "  'text': '2006'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = table_loader.normalize_table(\n",
    "                        table_loader.dataset[0])\n",
    "all_tokens = 0\n",
    "all_queries = []\n",
    "formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "sample_data = formatter.get_sample_data(sample_type='embedding', query=sample['query'], k=5)\n",
    "Zero_shot_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                  template=\"\"\"\n",
    "Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, answer the question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "Output: \"\"\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=model, prompt=Zero_shot_prompt, verbose=True)\n",
    "llm_chain.batch([{\"table\": TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']), \"claim\": sample['query']}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8211"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('–')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2001hhh\n",
       "1    2002hhh\n",
       "2    2003hhh\n",
       "3    2004hhh\n",
       "4    2005hhh\n",
       "5    2006hhh\n",
       "6    2007hhh\n",
       "7    2008hhh\n",
       "8    2009hhh\n",
       "9    2010hhh\n",
       "Name: Year, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter.all_data['Year'].apply(lambda x: x +'hhh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.81666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_to_minutes(time_str):\n",
    "   # 分割字符串以获取小时、分钟和秒\n",
    "   hours, minutes_seconds = time_str.split(\":\")\n",
    "   minutes, seconds = minutes_seconds.split(\".\")\n",
    "   \n",
    "   # 将小时、分钟和秒转换为分钟\n",
    "   total_minutes = int(hours) * 60 + int(minutes) + float(seconds)\n",
    "   \n",
    "   return total_minutes\n",
    "time_to_minutes(\"1:03.49\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2038, 5, 8, 0, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
