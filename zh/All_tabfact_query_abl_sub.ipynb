{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.30 shot加入thought\n",
    "- 6.5晚tabfact_query 测试去掉lieral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/disk1/chatgpt/zh/tabular_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import parse_specific_composition, add_row_number, parse_specific_composition_zh\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name='BAAI/bge-large-en',\n",
    "            model_kwargs={'device': 'cuda:3', 'trust_remote_code': True},\n",
    "            encode_kwargs={'normalize_embeddings': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "new_query_examples = [\n",
    "                      \"How many clubs play for the wru division one east in total?; How many clubs play 22 game for the wru division one east?\",\n",
    "                      \"When did polona hercog partner with alberta brianti?; When did polona hercog partner with stephanie vogt?\",\n",
    "                      \"what's the position gene hartley finish in 1952?; what's the position gene hartley finish in 1953?\",\n",
    "                      ]\n",
    "num_k = 3\n",
    "inds = [1, 5, 26]\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=False).get_sample_data(sample_type='random') for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"table\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": normalised_data[i]['query'],\n",
    "                                    \"table\": examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "decompose_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are capable of converting complex query into sub queries. Based on the table, decompose original query into at most 2 complete sub queries which can solve original query. Output new query directly.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "Decompose query: \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "# model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-WZtqZEeuE0Xb6syVghDgAxdwe0ASWLkQRGxl61UI7B9RqNC4\", temperature=0.7).bind(logprobs=True)\n",
    "schema_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "# string_information = pd.read_csv(f\"../result/aug/{task_name}_{split}_string.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:////media/disk1/chatgpt/zh/tabular_data/db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name=task_name, split='test', use_sample=False, small_test=False)\n",
    "sample = table_loader.normalize_table(table_loader.dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "inds = [8, 19, 33,]\n",
    "num_k = 3\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "normalised_data = [table_loader.normalize_table(table_loader.dataset[inds[i]]) for i in range(num_k)]\n",
    "example_samples = [TableFormat(format='none', data=normalised_data[i], save_embedding=False).get_sample_data(sample_type='random') for i in range(num_k)]\n",
    "examples = [TableFormat.format_html(example_samples[i], normalised_data[i]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\n",
    "    # \"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "    \"which college list be public?\",\n",
    "    \"what is the constuctor of the jean behra?\",\n",
    "    \"what are all the points when jason richardson be their leading scorer that month?\"\n",
    "    # \"how many race did davíd garza pérez participate in each season?\"\n",
    "    ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "New query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": normalised_data[i]['query'],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "step_back_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Based on the table, your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Table: {table}\n",
    "Query: {query}\n",
    "New query:\"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_answer_noinfo(k: int=1):\n",
    "    sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"\n",
    "            ]\n",
    "    thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. 3 is the fewest points they received. \"]\n",
    "    tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "    claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "    # inds from test split\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], template=\n",
    "    \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Query: {claim}\n",
    "Thought: {thought}\n",
    "Answer: {output}\n",
    "    \"\"\")\n",
    "    examples_dict = dict(zip([\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], claims[0], thoughts[0], '0']))\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=[examples_dict],\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\"\"\"Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Think step by step and verify whether the provided claim/query is true or false.\n",
    "You should output in the following format:\n",
    "Thought: your step by step thought\n",
    "Answer: return 0 if the query is false, or 1 if the query true\n",
    "Below is an example.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Query: {query}\"\"\",\n",
    "        input_variables=[\"table\", \"query\", \"SQL\"],\n",
    ")\n",
    "    return prompt_template\n",
    "\n",
    "def get_k_shot_with_answer(k: int=1):\n",
    "    sqls = [\"SELECT DISTINCT Type FROM DF WHERE Type != 'audio';\"\n",
    "            ]\n",
    "    thoughts = [\"Based on the SQL query and the extra information provided, the types include audio or video. Therefore, other than audio, the payload type is video, the claim is True.\"]\n",
    "    tables = [\"<table>\\n<thead>\\n<tr><th> Type </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>video   </td></tr>\\n<tr><td>audio/video   </td></tr>\\n</tbody>\\n</table>\"]\n",
    "    claims = [\"other than audio, the payload type is video.\"]\n",
    "    extras = [\"The payload types for audio include audio, video, and audio/video.\"]\n",
    "    # inds from test split\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"information\", \"claim\", \"thought\", \"output\"], template=\n",
    "    \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Claim: {claim}\n",
    "Thought: {thought}\n",
    "Answer: {output}\n",
    "    \"\"\")\n",
    "    examples_dict = dict(zip([\"SQL\", \"table\", \"information\", \"claim\", \"thought\", \"output\"], [sqls[0], tables[0],extras[0], claims[0], thoughts[0], '1']))\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=[examples_dict],\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\"\"\"Below is a sub-table generated by excuting the corresponding SQL with additional extra information. You need to understand the logic behind the SQL filtering. Think step by step and verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true.\n",
    "You should output in the following format:\n",
    "Thought: your step by step thought\n",
    "Answer: final answer\n",
    "Below is an example.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information:\n",
    "{information}\n",
    "\n",
    "Claim: {query}\n",
    "\"\"\",\n",
    "        input_variables=[\"table\", \"query\", \"SQL\", \"information\"],\n",
    ")\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_k_shot_with_answer(k: int=1):\n",
    "#     sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"\n",
    "#             ]\n",
    "#     thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. 3 is the fewest points they received. \"]\n",
    "#     tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "#     claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "#     # inds from test split\n",
    "#     examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], template=\n",
    "#     \"\"\"\n",
    "# SQL Excuted: \n",
    "# ```{SQL}```\n",
    "# Sub-table: {table}\n",
    "# Thought: {thought}\n",
    "# Query: {claim}\n",
    "# Answer: {output}\n",
    "#     \"\"\")\n",
    "#     examples_dict = dict(zip([\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], claims[0], thoughts[0], '0']))\n",
    "#     prompt_template = FewShotPromptTemplate(\n",
    "#         examples=[examples_dict],\n",
    "#         example_prompt=examples_prompt,\n",
    "#         prefix=\"\"\"Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Think step by step and verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true.\n",
    "# You should output in the following format:\n",
    "# Thought: your step by step thought\n",
    "# Answer: final answer\n",
    "# Below is an example.\"\"\",\n",
    "#         suffix=\n",
    "#         \"\"\"\n",
    "# SQL Excuted: \n",
    "# ```{SQL}```\n",
    "# Sub-table: {table}\n",
    "# Extra information:\n",
    "# {information}\n",
    "# Query: {query}\n",
    "# \"\"\",\n",
    "#         input_variables=[\"table\", \"query\", \"SQL\", \"information\"],\n",
    "# )\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "#     table_loader_wiki = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "#     inds = [3, 6, 260, 33]\n",
    "#     Output_examples = [\n",
    "#                        'team, goals_for',\n",
    "#                        'year, game, platform_s',\n",
    "#                        'name, population_density_km_2_, population_2011_census_'\n",
    "#                        'leading_scorer, score, date']\n",
    "#     linking_examples = ['the team -> team, the most goal for -> goals_for',\n",
    "#                         'gamecube -> platform_s, gamecube game -> game, the first 3 year -> year',\n",
    "#                         'alberta -> name, population density -> population_density_km_2_, 4257744 less people -> population_2011_census_, 2011 -> population_2011_census_'\n",
    "#                         'jason richardson -> leading_scorer, month -> date, 23 point per game -> score'\n",
    "#     ]\n",
    "#     examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "#     \"\"\"\n",
    "#     Table: {table}\n",
    "#     Query: {claim}\n",
    "#     Column linking: {linking}\n",
    "#     Columns: {output}\"\"\")\n",
    "    # num_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_aug(k: int=2):\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\n",
    "    Column linking: {linking}\n",
    "    Columns: {output}\"\"\")\n",
    "    examples_dict = []\n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>Hoot Kloot</caption>\\n<thead>\\n<tr><th> Number</th><th> Title</th><th> Directed_by_</th><th> Released_</th></tr>\\n</thead>\\n</table>',\n",
    "                                        \"claim\": 'Mesa Trouble was the last title that sid marcus directed.',\n",
    "                                        \"aug\": \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year. \\n1. Number: The episode number in the series \\n2. Title: The title of the episode \\n3. Directed_by_: The director of the episode \\n4. Released_: The release year of the episode\",\n",
    "                                        \"linking\": \"the last title -> Released_, the last title-> Number, title -> Title, sid marcus -> Directed_by_\",\n",
    "                                        \"output\": \"Released_, Number, Title, Directed_by_\"}])\n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>1943–44 Chicago Black Hawks season</caption>\\n<thead>\\n<tr><th>  num</th><th>       Date</th><th>            Visitor</th><th>  Score</th><th>               Home</th><th>  Record</th></tr>\\n</thead>\\n</table>',\n",
    "                                        \"claim\": 'The difference in score in the december 19th win was 3.',\n",
    "                                        \"aug\": 'The table contains information about the 1943-44 Chicago Black Hawks season, including the date, visitor, score, home team, record, and points for each game. \\n1. num: The game number in the season \\n2. Date: The date of the game\\n3. Vistor: The visiting team\\n4. Score: The final score, with the visitor score listed first\\n5. Home: The home team\\n6. Record: The team win-loss-overtime loss record at the time of the game',\n",
    "                                        \"linking\": 'difference in score -> Score, december 19th -> Date',\n",
    "                                        \"output\": 'Date, Score'}])\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "    Based on the Table below, your task is accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "    Approach this task as follows:\n",
    "    Read the query thoroughly and list every possible link from query term to column in the Table. \n",
    "    Then Based on the column linking, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\"\"\",\n",
    "        input_variables=[\"table\", \"claim\"],\n",
    ")\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_k_shot_with_aug_wiki(k: int=2):\n",
    "    table_loader = TableLoader(table_name='wikitable', split='train', use_sample=True, small_test=False)\n",
    "    examples_dict = []\n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>Hoot Kloot</caption>\\n<thead>\\n<tr><th> Number</th><th> Title</th><th> Directed_by_</th><th> Released_</th></tr>\\n</thead>\\n</table>',\n",
    "                                        \"claim\": table_loader.dataset[95]['question'],\n",
    "                                        \"aug\": \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year. \\n1. Number: The episode number in the series \\n2. Title: The title of the episode \\n3. Directed_by_: The director of the episode \\n4. Released_: The release year of the episode\",\n",
    "                                        \"linking\": \"the last title -> Released_, the last title-> Number, title -> Title, sid marcus -> Directed_by_\",\n",
    "                                        \"output\": \"Released_, Number, Title, Directed_by_\"}])\n",
    "    examples_dict.extend([{\"table\": '<table>\\n<caption>1943–44 Chicago Black Hawks season</caption>\\n<thead>\\n<tr><th>  num</th><th>       Date</th><th>            Visitor</th><th>  Score</th><th>               Home</th><th>  Record</th></tr>\\n</thead>\\n</table>',\n",
    "                                        \"claim\": 'what was the difference in score in the december 19th win?',\n",
    "                                        \"aug\": 'The table contains information about the 1943-44 Chicago Black Hawks season, including the date, visitor, score, home team, record, and points for each game. \\n1. num: The game number in the season \\n2. Date: The date of the game\\n3. Vistor: The visiting team\\n4. Score: The final score, with the visitor score listed first\\n5. Home: The home team\\n6. Record: The team win-loss-overtime loss record at the time of the game',\n",
    "                                        \"linking\": 'difference in score -> Score, december 19th -> Date',\n",
    "                                        \"output\": 'Date, Score'}])\n",
    "    \n",
    "    # \"The table contains information about the Hoot Kloot animated series, including the episode number, title, director, and release year.\"\n",
    "    # \"№<The episode number in the series>\\nTitle<The title of the episode>\\nDirected_by_<The director of the episode>\\nReleased_<The release year of the episode>\"\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\n",
    "    Column linking: {linking}\n",
    "    Columns: {output}\"\"\")\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "    Based on the Table below, your task is accurately output columns related to the query or contain useful information about the query. This process involves linking similar words or semantically similar terms to columns in the Table.\n",
    "    Approach this task as follows:\n",
    "    Read the query and extra information thoroughly and list every possible link from query term to column in the Table. \n",
    "    Then Based on the column linking, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "    Table: {table}\n",
    "    Query: {claim}\"\"\",\n",
    "        input_variables=[\"table\", \"claim\"],\n",
    ")\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "def scene_A(query, sample, k=0, verbose=True):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    \n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    sample_data = formatter.get_sample_data(sample_type='embedding',k=k, query=query)\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_aug(), verbose=verbose)\n",
    "        # summary_aug, column_aug = aug_information.loc[sample['table']['id']]['summary'], aug_information.loc[sample['table']['id']]['column_description'] \n",
    "        # col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        # extra_col_info = []\n",
    "        # for i_c in range(len(col_names)):\n",
    "        #     extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        \n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        if verbose:\n",
    "            print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        if verbose:\n",
    "            print(stage_1_batch_pred)\n",
    "        # extra_cols = formatter.get_sample_column(embeddings, column_aug)\n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')]))\n",
    "        \n",
    "        try: \n",
    "            # formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "            sample_data = add_row_number(sample_data.loc[:, columns])\n",
    "        except:\n",
    "            sample_data = add_row_number(sample_data)\n",
    "        \n",
    "        extra_information = []\n",
    "        # tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        # for col, com in tuples:\n",
    "        #     if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "        #         com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "        #         extra_information.append(col + ':' + com)\n",
    "        #     else:\n",
    "        #         com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "        #         extra_information.append(col + ':' + com)\n",
    "        # extra_information.append('row_number: row number in the original table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data = sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            # 'aug':  summary_aug + '\\nColumn information:\\n' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "        # print(stage_2_batch_pred)\n",
    "    # stage 3: SQL Excution\n",
    "    try: \n",
    "        execute_data = manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')[:20]\n",
    "    except:\n",
    "        execute_data = formatter.all_data[:20]\n",
    "        stage_2_batch_pred = 'SELECT * from DF;'\n",
    "    if len(execute_data) == 0:\n",
    "        return query, stage_2_batch_pred, 'No data from database', cb.total_tokens\n",
    "    return query, stage_2_batch_pred, TableFormat.format_html(data=execute_data, table_caption=sample['table']['caption']), cb.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_blury_string(pred_list):\n",
    "    pred_label = []\n",
    "    for pred in pred_list:\n",
    "        predict_ans = pred.split('\\n')[-1]\n",
    "        if '0' in predict_ans:\n",
    "            predict_ans = '0'\n",
    "        elif '1' in predict_ans:\n",
    "            predict_ans = '1'\n",
    "        else:\n",
    "            predict_ans = '2'\n",
    "        pred_label.append(predict_ans)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)\n",
    "\n",
    "def save_csv(input_list: List[List], label_list: List, file_path):\n",
    "    import pandas as pd\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    assert len(input_list) == len(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(label_list)):\n",
    "        df[label_list[i]] = pd.Series(input_list[i])\n",
    "    if os.path.exists(file_path) and file_path.endswith('.csv'):\n",
    "        df_origin = pd.read_csv(file_path)\n",
    "        df = pd.concat([df_origin, df], axis=0)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整extrainformation的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "# Please provide a clear, complete statement in response to the question. However, if you cannot answer the query based on the sub-table or sub-table returns no data, just output 'Cannot get answer from sub-table'. \n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering and answer the query using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: \n",
    "{table}\n",
    "Query: {claim}\n",
    "Please provide a clear, complete statement in response to the query. If you cannot answer the query based on the sub-table, just return 'Cannot get answer from sub-table'.\"\"\" )\n",
    "def scene_B(query, sample,k=0,  verbose=False):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"Our ultimate goal is to answer query based on the original table. Below we have a sub-table with rows randomly sampled from the original table, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the extra information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "SQL: \"\"\")\n",
    "    \n",
    "    formatter = TableFormat(format='none', data=sample, save_embedding=True, embeddings=embeddings)\n",
    "    formatter.normalize_schema(schema_information.loc[sample['table']['id']]['schema'])\n",
    "    sample_data = formatter.get_sample_data(sample_type='embedding', k=k,query=query)\n",
    "    # get columns\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_aug_wiki(), verbose=verbose)\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        # stage 2: SQL generation\n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = list(set([c.strip() for c in stage_1_batch_pred.split(',')]))\n",
    "        \n",
    "        try: \n",
    "            sample_data = add_row_number(sample_data.loc[:, columns])\n",
    "        except:\n",
    "            sample_data = add_row_number(sample_data)\n",
    "        # extra_information = []\n",
    "        # tuples = parse_specific_composition_zh(composition_information.loc[sample['table']['id']]['composition'], sample_data.columns)\n",
    "        # for col, com in tuples:\n",
    "        #     if len(pd.unique(formatter.all_data[col])) < 6:\n",
    "        #         com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique().astype(str)))})'\n",
    "        #         extra_information.append(col + ':' + com)\n",
    "        #     else:\n",
    "        #         com += f' (Values like {\", \".join(list(formatter.all_data[col].dropna().unique()[:3].astype(str)))}...)'\n",
    "        #         extra_information.append(col + ':' + com)\n",
    "        # extra_information.append('row_number: row number in the original table')\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': TableFormat.format_html(data=sample_data, table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            # 'aug':  summary_aug + '\\n Column information:' + '\\n'.join(extra_information)\n",
    "                                            })], return_only_outputs=True)[0]['text'].replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"―\", \"-\").replace(\"−\", \"-\")\n",
    "    \n",
    "        \n",
    "        # stage 3: SQL Excution\n",
    "        try: \n",
    "            execute_data= manager.execute_from_df(stage_2_batch_pred, add_row_number(formatter.all_data), table_name='DF')[:20]\n",
    "        except:\n",
    "            execute_data = formatter.all_data[:20]\n",
    "            stage_2_batch_pred = 'SELECT * from DF;'\n",
    "        llm_chain = LLMChain(llm=model, prompt=answer_instruction, verbose=verbose)\n",
    "        response = llm_chain.batch([dict({'table': TableFormat.format_html(execute_data, table_caption=sample['table']['caption']),\n",
    "                                                'claim': query,\n",
    "                                                'SQL':  stage_2_batch_pred\n",
    "                                                })], return_only_outputs=True)[0]['text']\n",
    "    # print(\"total_tokens:\", cb.total_tokens)\n",
    "    return response, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0c75de50975e4f278b882fe90da47f2f\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ces.openai.azure.com\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2b219db0d2984f9dae28b651ab8ab3d9\"\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://smsh.openai.azure.com/\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "# os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-35-turbo\"\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    temperature=0.3,\n",
    "    max_retries=8, request_timeout=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb008720e6b4b5ab18d700b261c6870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 1919\n",
      "saving 1929\n",
      "saving 1939\n",
      "saving 1949\n",
      "saving 1959\n",
      "saving 1969\n",
      "saving 1979\n",
      "saving 1989\n"
     ]
    }
   ],
   "source": [
    "##### no info\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "# from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError, RateLimitError\n",
    "from tqdm.notebook import tqdm\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "# model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "#                        openai_api_key=\"sk-WZtqZEeuE0Xb6syVghDgAxdwe0ASWLkQRGxl61UI7B9RqNC4\", temperature=0.01)\n",
    "# save_path = f\"../result/final_answer/tabfact_noaug_0_query_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "save_path = f\"../result/final_answer/tabfact_noaug_0_query_06-10_01-31-32.csv\"\n",
    "\n",
    "# reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "# template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "template = \"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table:\n",
    "{table}\n",
    "Extra information:\n",
    "{information}\n",
    "Query: {query}\n",
    "\n",
    "Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\"\"\" )\n",
    "# Task: answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\n",
    "\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "# for sample_n in range(3):\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i = 1910\n",
    "sample_k = 0\n",
    "with tqdm(total=len(table_loader.dataset) - 1910, desc=f\"Processing\",ncols=150) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        try:\n",
    "            sample = table_loader.normalize_table(\n",
    "                                table_loader.dataset[i])\n",
    "            all_tokens = 0\n",
    "            all_queries = []\n",
    "            formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "            sample_data = formatter.get_sample_data(sample_type='random',k=sample_k, query=sample['query'])\n",
    "            with get_openai_callback() as cb:\n",
    "                llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                    all_queries.append(batch_pred[0]['text'].strip())\n",
    "                llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "            all_tokens += cb.total_tokens\n",
    "            all_queries = list(set(all_queries))\n",
    "            args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "            # print(len(args_list))\n",
    "            ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "            results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0]]\n",
    "            all_tokens += sum([res[1] for res in ans_from_B])\n",
    "            #With answer\n",
    "            # results= []\n",
    "            with get_openai_callback() as cb:\n",
    "                imp_input = scene_A(sample['query'], sample,sample_k,  False)\n",
    "                llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                batch_pred = llm_chain.batch([{\"query\":sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "                # print(batch_pred[0])\n",
    "                all_tokens += cb.total_tokens\n",
    "                # print('ALL TOKENS', all_tokens)\n",
    "                ids.append(i)\n",
    "                labels.append(sample['query'])\n",
    "                outputs.append(batch_pred[0]['text'])\n",
    "                tokens.append(all_tokens)\n",
    "                extra_quries.append(';'.join(all_queries)) \n",
    "                if (i + 1) % 10 == 0:\n",
    "                        print(f'saving {i}')\n",
    "                        save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "                        outputs = []\n",
    "                        labels = []\n",
    "                        ids = []\n",
    "                        tokens = []\n",
    "                        extra_quries = []\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "        except ValueError as e:\n",
    "            print(f'******************Value Error {i}****************************')\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        except BadRequestError as e:\n",
    "            print('*************************Bad Request**************')\n",
    "            i += 1 \n",
    "            print('*************************Rate limit**************')\n",
    "            pass\n",
    "        \n",
    "        \n",
    "# save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "    \n",
    "#With no answer\n",
    "# temp = [f\"\"\"\n",
    "# SQL Excuted for extra information: \n",
    "# ```{res[1]}```\n",
    "# Sub-table for extra information: {res[2]}\"\"\" for res in results if res[2] != 'No data from database']\n",
    "# imp_input = scene_A(sample['query'], sample, False)\n",
    "# llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(temp)}], return_only_outputs=True)\n",
    "# print(batch_pred[0])\n",
    "# outputs.append(batch_pred[0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When did the witchfinder episode happen?', 'When did the sweet dream episode happen?', 'what is the order of the sweet dream episode in the series?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Thought: Since the sub-table does not contain any data, we cannot verify the claim based on the provided information.\\nAnswer: 0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_queries)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (578050692.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. prompt是否添加verrify instruction\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a\n",
    "1. prompt是否添加verrify instruction\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../result/answer/tabfact_test_query_gpt35_0125.csv')\n",
    "data2 = pd.read_csv('../result/answer/tabfact_05-13_08-47-41.csv')\n",
    "data3 = pd.read_csv('./result/answer/tabfact_05-14_13-38-48.csv')\n",
    "data['final_answer'] = data['preds'].apply(lambda x: x.split('Answer:')[-1].strip())\n",
    "data2['final_answer'] = data2['preds'].apply(lambda x: x.split('Answer:')[-1].strip())\n",
    "data3['final_answer'] = data3['preds'].apply(lambda x: x.split('Answer:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(table_loader.dataset)):\n",
    "    if i in list(data['ids']):\n",
    "        i += 1\n",
    "    else:\n",
    "        count+= 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4af03e7776544e69c9fb95bbcca42a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                                                             …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 684\n",
      "process 754\n",
      "process 757\n",
      "process 761\n",
      "process 762\n",
      "process 1330\n"
     ]
    }
   ],
   "source": [
    "##### add residual \n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "# from FlagEmbedding import FlagReranker\n",
    "from openai import BadRequestError, RateLimitError\n",
    "from tqdm.notebook import tqdm\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-bLZSHx4pKfPRZkYyIyyvUHSEjrlqj5sh2QIsxOM23yJnyoGD\", temperature=0.3)\n",
    "save_path = f\"../result/final_answer/tabfact_noaug_0_query_06-10_01-31-32.csv\"\n",
    "\n",
    "data = pd.read_csv(save_path)\n",
    "\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "# template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "template = \"\"\"\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table:\n",
    "{table}\n",
    "Extra information:\n",
    "{information}\n",
    "Query: {query}\n",
    "\n",
    "Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\"\"\" )\n",
    "# Task: answer the last question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# Task: verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\n",
    "\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "# for sample_n in range(3):\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "ids = []\n",
    "extra_quries = []\n",
    "i = 0\n",
    "sample_k = 0\n",
    "with tqdm(total=len(table_loader.dataset), desc=f\"Processing\",ncols=150) as pbar:\n",
    "    while i < len(table_loader.dataset):\n",
    "        if i in list(data['ids']):\n",
    "            i += 1\n",
    "        else:\n",
    "            try:\n",
    "                sample = table_loader.normalize_table(\n",
    "                                table_loader.dataset[i])\n",
    "                all_tokens = 0\n",
    "                all_queries = []\n",
    "                formatter = TableFormat(format='none', data=sample, save_embedding=False)\n",
    "                sample_data = formatter.get_sample_data(sample_type='random',k=sample_k, query=sample['query'])\n",
    "                with get_openai_callback() as cb:\n",
    "                    llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    if batch_pred[0]['text'].strip() != sample['query']:\n",
    "                        all_queries.append(batch_pred[0]['text'].strip())\n",
    "                    llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": TableFormat.format_html(sample_data)}], return_only_outputs=True)\n",
    "                    all_queries.extend([q.strip() for q in batch_pred[0]['text'].split(';')])\n",
    "                all_tokens += cb.total_tokens\n",
    "                all_queries = list(set(all_queries))\n",
    "                args_list = [{\"query\": q, \"sample\": sample, \"k\": sample_k} for q in all_queries]\n",
    "                # print(len(args_list))\n",
    "                ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "                results = [res[0] for res in ans_from_B if 'Cannot get answer from sub-table' not in res[0]]\n",
    "                all_tokens += sum([res[1] for res in ans_from_B])\n",
    "                #With answer\n",
    "                # results= []\n",
    "                with get_openai_callback() as cb:\n",
    "                    imp_input = scene_A(sample['query'], sample,sample_k,  False)\n",
    "                    llm_chain = LLMChain(llm=model, prompt=get_k_shot_with_answer(), verbose=False)\n",
    "                    batch_pred = llm_chain.batch([{\"query\":sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "                    # print(batch_pred[0])\n",
    "                    all_tokens += cb.total_tokens\n",
    "                    # print('ALL TOKENS', all_tokens)\n",
    "                    ids.append(i)\n",
    "                    labels.append(sample['query'])\n",
    "                    outputs.append(batch_pred[0]['text'])\n",
    "                    tokens.append(all_tokens)\n",
    "                    extra_quries.append(';'.join(all_queries))\n",
    "                    print(f'process {i}')\n",
    "                    i += 1\n",
    "                    \n",
    "            #         if (i + 1) % 10 == 0:\n",
    "            #                 print(f'saving {i}')\n",
    "\n",
    "            #         i += 1\n",
    "            #         pbar.update(1)\n",
    "            # except ValueError as e:\n",
    "            #     print(f'******************Value Error {i}****************************')\n",
    "            #     i += 1\n",
    "            #     pbar.update(1)\n",
    "            # except BadRequestError as e:\n",
    "            #     print('*************************Bad Request**************')\n",
    "            #     i += 1\n",
    "            #     pbar.update(1)\n",
    "            except RateLimitError as e:\n",
    "                print('*************************Rate limit**************')\n",
    "                pass\n",
    "            except ValueError as e:\n",
    "                print(f'******************Value Error {i}****************************')\n",
    "                i += 1\n",
    "        \n",
    "            except BadRequestError as e:\n",
    "                print('*************************Bad Request**************')\n",
    "                i += 1 \n",
    "\n",
    "        pbar.update(1)\n",
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids, tokens, extra_quries], ['preds', 'statements','ids', 'tokens', 'extra'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv([outputs, labels, ids], ['preds', 'statements','ids'], file_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (4231422701.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    46: west berkshire brewery 's maggs magnificent mild be its most decorate beer between 1995 and 2009\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "46: west berkshire brewery 's maggs magnificent mild be its most decorate beer between 1995 and 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 ：Based on the information provided, the SQL query filtered for records where the place starts with 't9' and the country is 'united states'. The result of the query shows that there are 3 people who meet these criteria. However, the extra information states that there were actually 4 people who tied for ninth place, and all of them were from the United States.\\n\\nTherefore, the provided claim/query is false. The correct number of people from the United States who tied for ninth place is 4, not 3. \\n\\nFinal answer: 0\n",
    "88: To verify whether the term start for Bashkim Fino is after the term start for Vilson Ahmeti, we need to compare the dates mentioned in the sub-table. From the sub-table:\\n- Vilson Ahmeti's term started on December 10, 1991.\\n- Bashkim Fino's term started on March 11, 1997.\\n\\nSince December comes before March in the calendar year, it is evident that Vilson Ahmeti's term started before Bashkim Fino's term. Therefore, the claim that the term start for Bashkim Fino is after the term start for Vilson Ahmeti is FALSE.\\n\\nHence, the answer is 0\n",
    "72:To verify the claim that the gap between the first and last player being a total of 58.04 is true, we need to calculate the difference between the points of the first and last player based on the given information.\\n\\nGiven:\\n- Total gap between the first and last rank is 18.\\n- Rank of the first player is 1.\\n- Rank of the last player is 19.\\n\\nLet's calculate the points difference between the first and last player:\\npoints difference = (points of last player) - (points of first player)\\n\\nSince the total gap between the first and last rank is 18, we can calculate the points difference as follows:\\npoints difference = 18 * (MAX(points) - MIN(points))\\n\\nGiven that the calculated point gap from the sub-table is 58.04, we can substitute this value into the formula:\\n18 * 58.04 = 1044.72\\n\\nTherefore, the claim that the gap between the first and last player being a total of 58.04 is false.\\n\\nFinal answer: 0\n",
    "76:The SQL query filters for competitors from France with a rank less than 5. Since the sub-table generated from the query shows no data, it means that there were no competitors from France who finished better than 5th place. \\n\\nTherefore, the provided claim that France's competitors all finished better than 5th place is TRUE.\\n\\nFinal answer: 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
