{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_manager import get_k_shot_with_answer, view_instruction, row_instruction\n",
    "import pandas as pd\n",
    "from utils import parse_specific_composition\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from data_loader import TableFormat, TableLoader\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from sqlalchemy import create_engine\n",
    "from executor import SQLManager\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\n",
    "    # \"after 2005 , the winner of the lifetime achievement award be andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle\",\n",
    "                  \"all 12 club play a total of 22 game for the wru division one east\",\n",
    "                #   \"a gamecube game loss the award in each of the first 3 year\",\n",
    "                \"from 1980 to 2011 , apoel bc lose more than 2 time as many game as it win\",\n",
    "                  \"polona hercog 1890partner with alberta brianti after she have stephanie vogt as the partner\",\n",
    "                  ]\n",
    "task_examples = [\"query rewrite\", \"query decompose\", \"query ambiguity resolve\"]\n",
    "new_query_examples = [\n",
    "    # \"Who were the winners of the lifetime achievement award after 2005?;\",\n",
    "                      \"How many clubs play for the wru division one east in total?; How many clubs play 22 game for the wru division one east?;\",\n",
    "                    #   \"a gamecube game loss the award in each of the first 3 year\",\n",
    "                    \"from 1980 to 2011 , how many games did apoel bc lose?; from 1980 to 2011 , how many games did apoel bc win?;\",\n",
    "                      \"When did polona hercog partner with alberta brianti?; When did polona hercog partner with stephanie vogt?\",\n",
    "                      ]\n",
    "num_k = 3\n",
    "inds = [1, 124, 5]\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "examples = [TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_html(table_loader.dataset[inds[i]]['table']['caption']) for i in range(num_k)]\n",
    "\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"table\", \"new_query\"], template=\n",
    "\"\"\"Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                                    \"table\": examples[i],\n",
    "                                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "decompose_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"You are capable of converting complex query into sub-queries. Based on the table, provide at most 2 continuity sub-queries for knowledge that you need. \n",
    "Split the queries with ’;’.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: \n",
    "    \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n",
    "\n",
    "# Sub-questions are separated by semicolons.\n",
    "# answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "#                                     template=\"\"\"\n",
    "# Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "# SQL Excuted: \n",
    "# ```{SQL}```\n",
    "# Sub-table: {table}\n",
    "# Query: {claim}\n",
    "# answer the question given in the query. Only return the string instead of other format information. Do not repeat the question.\n",
    "# \"\"\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'tabfact'\n",
    "split = 'test'\n",
    "model_name = 'gpt-3.5-turbo-0125'\n",
    "model = ChatOpenAI(model_name=model_name, openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "schema_information = pd.read_csv(f\"result/aug/{task_name}_{split}_schema.csv\", index_col='table_id')\n",
    "aug_information = pd.read_csv(f\"result/aug/{task_name}_{split}_summary.csv\", index_col='table_id')\n",
    "composition_information = pd.read_csv(f\"result/aug/{task_name}_{split}_composition.csv\", index_col='table_id')\n",
    "engine = create_engine('sqlite:///db/sqlite/tabfact.db', echo=False)\n",
    "manager = SQLManager(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "sample = table_loader.normalize_table(table_loader.dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from data_loader import TableFormat\n",
    "query_examples = [\n",
    "    # \"the country of ecuador be the only country that use the us dollar (usd) as its currency , and the central bank be the federal reserve\",\n",
    "                  \"only 1 of the college list be public , and it be in new orleans\"]\n",
    "task_examples = [\"query rewrite\", \"query decompose\", \"query ambiguity resolve\"]\n",
    "inds = [8]\n",
    "num_k = 1\n",
    "table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "examples = [TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_html(table_loader.dataset[inds[i]]['table']['caption']) for i in range(num_k)]\n",
    "new_query_examples = [\n",
    "    # \"Which country uses the US dollar as its currency and has the Federal Reserve as its central bank?\",\n",
    "    \"which college list be public?\",\n",
    "                    #   \"what is the number of listings from barrington?; what is the number of listings from farmington?; what is the number of listings from rochester combined?\",\n",
    "                      ]\n",
    "examples_prompt = PromptTemplate(input_variables=[\"query\", \"new_query\"], template=\n",
    "\"\"\"\n",
    "Query: {query}\n",
    "Sub-Table: {table}\n",
    "new_query: {new_query}\"\"\")\n",
    "\n",
    "examples_dict = [{\"query\": query_examples[i],\n",
    "                  \"table\": examples[i],\n",
    "                    \"new_query\": new_query_examples[i]} for i in range(num_k)]\n",
    "step_back_prompt = FewShotPromptTemplate(\n",
    "    examples=examples_dict,\n",
    "    example_prompt=examples_prompt,\n",
    "    prefix=\"\"\"Based on the table, your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\"\"\",\n",
    "    suffix=\n",
    "    \"\"\"\n",
    "Query: {query}\n",
    "Sub-Table: {table}\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\", \"table\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are capable of converting complex query into sub-queries. Based on the table, provide at most 2 continuity sub-queries for knowledge that you need. \n",
      "Split the queries with ’;’.\n",
      "\n",
      "Query: all 12 club play a total of 22 game for the wru division one east\n",
      "Sub-Table: <table>\n",
      "<caption>wru division one east</caption>\n",
      "<thead>\n",
      "<tr><th>            club</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  points_for</th><th>  points_against</th><th>  tries_for</th><th>  tries_against</th><th>  try_bonus</th><th>  losing_bonus</th><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>fleur de lys rfc</td><td>22      </td><td>1      </td><td>16    </td><td>300         </td><td>617             </td><td>34         </td><td>77             </td><td>2          </td><td>4             </td><td>28      </td></tr>\n",
      "<tr><td>beddau rfc      </td><td>22      </td><td>0      </td><td>15    </td><td>310         </td><td>483             </td><td>32         </td><td>61             </td><td>2          </td><td>4             </td><td>34      </td></tr>\n",
      "<tr><td>pontypool rfc   </td><td>22      </td><td>2      </td><td>2     </td><td>648         </td><td>274             </td><td>81         </td><td>32             </td><td>12         </td><td>1             </td><td>89      </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "new_query: How many clubs play for the wru division one east in total?; How many clubs play 22 game for the wru division one east?;\n",
      "\n",
      "Query: from 1980 to 2011 , apoel bc lose more than 2 time as many game as it win\n",
      "Sub-Table: <table>\n",
      "<caption>apoel b.c</caption>\n",
      "<thead>\n",
      "<tr><th>  season</th><th>                  competition</th><th>  games</th><th>  wins</th><th>  loses</th><th>  against</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2004    </td><td>fiba europe cup              </td><td>6      </td><td>3     </td><td>3      </td><td>81.7     </td></tr>\n",
      "<tr><td>1995    </td><td>european cup for men 's clubs</td><td>2      </td><td>1     </td><td>1      </td><td>81       </td></tr>\n",
      "<tr><td>2011    </td><td>uleb eurocup                 </td><td>2      </td><td>0     </td><td>2      </td><td>90.5     </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "new_query: from 1980 to 2011 , how many games did apoel bc lose?; from 1980 to 2011 , how many games did apoel bc win?;\n",
      "\n",
      "Query: polona hercog 1890partner with alberta brianti after she have stephanie vogt as the partner\n",
      "Sub-Table: <table>\n",
      "<caption>polona hercog</caption>\n",
      "<thead>\n",
      "<tr><th>            date</th><th>        tournament</th><th>  surface</th><th>        partner</th><th>                                     opponents</th><th>                score</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>11 february 2008</td><td>mallorca 2 , spain</td><td>clay     </td><td>stephanie vogt </td><td>leticia costas - moreira maite gabarrus alonso</td><td>7 - 6 (7 - 2) , 6 - 3</td></tr>\n",
      "<tr><td>8 february 2010 </td><td>cali , colombia   </td><td>clay     </td><td>edina gallovits</td><td>estrella cabeza candella laura pous tió       </td><td>3 - 6 , 6 - 3 ,      </td></tr>\n",
      "<tr><td>28 april 2008   </td><td>makarska , croatia</td><td>clay     </td><td>stephanie vogt </td><td>tadeja majerić maša zec peškirič              </td><td>7 - 5 , 6 - 2        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "new_query: When did polona hercog partner with alberta brianti?; When did polona hercog partner with stephanie vogt?\n",
      "\n",
      "Query: 5 player be from the united state and canada each\n",
      "Sub-Table: <table>\n",
      "<thead>\n",
      "<tr><th>  round</th><th>         player</th><th>  position</th><th>  nationality</th><th>  college_junior_club_team_league</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>6      </td><td>ladislav scurko</td><td>center    </td><td>slovakia     </td><td>spišská nová ves (slovakia)      </td></tr>\n",
      "<tr><td>3      </td><td>rob bellamy    </td><td>right wing</td><td>united states</td><td>new england jr coyotes ( ejhl )  </td></tr>\n",
      "<tr><td>9      </td><td>triston grant  </td><td>left wing </td><td>canada       </td><td>vancouver giants ( whl )         </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "new_query: \n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "i = 31\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "\n",
    "all_queries = []\n",
    "formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many players are from the United States?; How many players are from Canada?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pred[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_answer(k: int=1):\n",
    "    sqls = [\"SELECT MIN(points) FROM DF WHERE rider = 'roger dutton / tony wright';\"]\n",
    "    thoughts = [\"Based on the SQL query provided, the minimum number of points that Roger Dutton / Tony Wright received in the 1972 Isle of Man TT event was 3. Therefore, the claim that 2 is the fewest points they received is false. The output should be 0.\"]\n",
    "    tables = [\"<table>\\n<caption>1972 isle of man tt</caption>\\n<thead>\\n<tr><th>  MIN(points)</th></tr>\\n</thead>\\n<tbody>\\n<tr><td>3            </td></tr>\\n</tbody>\\n</table>\"]\n",
    "    claims = [\"2 be the fewest point that roger dutton / tony wright receive\"]\n",
    "    # inds from test split\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], template=\n",
    "    \"\"\"\n",
    "    SQL Excuted: \n",
    "    ```{SQL}```\n",
    "    Sub-table: {table}\n",
    "    ...\n",
    "\n",
    "    Query: {claim}\n",
    "    Thought: {thought}\n",
    "    Output: {output}\n",
    "    \"\"\")\n",
    "    examples_dict = dict(zip([\"SQL\", \"table\", \"claim\", \"thought\", \"output\"], [sqls[0], tables[0], claims[0], thoughts[0], '0']))\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=[examples_dict],\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\"\"\"Below are some sub-tables generated by excuting the SQL. You need to understand the logic behind the SQL filtering. Complete task using all information below. Please think step by step and return 0 or 1 without any other information at last.\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "    {information}\n",
    "    Query: {query}\n",
    "    Thought: \"\"\",\n",
    "        input_variables=[\"table\", \"query\"],\n",
    ")\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_shot_with_aug(k: int=2):\n",
    "    table_loader = TableLoader(table_name='tabfact', split='validation', use_sample=True, small_test=False)\n",
    "\n",
    "    inds = [3, 6]\n",
    "    Output_examples = [\n",
    "                       'team, goals_for',\n",
    "                       'year, game, platform_s']\n",
    "    linking_examples = ['the team -> team; the most goal for -> goals_for',\n",
    "                        'gamecube -> platform_s; gamecube game -> game; the first 3 year -> year;'\n",
    "    ]\n",
    "    examples_prompt = PromptTemplate(input_variables=[\"table\", \"claim\", \"output\", \"linking\"], template=\n",
    "    \"\"\"\n",
    "    Table: {table}\n",
    "    Claim: {claim}\n",
    "    Column linking: {linking}\n",
    "    Columns: {output}\"\"\")\n",
    "    num_k = 2\n",
    "    examples_dict = [{\"table\": TableFormat(format='none', data=table_loader.dataset[inds[i]], use_sampling=True).format_html(table_loader.dataset[inds[i]]['table']['caption']),\n",
    "                                        \"claim\": table_loader.dataset[inds[i]]['statement'],\n",
    "                                        \"linking\": linking_examples[i],\n",
    "                                        # \"summary\": summary_examples[i],\n",
    "                                        \"output\": Output_examples[i]} for i in range(num_k)]\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=examples_dict,\n",
    "        example_prompt=examples_prompt,\n",
    "        prefix=\n",
    "        \"\"\"\n",
    "    Your task is accurately output columns related to the query or contain useful information about the query. This process involves linking relevant columns in the table on the terms in the query.\n",
    "    Approach this task as follows:\n",
    "    Read the question thoroughly and list every possible link from query term to column in Table.\n",
    "    Based on the linking columns, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\n",
    "    \"\"\",\n",
    "    # You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "    # Given the following table and query, you should output columns related to the query or contain useful information about the query. \n",
    "    # Here are some examples:\"\"\",\n",
    "        suffix=\n",
    "        \"\"\"\n",
    "    Table: {table}\n",
    "    Claim: {claim}\n",
    "    Extra information: {aug}\n",
    "    \"\"\",\n",
    "        input_variables=[\"table\", \"claim\", \"aug\"],\n",
    ")\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = table_loader.normalize_table(table_loader.dataset[0])\n",
    "k_shot_prompt = get_k_shot_with_aug()\n",
    "formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=True)\n",
    "summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                        #     'claim': 'Who is one of the three nominees for a Drama Desk Award?',\n",
    "                                    'claim': sample['query'],\n",
    "                                    'aug':  extra_information\n",
    "                                    })], return_only_outputs=True)[0]['text']\n",
    "# pred = llm_chain.batch([dict({\"query\": 'Who is one of the three nominees for a Drama Desk Award?'})])\n",
    "# print(pred[0]['text'])\n",
    "print(stage_1_batch_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "def scene_A(query, sample, verbose=True):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"\n",
    "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "Extra information: {aug}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        # stage 2: SQL generation\n",
    "        \n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        try: \n",
    "            formatter.data = formatter.data.loc[:, columns]\n",
    "            formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "        except:\n",
    "            pass\n",
    "        extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + extra_information\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "    print(\"total_tokens:\", cb.total_tokens)\n",
    "    print(stage_2_batch_pred)\n",
    "    # stage 3: SQL Excution\n",
    "    try: \n",
    "        formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "    except:\n",
    "        formatter.data = formatter.all_data\n",
    "        stage_2_batch_pred = 'SELECT * from DF;'\n",
    "    if len(formatter.data) == 0:\n",
    "        return query, stage_2_batch_pred, 'No data from database', cb.total_tokens\n",
    "    return query, stage_2_batch_pred, formatter.format_html(), cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [0,   1,  11,  14,  22,  24,  26,  31,  37,  38,  39,  40,  47,\n",
    "        49,  50,  59,  62,  63,  65,  66,  68,  69,  70,  75,  80,  81,\n",
    "        82,  90,  91,  93,  95,  99, 100, 105, 106, 108, 110, 116, 119,\n",
    "       124, 129, 136, 137, 145, 147, 148, 151, 153, 155, 158, 160, 164,\n",
    "       167, 169, 171, 173, 175, 179, 180, 181, 187, 188, 190, 192, 198,\n",
    "       200, 207, 209, 210, 211, 217, 225, 227, 230, 233, 236, 237, 241,\n",
    "       251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "def save_csv(input_list: List[List], label_list: List, file_path):\n",
    "    import pandas as pd\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    assert len(input_list) == len(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(label_list)):\n",
    "        df[label_list[i]] = pd.Series(input_list[i])\n",
    "    if os.path.exists(file_path) and file_path.endswith('.csv'):\n",
    "        df_origin = pd.read_csv(file_path)\n",
    "        df = pd.concat([df_origin, df], axis=0)\n",
    "    df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1272733091.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[67], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    i =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.com.cn/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "\n",
    "save_path = f\"result/answer/tabfact_zh_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "Below are some sub-tables, each sub-table is generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task by using useful information below.\n",
    "You can use one of the sub-tables or use all sub-tables.\n",
    "{information}\n",
    "Query: {query}\n",
    "verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\"\"\" )\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "stage_1 = []\n",
    "stage_2 = []\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "i = \n",
    "    \n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "labels.append(sample['label'])\n",
    "all_queries = [sample['query']]\n",
    "formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=False)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query']}], return_only_outputs=True)\n",
    "all_queries.append(batch_pred[0]['text'].split(':')[-1])\n",
    "print(batch_pred[0]['text'].split(':')[-1])\n",
    "\n",
    "args_list = [{\"query\": q, \"sample\": sample} for q in all_queries]\n",
    "results = parallel_run_kwargs(scene_A, args_list)\n",
    "temp = [f\"\"\"\n",
    "SQL Excuted: \n",
    "```{res[1]}```\n",
    "Sub-table: {res[2]}\"\"\" for res in results if len(res[1]) > 0]\n",
    "tokens.append(sum([res[3] for res in results]) / len(results))\n",
    "llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query'], \"information\": '\\n'.join(temp)}], return_only_outputs=True)\n",
    "print(batch_pred[0])\n",
    "outputs.append(batch_pred[0]['text'])\n",
    "# llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=False)\n",
    "# batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html(table_caption=sample['table']['caption'])}], return_only_outputs=True)\n",
    "# print(batch_pred[0]['text'].split(':')[-1].split(';'))\n",
    "# all_queries.extend(batch_pred[0]['text'].split(':')[-1].split(';'))\n",
    "# \"Is the following query true or false?\" +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "non_equal = []\n",
    "for ind, (pred, gold) in enumerate(zip(table['pred'], table['label'] )):\n",
    "            if pred == str(gold):\n",
    "                acc += 1\n",
    "            else:\n",
    "                non_equal.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 11,\n",
       " 14,\n",
       " 22,\n",
       " 24,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 44,\n",
       " 47,\n",
       " 51,\n",
       " 52,\n",
       " 58,\n",
       " 59,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 69,\n",
       " 76,\n",
       " 80,\n",
       " 83,\n",
       " 88]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To verify the claim that \"pierre lamine beat shinji someya by more than a point\", we need to compare the points of both individuals from the sub-table provided.\n",
      "\n",
      "From the sub-table:\n",
      "- pierre lamine has 150.5 points\n",
      "- shinji someya has 150.34 points\n",
      "\n",
      "To determine if pierre lamine beat shinji someya by more than a point, we need to calculate the point difference between them:\n",
      "150.5 - 150.34 = 0.16\n",
      "\n",
      "Since the point difference is less than 1, the claim that pierre lamine beat shinji someya by more than a point is false.\n",
      "\n",
      "Therefore, the answer is 0.\n"
     ]
    }
   ],
   "source": [
    "print(table.iloc[75, :]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_equals = []\n",
    "for i in non_equal:\n",
    "    non_equals.append(inds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table = pd.read_csv('./result/answer/tabfact_zh_04-23_15-22-00.csv', encoding='utf-8')\n",
    "table.head()\n",
    "table['pred'] = eval_blury_string(table['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_blury_string(pred_list):\n",
    "    pred_label = []\n",
    "    for pred in pred_list:\n",
    "        predict_ans = pred.split('\\n')[-1]\n",
    "        if '0' in predict_ans:\n",
    "            predict_ans = '0'\n",
    "        elif '1' in predict_ans:\n",
    "            predict_ans = '1'\n",
    "        else:\n",
    "            predict_ans = '2'\n",
    "        pred_label.append(predict_ans)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "def parallel_run(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(func, arg) for arg in args_list]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(results)]\n",
    "\n",
    "def parallel_run_kwargs(func, args_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda kwargs: func(**kwargs), args_list)\n",
    "        return list(results)\n",
    "# args_list = [{\"query\": 'Who took the loss in the game on August 30 and who suffered the loss in the game on August 31?', \"sample\": sample},{\"query\": sample['query'], \"sample\": sample}]\n",
    "# args_list = [{\"query\": q, \"sample\": sample} for q in all_queries]\n",
    "\n",
    "# results = parallel_run_kwargs(scene_A, args_list)\n",
    "# print(results)\n",
    "# print(cb.total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below are some sub-tables generated by excuting the SQL. You need to understand the logic behind the SQL filtering. Complete task using all information below. \n",
    "{information}\n",
    "Query: {query}\n",
    "verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step.\n",
    "\"\"\" )\n",
    "temp = [f\"\"\"\n",
    "SQL Excuted: \n",
    "```{res[1]}```\n",
    "Sub-table: {res[2]}\"\"\" for res in results]\n",
    "muilti_answer_instruction = get_k_shot_with_answer()\n",
    "llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query'], \"information\": '\\n'.join(temp)}], return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_B():\n",
    "    agent_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Based on the history information, your task is to only based on the conversation information to answer the user query.\n",
    "    If you cannot get the answer from past history, reorganize the question and return the question explicitly. If you are confident in the answer, answer it directly.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "    chain = LLMChain(llm=model, prompt=agent_prompt, verbose=True)\n",
    "    return chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": Agent_history.messages,\n",
    "    }\n",
    ")['text']\n",
    "    \n",
    "    #维护一个Agent Memory\n",
    "Agent_history = []\n",
    "agent_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # (\n",
    "        #     \"system\",\n",
    "        #     \"\"\"\"\"\"\n",
    "            \n",
    "        #     # return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.,\n",
    "        # ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "system =  PromptTemplate(input_variables=[\"history\"], template=\"\"\"Based on the history information, your task is to only based on the conversation information to answer the user query. \n",
    "Note: Do not use information on your own, only use information from the conversation history!\n",
    "\n",
    "conversation histroy:\n",
    "{history}\n",
    "\n",
    "The output should choose from Choice A and Choice B:\n",
    "Choice A: ###If you cannot get the answer from conversation histroy, reorganize the question and return the question explicitly.\n",
    "Choice B: ###If you are confident in the answer, answer it directly.\"\"\")\n",
    "chain = LLMChain(llm=model, prompt=system, verbose=True)\n",
    "Agent_history.append('Q: Who were the winners of the lifetime achievement award after 2005?')\n",
    "# Agent_history.append('A: the winners are andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle')\n",
    "# Agent_history.append('Q: Are the winners andrew rule john silvester , sandra harvey lindsay simpson , marele day , shane maloney , and peter doyle?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    " \n",
    "def outer_task(url):\n",
    "   # 外层任务\n",
    "   print(f\"Processing {url}\")\n",
    "   # 内部再次使用线程池\n",
    "   with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "       # 执行一些依赖于外层任务的结果的并行操作\n",
    "       inner_results = [executor.submit(inner_task, f\"{url}_{i}\") for i in range(2)]\n",
    "       # 等待内部任务完成并收集结果\n",
    "       return [r.result() for r in inner_results]\n",
    " \n",
    "def inner_task(url):\n",
    "   # 内层任务\n",
    "   print(f\"Inner task for {url}\")\n",
    "   # 这里可以执行一些操作，比如I/O密集型的任务\n",
    "   return f\"Result for {url}\"\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "   # 创建线程池\n",
    "   with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "       # 提交外层任务到线程池\n",
    "       outer_results = [executor.submit(outer_task, f\"http://example.com/{i}\") for i in range(2)]\n",
    "       # 等待外层任务完成并收集结果\n",
    "       for future in concurrent.futures.as_completed(outer_results):\n",
    "           print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调整extrainformation的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import parse_output\n",
    "answer_instruction = PromptTemplate(input_variables=[\"SQL\", \"table\", \"claim\"], \n",
    "                                    template=\"\"\"\n",
    "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Query: {claim}\n",
    "answer the question given in the query. If you cannot answer the question based on the sub-table, just say 'Cannot get answer from sub-table'\n",
    "\"\"\" )\n",
    "def scene_B(query, sample, verbose=True):\n",
    "    row_instruction = PromptTemplate(input_variables=[\"table\", \"claim\", \"aug\"], \n",
    "                                 template=\"\"\"\n",
    "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
    "sub-table: {table}\n",
    "Query: {claim}\n",
    "Extra information: {aug}\n",
    "SQL: \"\"\")\n",
    "    formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "    k_shot_prompt = get_k_shot_with_aug()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        llm_chain = LLMChain(llm=model, prompt=k_shot_prompt, verbose=verbose)\n",
    "        summary_aug, column_aug = aug_information.loc[sample['id']]['summary'], aug_information.loc[sample['id']]['column_description'] \n",
    "        col_names, col_infos = parse_output(column_aug, pattern=r'([^<]*)<([^>]*)>')\n",
    "        extra_col_info = []\n",
    "        for i_c in range(len(col_names)):\n",
    "            extra_col_info.append(f'{i_c + 1}. {col_names[i_c]}: {col_infos[i_c]}')\n",
    "        stage_1_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n'.join(extra_col_info)\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "        print(stage_1_batch_pred)\n",
    "        stage_1_batch_pred = stage_1_batch_pred.split(':')[-1]\n",
    "        \n",
    "        # stage 2: SQL generation\n",
    "        \n",
    "        llm_chain = LLMChain(llm=model, prompt=row_instruction, verbose=verbose)\n",
    "        columns = [formatter.normalize_col_name(c.strip()) for c in stage_1_batch_pred.split(',')]\n",
    "        formatter.normalize_schema(schema_information.loc[sample['id']]['schema'])\n",
    "        try: \n",
    "            formatter.data = formatter.data.loc[:, columns]\n",
    "            formatter.all_data = formatter.all_data.loc[:, columns]\n",
    "        except:\n",
    "            pass\n",
    "        extra_information = '\\n'.join(parse_specific_composition(composition_information.loc[sample['id']]['composition'], formatter.data.columns))\n",
    "        stage_2_batch_pred = llm_chain.batch([dict({'table': formatter.format_html(table_caption=sample['table']['caption']),\n",
    "                                            'claim': query,\n",
    "                                            'aug':  summary_aug + '\\n Column information:' + extra_information\n",
    "                                            })], return_only_outputs=True)[0]['text']\n",
    "    \n",
    "        print(stage_2_batch_pred)\n",
    "        # stage 3: SQL Excution\n",
    "        try: \n",
    "            formatter.data = manager.execute_from_df(stage_2_batch_pred, formatter.all_data, table_name='DF')\n",
    "        except:\n",
    "            formatter.data = formatter.all_data\n",
    "            stage_2_batch_pred = 'SELECT * from DF;'\n",
    "        \n",
    "        llm_chain = LLMChain(llm=model, prompt=answer_instruction, verbose=verbose)\n",
    "        response = llm_chain.batch([dict({'table': formatter.format_html(),\n",
    "                                                'claim': query,\n",
    "                                                'SQL':  stage_2_batch_pred\n",
    "                                                })], return_only_outputs=True)[0]['text']\n",
    "    print(\"total_tokens:\", cb.total_tokens)\n",
    "    return response, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Your task is accurately output columns related to the query or contain useful information about the query. This process involves linking relevant columns in the table on the terms in the query.\n",
      "    Approach this task as follows:\n",
      "    Read the question thoroughly and list every possible link from query term to column in Table.\n",
      "    Based on the linking columns, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\n",
      "    \n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1986 - 87 north west counties football league</caption>\n",
      "<thead>\n",
      "<tr><th>  position</th><th>               team</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  goals_for</th><th>  goals_against</th><th>  goal_difference</th><th>  points_1</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>12        </td><td>ashton town        </td><td>24      </td><td>9      </td><td>11    </td><td>29         </td><td>39             </td><td>10               </td><td>15 2      </td></tr>\n",
      "<tr><td>10        </td><td>padiham            </td><td>24      </td><td>8      </td><td>12    </td><td>35         </td><td>39             </td><td>4                </td><td>16        </td></tr>\n",
      "<tr><td>1         </td><td>atherton collieries</td><td>24      </td><td>4      </td><td>4     </td><td>46         </td><td>22             </td><td>+ 24             </td><td>36        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: the team with the most goal for in the 1986 - 87 north west county football league be flixton\n",
      "    Column linking: the team -> team; the most goal for -> goals_for\n",
      "    Columns: team, goals_for\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>list of game of the year awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>                               game</th><th>                                  genre</th><th>                        platform_s</th><th>                 developer_s</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2011  </td><td>the legend of zelda : skyward sword</td><td>action - adventure                     </td><td>wii                               </td><td>nintendo ead , monolith soft</td></tr>\n",
      "<tr><td>2010  </td><td>mass effect 2                      </td><td>action rpg : ( third - person ) shooter</td><td>xbox 360 , windows , playstation 3</td><td>bioware                     </td></tr>\n",
      "<tr><td>2001  </td><td>super smash bros melee             </td><td>fighting                               </td><td>gamecube                          </td><td>hal laboratory , inc        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: a gamecube game loss the award in each of the first 3 year\n",
      "    Column linking: gamecube -> platform_s; gamecube game -> game; the first 3 year -> year;\n",
      "    Columns: year, game, platform_s\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>  rank</th><th>          name</th><th>       nation</th><th>  sp_+_fs</th><th>  points</th><th>  places</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1     </td><td>mark cockerell</td><td>united states</td><td>1        </td><td>172.42  </td><td>11      </td></tr>\n",
      "<tr><td>6     </td><td>stephan bril  </td><td>west germany </td><td>7        </td><td>155.72  </td><td>57      </td></tr>\n",
      "<tr><td>12    </td><td>gerald schranz</td><td>austria      </td><td>11       </td><td>143.04  </td><td>102     </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: What is the point of Pierre Lamine?\n",
      "    Extra information: 1976 world junior figure skating championships1. rank: The ranking of the skater in the competition\n",
      "2. name: The name of the skater\n",
      "3. nation: The country the skater represents\n",
      "4. sp_+_fs: The sum of the skater's short program and free skate scores\n",
      "5. points: The total points scored by the skater\n",
      "6. places: The number of competitors the skater placed ahead of\n",
      "    \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Your task is accurately output columns related to the query or contain useful information about the query. This process involves linking relevant columns in the table on the terms in the query.\n",
      "    Approach this task as follows:\n",
      "    Read the question thoroughly and list every possible link from query term to column in Table.\n",
      "    Based on the linking columns, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\n",
      "    \n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1986 - 87 north west counties football league</caption>\n",
      "<thead>\n",
      "<tr><th>  position</th><th>               team</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  goals_for</th><th>  goals_against</th><th>  goal_difference</th><th>  points_1</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>12        </td><td>ashton town        </td><td>24      </td><td>9      </td><td>11    </td><td>29         </td><td>39             </td><td>10               </td><td>15 2      </td></tr>\n",
      "<tr><td>10        </td><td>padiham            </td><td>24      </td><td>8      </td><td>12    </td><td>35         </td><td>39             </td><td>4                </td><td>16        </td></tr>\n",
      "<tr><td>1         </td><td>atherton collieries</td><td>24      </td><td>4      </td><td>4     </td><td>46         </td><td>22             </td><td>+ 24             </td><td>36        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: the team with the most goal for in the 1986 - 87 north west county football league be flixton\n",
      "    Column linking: the team -> team; the most goal for -> goals_for\n",
      "    Columns: team, goals_for\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>list of game of the year awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>                               game</th><th>                                  genre</th><th>                        platform_s</th><th>                 developer_s</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2011  </td><td>the legend of zelda : skyward sword</td><td>action - adventure                     </td><td>wii                               </td><td>nintendo ead , monolith soft</td></tr>\n",
      "<tr><td>2010  </td><td>mass effect 2                      </td><td>action rpg : ( third - person ) shooter</td><td>xbox 360 , windows , playstation 3</td><td>bioware                     </td></tr>\n",
      "<tr><td>2001  </td><td>super smash bros melee             </td><td>fighting                               </td><td>gamecube                          </td><td>hal laboratory , inc        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: a gamecube game loss the award in each of the first 3 year\n",
      "    Column linking: gamecube -> platform_s; gamecube game -> game; the first 3 year -> year;\n",
      "    Columns: year, game, platform_s\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>  rank</th><th>          name</th><th>       nation</th><th>  sp_+_fs</th><th>  points</th><th>  places</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1     </td><td>mark cockerell</td><td>united states</td><td>1        </td><td>172.42  </td><td>11      </td></tr>\n",
      "<tr><td>6     </td><td>stephan bril  </td><td>west germany </td><td>7        </td><td>155.72  </td><td>57      </td></tr>\n",
      "<tr><td>12    </td><td>gerald schranz</td><td>austria      </td><td>11       </td><td>143.04  </td><td>102     </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim:  What is the point of Shinji Someya?\n",
      "    Extra information: 1976 world junior figure skating championships1. rank: The ranking of the skater in the competition\n",
      "2. name: The name of the skater\n",
      "3. nation: The country the skater represents\n",
      "4. sp_+_fs: The sum of the skater's short program and free skate scores\n",
      "5. points: The total points scored by the skater\n",
      "6. places: The number of competitors the skater placed ahead of\n",
      "    \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Your task is accurately output columns related to the query or contain useful information about the query. This process involves linking relevant columns in the table on the terms in the query.\n",
      "    Approach this task as follows:\n",
      "    Read the question thoroughly and list every possible link from query term to column in Table.\n",
      "    Based on the linking columns, output all useful columns at last. Make sure all columns in the link step are included and every column is in the Table.\n",
      "    \n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1986 - 87 north west counties football league</caption>\n",
      "<thead>\n",
      "<tr><th>  position</th><th>               team</th><th>  played</th><th>  drawn</th><th>  lost</th><th>  goals_for</th><th>  goals_against</th><th>  goal_difference</th><th>  points_1</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>12        </td><td>ashton town        </td><td>24      </td><td>9      </td><td>11    </td><td>29         </td><td>39             </td><td>10               </td><td>15 2      </td></tr>\n",
      "<tr><td>10        </td><td>padiham            </td><td>24      </td><td>8      </td><td>12    </td><td>35         </td><td>39             </td><td>4                </td><td>16        </td></tr>\n",
      "<tr><td>1         </td><td>atherton collieries</td><td>24      </td><td>4      </td><td>4     </td><td>46         </td><td>22             </td><td>+ 24             </td><td>36        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: the team with the most goal for in the 1986 - 87 north west county football league be flixton\n",
      "    Column linking: the team -> team; the most goal for -> goals_for\n",
      "    Columns: team, goals_for\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>list of game of the year awards</caption>\n",
      "<thead>\n",
      "<tr><th>  year</th><th>                               game</th><th>                                  genre</th><th>                        platform_s</th><th>                 developer_s</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>2011  </td><td>the legend of zelda : skyward sword</td><td>action - adventure                     </td><td>wii                               </td><td>nintendo ead , monolith soft</td></tr>\n",
      "<tr><td>2010  </td><td>mass effect 2                      </td><td>action rpg : ( third - person ) shooter</td><td>xbox 360 , windows , playstation 3</td><td>bioware                     </td></tr>\n",
      "<tr><td>2001  </td><td>super smash bros melee             </td><td>fighting                               </td><td>gamecube                          </td><td>hal laboratory , inc        </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim: a gamecube game loss the award in each of the first 3 year\n",
      "    Column linking: gamecube -> platform_s; gamecube game -> game; the first 3 year -> year;\n",
      "    Columns: year, game, platform_s\n",
      "\n",
      "\n",
      "    Table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>  rank</th><th>          name</th><th>       nation</th><th>  sp_+_fs</th><th>  points</th><th>  places</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>1     </td><td>mark cockerell</td><td>united states</td><td>1        </td><td>172.42  </td><td>11      </td></tr>\n",
      "<tr><td>6     </td><td>stephan bril  </td><td>west germany </td><td>7        </td><td>155.72  </td><td>57      </td></tr>\n",
      "<tr><td>12    </td><td>gerald schranz</td><td>austria      </td><td>11       </td><td>143.04  </td><td>102     </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "    Claim:  what is the point difference between pierre lamine and shinji someya?\n",
      "    Extra information: 1976 world junior figure skating championships1. rank: The ranking of the skater in the competition\n",
      "2. name: The name of the skater\n",
      "3. nation: The country the skater represents\n",
      "4. sp_+_fs: The sum of the skater's short program and free skate scores\n",
      "5. points: The total points scored by the skater\n",
      "6. places: The number of competitors the skater placed ahead of\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Column linking: Pierre Lamine -> name; points -> points\n",
      "Columns: name, points\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
      "sub-table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>          name</th><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>mark cockerell</td><td>172.42  </td></tr>\n",
      "<tr><td>stephan bril  </td><td>155.72  </td></tr>\n",
      "<tr><td>gerald schranz</td><td>143.04  </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query: What is the point of Pierre Lamine?\n",
      "Extra information: 1976 world junior figure skating championships\n",
      " Column information:name:Names of the skaters participating in the competition\n",
      "points:Decimal numbers representing the total points scored by the skater\n",
      "SQL: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "SELECT points\n",
      "FROM DF\n",
      "WHERE name = 'Pierre Lamine';\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
      "SQL Excuted: \n",
      "```SELECT points\n",
      "FROM DF\n",
      "WHERE name = 'Pierre Lamine';```\n",
      "Sub-table: <table>\n",
      "<thead>\n",
      "<tr></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "</tbody>\n",
      "</table>\n",
      "Query: What is the point of Pierre Lamine?\n",
      "answer the question given in the query. If you cannot answer the question based on the sub-table, just say 'Cannot get answer from sub-table'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Column linking: pierre lamine -> name; shinji someya -> name; points -> points\n",
      "Columns: name, points\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
      "sub-table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>          name</th><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>mark cockerell</td><td>172.42  </td></tr>\n",
      "<tr><td>stephan bril  </td><td>155.72  </td></tr>\n",
      "<tr><td>gerald schranz</td><td>143.04  </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query:  what is the point difference between pierre lamine and shinji someya?\n",
      "Extra information: 1976 world junior figure skating championships\n",
      " Column information:name:Names of the skaters participating in the competition\n",
      "points:Decimal numbers representing the total points scored by the skater\n",
      "SQL: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Column linking: Shinji Someya -> name\n",
      "Columns: points\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Our ultimate goal is to answer query based on the table. Below is a subtable with columns filtered, you are required to infer the data distribution and format from the sample data of the sub-table. Carefully analyze the query, based on the augmentation information, write a SQLITE3 SELECT SQL statement using table DF that complete query. Directly Output SQL, do not add other string.\n",
      "sub-table: <table>\n",
      "<caption>1976 world junior figure skating championships</caption>\n",
      "<thead>\n",
      "<tr><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>172.42  </td></tr>\n",
      "<tr><td>155.72  </td></tr>\n",
      "<tr><td>143.04  </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query:  What is the point of Shinji Someya?\n",
      "Extra information: 1976 world junior figure skating championships\n",
      " Column information:points:Decimal numbers representing the total points scored by the skater\n",
      "SQL: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "total_tokens: 1527\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "SELECT ABS((SELECT points FROM DF WHERE name = 'pierre lamine') - (SELECT points FROM DF WHERE name = 'shinji someya')) as point_difference;\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
      "SQL Excuted: \n",
      "```SELECT ABS((SELECT points FROM DF WHERE name = 'pierre lamine') - (SELECT points FROM DF WHERE name = 'shinji someya')) as point_difference;```\n",
      "Sub-table: <table>\n",
      "<thead>\n",
      "<tr><th>  point_difference</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>0.16              </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query:  what is the point difference between pierre lamine and shinji someya?\n",
      "answer the question given in the query. If you cannot answer the question based on the sub-table, just say 'Cannot get answer from sub-table'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "SELECT points\n",
      "FROM DF\n",
      "WHERE skater = 'Shinji Someya';\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Below is a sub-table generated by excuting the SQL. You need to understand the logic behind the SQL filtering and complete task using the final sub-table. \n",
      "SQL Excuted: \n",
      "```SELECT * from DF;```\n",
      "Sub-table: <table>\n",
      "<thead>\n",
      "<tr><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>172.42  </td></tr>\n",
      "<tr><td>165.7   </td></tr>\n",
      "<tr><td>166.62  </td></tr>\n",
      "<tr><td>159.8   </td></tr>\n",
      "<tr><td>158.1   </td></tr>\n",
      "<tr><td>155.72  </td></tr>\n",
      "<tr><td>151.76  </td></tr>\n",
      "<tr><td>150.5   </td></tr>\n",
      "<tr><td>150.34  </td></tr>\n",
      "<tr><td>148.88  </td></tr>\n",
      "<tr><td>146.18  </td></tr>\n",
      "<tr><td>143.04  </td></tr>\n",
      "<tr><td>137.32  </td></tr>\n",
      "<tr><td>136.6   </td></tr>\n",
      "<tr><td>131.02  </td></tr>\n",
      "<tr><td>127.74  </td></tr>\n",
      "<tr><td>127.3   </td></tr>\n",
      "<tr><td>114.98  </td></tr>\n",
      "<tr><td>114.38  </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Query:  What is the point of Shinji Someya?\n",
      "answer the question given in the query. If you cannot answer the question based on the sub-table, just say 'Cannot get answer from sub-table'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "total_tokens: 1634\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "total_tokens: 1728\n",
      "Column linking: pierre lamine -> name; shinji someya -> name; mere 0.16 more point -> points\n",
      "Columns: name, points\n",
      "total_tokens: 1438\n",
      "SELECT points\n",
      "FROM DF\n",
      "WHERE name = 'pierre lamine' OR name = 'shinji someya'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
      "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
      "SQL Excuted: \n",
      "```SELECT points\n",
      "FROM DF\n",
      "WHERE name = 'pierre lamine' OR name = 'shinji someya'```\n",
      "Sub-table: <table>\n",
      "<thead>\n",
      "<tr><th>  points</th></tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr><td>150.5   </td></tr>\n",
      "<tr><td>150.34  </td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "Extra information: The point difference between Pierre Lamine and Shinji Someya is 0.16.\n",
      "Cannot get answer from sub-table\n",
      "Cannot get answer from sub-table\n",
      "Query: pierre lamine have a mere 0.16 more point than shinji someya\n",
      "verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'text': 'To verify the claim that Pierre Lamine has a mere 0.16 more points than Shinji Someya, we need to compare the points of both individuals from the sub-table provided.\\n\\nFrom the sub-table:\\n- Pierre Lamine has 150.5 points\\n- Shinji Someya has 150.34 points\\n\\nCalculating the point difference:\\n150.5 - 150.34 = 0.16\\n\\nTherefore, the claim that Pierre Lamine has a mere 0.16 more points than Shinji Someya is true.\\n\\nFinal answer: 1'}\n",
      "ALL TOKENS: 4889\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import datetime\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo-0125', openai_api_base=\"https://api.chatanywhere.tech/v1\",\n",
    "                       openai_api_key=\"sk-kxgtm71G6zwC44lglIF5CfiEVVzjjc39TOtppkNAwrVA2fUW\", temperature=0.1)\n",
    "table_loader = TableLoader(table_name='tabfact', split='test', use_sample=False, small_test=True)\n",
    "\n",
    "save_path = f\"result/answer/tabfact_zh_{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.csv\"\n",
    "muilti_answer_instruction = PromptTemplate(input_variables=[\"information\", \"claim\"], \n",
    "template=\"\"\"You are a brilliant table executor with the capabilities information retrieval, table parsing, table partition and semantic understanding who can understand the structural information of the table.\n",
    "Below is a sub-table generated by excuting the corresponding SQL. You need to understand the logic behind the SQL filtering. Complete task with the help of extra information below.\n",
    "SQL Excuted: \n",
    "```{SQL}```\n",
    "Sub-table: {table}\n",
    "Extra information: {information}\n",
    "Query: {query}\n",
    "verify whether the provided claim/query is true or false, return 0 if it's false, or 1 if it's true. Please think step by step and return 0/1 at last.\n",
    "\"\"\" )\n",
    "tokens = []\n",
    "outputs = []\n",
    "labels = []\n",
    "# muilti_answer_instruction = get_k_shot_with_answer()\n",
    "# for i in range(30, 41):\n",
    "i = 70\n",
    "sample = table_loader.normalize_table(\n",
    "                    table_loader.dataset[i])\n",
    "labels.append(sample['label'])\n",
    "all_queries = []\n",
    "formatter = TableFormat(format='none', data=sample, use_sampling=True)\n",
    "llm_chain = LLMChain(llm=model, prompt=step_back_prompt, verbose=False)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)\n",
    "all_queries.append(batch_pred[0]['text'].split(':')[-1])\n",
    "llm_chain = LLMChain(llm=model, prompt=decompose_prompt, verbose=False)\n",
    "batch_pred = llm_chain.batch([{\"query\": sample['query'], \"table\": formatter.format_html()}], return_only_outputs=True)\n",
    "all_queries.extend(batch_pred[0]['text'].split(';'))\n",
    "print(len(all_queries))\n",
    "args_list = [{\"query\": q, \"sample\": sample} for q in all_queries]\n",
    "ans_from_B = parallel_run_kwargs(scene_B, args_list)\n",
    "results = [res[0] for res in ans_from_B if res != 'Cannot get answer from sub-table']\n",
    "all_tokens = sum([res[1] for res in ans_from_B])\n",
    "#With answer\n",
    "with get_openai_callback() as cb:\n",
    "    imp_input = scene_A(sample['query'], sample, False)\n",
    "    llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "    batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(results)}], return_only_outputs=True)\n",
    "print(batch_pred[0])\n",
    "all_tokens += cb.total_tokens\n",
    "print('ALL TOKENS:', all_tokens)\n",
    "outputs.append(batch_pred[0]['text'])\n",
    "\n",
    "#With no answer\n",
    "# temp = [f\"\"\"\n",
    "# SQL Excuted for extra information: \n",
    "# ```{res[1]}```\n",
    "# Sub-table for extra information: {res[2]}\"\"\" for res in results if res[2] != 'No data from database']\n",
    "# imp_input = scene_A(sample['query'], sample, False)\n",
    "# llm_chain = LLMChain(llm=model, prompt=muilti_answer_instruction, verbose=True)\n",
    "# batch_pred = llm_chain.batch([{\"query\": sample['query'],\"SQL\": imp_input[1], \"table\": imp_input[2], \"information\": '\\n'.join(temp)}], return_only_outputs=True)\n",
    "# print(batch_pred[0])\n",
    "# outputs.append(batch_pred[0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '0']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_blury_string(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '1', '1', '1', '1', '0', '0', '1', '1', '0', '1']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the mintage of the 31997 release?; When was the 31997 release released?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the mintage of the 31997 release?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, theme, mintage\n",
      "SELECT mintage\n",
      "FROM DF\n",
      "WHERE theme = 'hmcs bras dor' AND mintage = '31997';\n",
      "Yes, the 31997 mintage was released in 2001.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the mintage of the 31997 release?\n",
      "AI: Yes, the 31997 mintage was released in 2001.\n",
      "Human:  When was the 31997 release released?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, mintage\n",
      "SELECT year\n",
      "FROM DF\n",
      "WHERE mintage = '31997';\n",
      "the 31997 mintage was released in 2001\n",
      "3041\n",
      " What is the lowest number of points received by Roger Dutton?; What is the lowest number of points received by Tony Wright?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the lowest number of points received by Roger Dutton?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What is the lowest number of points received by Roger Dutton?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the lowest number of points received by Roger Dutton?\n",
      "Human:  What is the lowest number of points received by Tony Wright?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "rider, points\n",
      "SELECT MIN(points) \n",
      "FROM DF \n",
      "WHERE rider LIKE '%Tony Wright%';\n",
      "3\n",
      "1714\n",
      " Who were the winners of the lifetime achievement award after 2005?; Are the winners Andrew Rule, John Silvester, Sandra Harvey, Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Who were the winners of the lifetime achievement award after 2005?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, lifetime achievement\n",
      "SELECT lifetime_achievement\n",
      "FROM DF\n",
      "WHERE year > 2005;\n",
      "The winners of the lifetime achievement award after 2005 are Andrew Rule and John Silvester, Sandra Harvey and Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Who were the winners of the lifetime achievement award after 2005?\n",
      "AI: The winners of the lifetime achievement award after 2005 are Andrew Rule and John Silvester, Sandra Harvey and Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle.\n",
      "Human:  Are the winners Andrew Rule, John Silvester, Sandra Harvey, Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  Who were the winners of the lifetime achievement award after 2005?\n",
      "AI: The winners of the lifetime achievement award after 2005 are Andrew Rule and John Silvester, Sandra Harvey and Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle.\n",
      "Human:  Are the winners Andrew Rule, John Silvester, Sandra Harvey, Lindsay Simpson, Marele Day, Shane Maloney, and Peter Doyle?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "1893\n",
      " Where did each player come from - a college program or a junior/club team?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Where did each player come from - a college program or a junior/club team?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "player, college_junior_club_team_league\n",
      "SELECT player, \n",
      "CASE \n",
      "    WHEN college_junior_club_team_league LIKE '%college%' THEN 'college program'\n",
      "    ELSE 'junior/club team'\n",
      "END AS player_origin\n",
      "FROM DF;\n",
      "All players in the sub-table come from a junior/club team.\n",
      "1845\n",
      " What tournament has the highest number of events?; Is the Open Championship the tournament with the highest number of events?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What tournament has the highest number of events?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What tournament has the highest number of events?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What tournament has the highest number of events?\n",
      "Human:  Is the Open Championship the tournament with the highest number of events?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What tournament has the highest number of events?\n",
      "Human:  Is the Open Championship the tournament with the highest number of events?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "294\n",
      " What is the station with a frequency of 96.3 FM?; Does the station with a frequency of 96.3 FM play classic rock?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the station with a frequency of 96.3 FM?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "frequency, call_sign, branding, format, owner\n",
      "SELECT * FROM DF WHERE frequency = '96.3 fm';\n",
      "classic rock\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the station with a frequency of 96.3 FM?\n",
      "AI: classic rock\n",
      "Human:  Does the station with a frequency of 96.3 FM play classic rock?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "frequency, branding, format\n",
      "SELECT branding, format\n",
      "FROM DF\n",
      "WHERE frequency = '96.3 fm' AND format = 'classic rock';\n",
      "The station with a frequency of 96.3 fm plays classic rock.\n",
      "3153\n",
      " What was the outcome of the Yugoslavian national team's match against Poland in the Balken Cup on March 22?; What was the score of the match?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the outcome of the Yugoslavian national team's match against Poland in the Balken Cup on March 22?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "date, city, opponent, results, type_of_game\n",
      "SELECT results\n",
      "FROM DF\n",
      "WHERE opponent = 'poland' AND type_of_game = 'balkan cup' AND date = 'march 22';\n",
      "the yugoslavian national team suffer its worst outcome lose 2:1 in the balken cup against poland on march 22\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the outcome of the Yugoslavian national team's match against Poland in the Balken Cup on March 22?\n",
      "AI: the yugoslavian national team suffer its worst outcome lose 2:1 in the balken cup against poland on march 22\n",
      "Human:  What was the score of the match?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What was the outcome of the Yugoslavian national team's match against Poland in the Balken Cup on March 22?\n",
      "AI: the yugoslavian national team suffer its worst outcome lose 2:1 in the balken cup against poland on march 22\n",
      "Human:  What was the score of the match?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "1948\n",
      " Who is the outgoing manager of Nejapa?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Who is the outgoing manager of Nejapa?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "team, outgoing_manager\n",
      "SELECT outgoing_manager\n",
      "FROM DF\n",
      "WHERE team = 'nejapa';\n",
      "No, Roberto Gamarra is not the outgoing manager of Nejapa.\n",
      "1601\n",
      " Which location has the highest ERP W?; Which location has the lowest ERP W?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Which location has the highest ERP W?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  Which location has the highest ERP W?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Which location has the highest ERP W?\n",
      "Human:  Which location has the lowest ERP W?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  Which location has the highest ERP W?\n",
      "Human:  Which location has the lowest ERP W?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "280\n",
      " Who was the opponent in the final on 6 February 2000 in Wellington, New Zealand on a hard surface?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Who was the opponent in the final on 6 February 2000 in Wellington, New Zealand on a hard surface?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "opponent_in_final\n",
      "SELECT opponent_in_final\n",
      "FROM DF\n",
      "WHERE date = '6 February 2000'\n",
      "AND location = 'Wellington, New Zealand'\n",
      "AND surface = 'hard';\n",
      "katerina kramperová\n",
      "1547\n",
      " What surface was the opponent for Roger Federer on July 13, 2003?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What surface was the opponent for Roger Federer on July 13, 2003?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "surface, opponent\n",
      "SELECT surface\n",
      "FROM DF\n",
      "WHERE opponent = 'Roger Federer'\n",
      "AND date = '2003-07-13';\n",
      "Clay\n",
      "1480\n",
      " What is the version of the b-58 aircraft model that originated in the United States?; How many bell uh-1 iroquois aircraft are currently in service?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the version of the b-58 aircraft model that originated in the United States?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "aircraft, origin, type, versions\n",
      "SELECT versions\n",
      "FROM DF\n",
      "WHERE aircraft LIKE '%b-58%'\n",
      "AND origin = 'united states';\n",
      "bell uh - 1 iroquois\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the version of the b-58 aircraft model that originated in the United States?\n",
      "AI: bell uh - 1 iroquois\n",
      "Human:  How many bell uh-1 iroquois aircraft are currently in service?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "aircraft, in_service\n",
      "SELECT COUNT(*) \n",
      "FROM DF \n",
      "WHERE aircraft LIKE '%bell uh-1 iroquois%';\n",
      "bell uh - 1 iroquois\n",
      "3147\n",
      " What is the least amount of laps when the grid is 5?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the least amount of laps when the grid is 5?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "laps, grid\n",
      "SELECT MIN(laps) \n",
      "FROM DF \n",
      "WHERE grid = 5;\n",
      "The least amount of laps when the grid is 5 is 43.0.\n",
      "1458\n",
      " What years was Titus Ozon active?; How many goals did Titus Ozon score?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What years was Titus Ozon active?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "name, years\n",
      "SELECT years\n",
      "FROM DF\n",
      "WHERE name = 'titus ozon';\n",
      "titus ozon scored 157 goals from 1947 to 1964.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What years was Titus Ozon active?\n",
      "AI: titus ozon scored 157 goals from 1947 to 1964.\n",
      "Human:  How many goals did Titus Ozon score?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What years was Titus Ozon active?\n",
      "AI: titus ozon scored 157 goals from 1947 to 1964.\n",
      "Human:  How many goals did Titus Ozon score?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "1661\n",
      " What was the score of the Super League XII match on 21/07/07?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the score of the Super League XII match on 21/07/07?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "date, home_team, score, away_team\n",
      "SELECT score\n",
      "FROM DF\n",
      "WHERE date = '2007-07-21 00:00:00';\n",
      "The super league xii score on 21 / 07 / 07 was 14 - 10.\n",
      "1708\n",
      " When did the Nets win at the Air Canada Centre?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  When did the Nets win at the Air Canada Centre?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "date, team, score, location_attendance\n",
      "SELECT date\n",
      "FROM DF\n",
      "WHERE team = 'toronto'\n",
      "AND location_attendance LIKE '%air canada centre%';\n",
      "april 10\n",
      "1776\n",
      " What was the record of the team on April 11 when the opponent was the Yankees?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the record of the team on April 11 when the opponent was the Yankees?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "date, opponent, record\n",
      "SELECT record\n",
      "FROM DF\n",
      "WHERE date = 'april 12'\n",
      "AND opponent = 'yankees';\n",
      "The opponent was not the Yankees on April 11.\n",
      "1597\n",
      " When did equestrian events take place at the Asian Games from 1982 to 2010?; Were there any exceptions to the equestrian events at the Asian Games during this time period?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  When did equestrian events take place at the Asian Games from 1982 to 2010?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "year, location\n",
      "SELECT year\n",
      "FROM DF\n",
      "WHERE year BETWEEN 1982 AND 2010;\n",
      "The equestrian at the Asian Games did not happen in 1990.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  When did equestrian events take place at the Asian Games from 1982 to 2010?\n",
      "AI: The equestrian at the Asian Games did not happen in 1990.\n",
      "Human:  Were there any exceptions to the equestrian events at the Asian Games during this time period?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  When did equestrian events take place at the Asian Games from 1982 to 2010?\n",
      "AI: The equestrian at the Asian Games did not happen in 1990.\n",
      "Human:  Were there any exceptions to the equestrian events at the Asian Games during this time period?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "1847\n",
      " How many games did Otto Graham win?; How many games did Brady Quinn win?; What is the difference in the number of games won by Otto Graham and Brady Quinn?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many games did Otto Graham win?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "quarterback, games_started, wins\n",
      "SELECT wins\n",
      "FROM DF\n",
      "WHERE quarterback = 'graham , otto';\n",
      "Otto Graham has won 44 more games than Brady Quinn.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many games did Otto Graham win?\n",
      "AI: Otto Graham has won 44 more games than Brady Quinn.\n",
      "Human:  How many games did Brady Quinn win?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "quarterback, games_started, wins\n",
      "SELECT wins\n",
      "FROM DF\n",
      "WHERE quarterback = 'quinn , brady';\n",
      "Otto Graham has won 44 more games than Brady Quinn.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many games did Otto Graham win?\n",
      "AI: Otto Graham has won 44 more games than Brady Quinn.\n",
      "Human:  How many games did Brady Quinn win?\n",
      "AI: Otto Graham has won 44 more games than Brady Quinn.\n",
      "Human:  What is the difference in the number of games won by Otto Graham and Brady Quinn?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "quarterback, wins\n",
      "SELECT (SELECT wins FROM DF WHERE quarterback = 'graham , otto') - (SELECT wins FROM DF WHERE quarterback = 'quinn , brady') as difference;\n",
      "Otto Graham has won 44 more games than Brady Quinn.\n",
      "4643\n",
      " How many game features are there in total?; How many game features have an attendance in the 3000s?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many game features are there in total?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "game, date, opponent, venue, result, attendance\n",
      "SELECT COUNT(game) AS total_game_features FROM DF;\n",
      "3 of the total game features have an attendance in the 3000s.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many game features are there in total?\n",
      "AI: 3 of the total game features have an attendance in the 3000s.\n",
      "Human:  How many game features have an attendance in the 3000s?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  How many game features are there in total?\n",
      "AI: 3 of the total game features have an attendance in the 3000s.\n",
      "Human:  How many game features have an attendance in the 3000s?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "1901\n",
      " What was the highest number of episodes in region 4 on March 15, 2007?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the highest number of episodes in region 4 on March 15, 2007?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "episodes, region_4\n",
      "SELECT MAX(episodes) \n",
      "FROM DF \n",
      "WHERE region_4 = 'march 15 , 2007';\n",
      "The highest number of episodes on March 15, 2007 in region 4 is 4.\n",
      "1575\n",
      " How many A results did Wimbledon have during the years 1986 to 1999?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many A results did Wimbledon have during the years 1986 to 1999?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "tournament, c_1986, c_1991, c_1995\n",
      "SELECT COUNT(*) \n",
      "FROM DF \n",
      "WHERE tournament = 'wimbledon' AND (c_1986 = 'a' OR c_1991 = 'a' OR c_1995 = 'a');\n",
      "wimbledon have only 1 a result during the year 1986 to 1999\n",
      "1884\n",
      " Is series 101 part of season 10?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Is series 101 part of season 10?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "series, season, title\n",
      "SELECT * \n",
      "FROM DF \n",
      "WHERE series = 101 AND season = 10;\n",
      "the honeymoon 's over\n",
      "1693\n",
      " Who led the team in points for each of the 5 straight games?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Who led the team in points for each of the 5 straight games?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "game, date, team, score, high_points\n",
      "SELECT game, team, MAX(high_points) AS player_with_high_points\n",
      "FROM DF\n",
      "GROUP BY game;\n",
      "Yes, Brook Lopez led the team in points for 5 straight games.\n",
      "2057\n",
      " What is the location of the Verizon Center in 20173?; What was the attendance at the Verizon Center when the record was 49 - 36%?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the location of the Verizon Center in 20173?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "location_attendance\n",
      "SELECT location_attendance\n",
      "FROM DF\n",
      "WHERE location_attendance LIKE '%verizon center%' AND location_attendance LIKE '%20173%';\n",
      "49 - 36%\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the location of the Verizon Center in 20173?\n",
      "AI: 49 - 36%\n",
      "Human:  What was the attendance at the Verizon Center when the record was 49 - 36%?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "location_attendance, record\n",
      "SELECT location_attendance\n",
      "FROM DF\n",
      "WHERE record = '49 - 36';\n",
      "verizon center 20173\n",
      "3373\n",
      " What was the score of Carlton when they played Richmond at Prince Park?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the score of Carlton when they played Richmond at Prince Park?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "home_team, home_team_score, away_team, away_team_score, venue\n",
      "SELECT home_team, home_team_score, away_team, away_team_score, venue\n",
      "FROM DF\n",
      "WHERE (home_team = 'carlton' AND away_team = 'richmond' AND venue = 'prince park') OR (home_team = 'richmond' AND away_team = 'carlton' AND venue = 'prince park');\n",
      "carlton score 20.7 when they play richmond at prince park\n",
      "1840\n",
      " How many players named Dawkins have played for the Jazz?; How many guards named Dawkins have played for the Jazz?; Did the two Dawkins players' time on the team overlap?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many players named Dawkins have played for the Jazz?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "player, nationality, position, years_for_jazz, school_club_team\n",
      "SELECT COUNT(*) \n",
      "FROM DF \n",
      "WHERE player LIKE '%dawkins%';\n",
      "the jazz have 2 player , both guard , with the last name dawkins but their time on the team do not overlap\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many players named Dawkins have played for the Jazz?\n",
      "AI: the jazz have 2 player , both guard , with the last name dawkins but their time on the team do not overlap\n",
      "Human:  How many guards named Dawkins have played for the Jazz?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  How many players named Dawkins have played for the Jazz?\n",
      "AI: the jazz have 2 player , both guard , with the last name dawkins but their time on the team do not overlap\n",
      "Human:  How many guards named Dawkins have played for the Jazz?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  How many players named Dawkins have played for the Jazz?\n",
      "AI: the jazz have 2 player , both guard , with the last name dawkins but their time on the team do not overlap\n",
      "Human:  How many guards named Dawkins have played for the Jazz?\n",
      "Human:  Did the two Dawkins players' time on the team overlap?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  How many players named Dawkins have played for the Jazz?\n",
      "AI: the jazz have 2 player , both guard , with the last name dawkins but their time on the team do not overlap\n",
      "Human:  How many guards named Dawkins have played for the Jazz?\n",
      "Human:  Did the two Dawkins players' time on the team overlap?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "2153\n",
      " What is the name of the team that has been called the Coquitlam Adanacs throughout all 45 seasons since 1965?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the name of the team that has been called the Coquitlam Adanacs throughout all 45 seasons since 1965?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "team_name\n",
      "SELECT team_name\n",
      "FROM DF\n",
      "WHERE team_name = \"coquitlam adanacs\"\n",
      "GROUP BY team_name\n",
      "HAVING COUNT(*) = 45;\n",
      "Yes, the team has been called the Coquitlam Adanacs throughout all 45 seasons since 1965.\n",
      "1489\n",
      " In which stage was a mountain classification not awarded?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  In which stage was a mountain classification not awarded?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  In which stage was a mountain classification not awarded?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0\n",
      "132\n",
      " What is the score of port autonome versus sport clube da praia?; What is the score of lprc oiler versus mighty blackpool?; Which match had a higher score, port autonome versus sport clube da praia or lprc oiler versus mighty blackpool?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the score of port autonome versus sport clube da praia?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "team_1, agg, team_2, c_1st_leg, c_2nd_leg\n",
      "SELECT c_1st_leg, c_2nd_leg\n",
      "FROM DF\n",
      "WHERE team_1 = 'port autonome' AND team_2 = 'sporting clube da praia';\n",
      "Yes, port autonome versus sport clube da praia have higher score than of lprc oiler versus mighty blackpool.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the score of port autonome versus sport clube da praia?\n",
      "AI: Yes, port autonome versus sport clube da praia have higher score than of lprc oiler versus mighty blackpool.\n",
      "Human:  What is the score of lprc oiler versus mighty blackpool?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "team_1, agg, team_2, c_1st_leg, c_2nd_leg\n",
      "SELECT c_1st_leg, c_2nd_leg\n",
      "FROM DF\n",
      "WHERE team_1 = 'lprc oiler' AND team_2 = 'mighty blackpool';\n",
      "False\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the score of port autonome versus sport clube da praia?\n",
      "AI: Yes, port autonome versus sport clube da praia have higher score than of lprc oiler versus mighty blackpool.\n",
      "Human:  What is the score of lprc oiler versus mighty blackpool?\n",
      "AI: False\n",
      "Human:  Which match had a higher score, port autonome versus sport clube da praia or lprc oiler versus mighty blackpool?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What is the score of port autonome versus sport clube da praia?\n",
      "AI: Yes, port autonome versus sport clube da praia have higher score than of lprc oiler versus mighty blackpool.\n",
      "Human:  What is the score of lprc oiler versus mighty blackpool?\n",
      "AI: False\n",
      "Human:  Which match had a higher score, port autonome versus sport clube da praia or lprc oiler versus mighty blackpool?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "3862\n",
      " What was the score when the athletics record was 53 - 32 against the colorado rockies?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the score when the athletics record was 53 - 32 against the colorado rockies?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "date, opponent, score, record\n",
      "SELECT score\n",
      "FROM DF\n",
      "WHERE opponent = 'athletics'\n",
      "AND record = '53 - 32';\n",
      "The score was 4 - 5.\n",
      "1630\n",
      " Was Glenn Capriola not selected before round 6?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Was Glenn Capriola not selected before round 6?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  Was Glenn Capriola not selected before round 6?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "136\n",
      " What was the crowd attendance for the New York Jets in their second game against the Miami Dolphins in the 1993 season?; What was the crowd attendance for the New York Jets in their first game against the Miami Dolphins in the 1993 season?; What is the difference in crowd attendance between the two games?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the crowd attendance for the New York Jets in their second game against the Miami Dolphins in the 1993 season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "week, date, opponent, result, game site, attendance\n",
      "SELECT attendance\n",
      "FROM DF\n",
      "WHERE opponent = 'miami dolphins' AND week = 2;\n",
      "The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the crowd attendance for the New York Jets in their second game against the Miami Dolphins in the 1993 season?\n",
      "AI: The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "Human:  What was the crowd attendance for the New York Jets in their first game against the Miami Dolphins in the 1993 season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "week, date, opponent, result, game site, attendance\n",
      "SELECT attendance\n",
      "FROM DF\n",
      "WHERE opponent = 'miami dolphins' AND week = 2;\n",
      "The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the crowd attendance for the New York Jets in their second game against the Miami Dolphins in the 1993 season?\n",
      "AI: The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "Human:  What was the crowd attendance for the New York Jets in their first game against the Miami Dolphins in the 1993 season?\n",
      "AI: The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "Human:  What is the difference in crowd attendance between the two games?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What was the crowd attendance for the New York Jets in their second game against the Miami Dolphins in the 1993 season?\n",
      "AI: The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "Human:  What was the crowd attendance for the New York Jets in their first game against the Miami Dolphins in the 1993 season?\n",
      "AI: The New York Jets had 992 more crowd attendance in their second game against the Miami Dolphins than from their first game played against them during the 1993 season.\n",
      "Human:  What is the difference in crowd attendance between the two games?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "4070\n",
      " What is the visitor score for Detroit this season?; Is Detroit ranked as having one of the lowest visitor scores this season?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the visitor score for Detroit this season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "visitor, score\n",
      "SELECT score\n",
      "FROM DF\n",
      "WHERE visitor = 'detroit';\n",
      "detroit have 1 of the lowest visitor score this season\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What is the visitor score for Detroit this season?\n",
      "AI: detroit have 1 of the lowest visitor score this season\n",
      "Human:  Is Detroit ranked as having one of the lowest visitor scores this season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer whether the provided claim/query are true or false. Return 0 if it's false, or 1 if it's true. Only return 0 or 1 without any other information.\n",
      "Human:  What is the visitor score for Detroit this season?\n",
      "AI: detroit have 1 of the lowest visitor score this season\n",
      "Human:  Is Detroit ranked as having one of the lowest visitor scores this season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "1809\n",
      " Did Stephen Jackson lead the team in points for the entire game?; If not, who led the team in points for the other half of the game?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Did Stephen Jackson lead the team in points for the entire game?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "game, date, team, score, high_points\n",
      "SELECT high_points\n",
      "FROM DF\n",
      "WHERE high_points LIKE '%stephen jackson%';\n",
      "stephen jackson lead the team in point for less than half the game\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  Did Stephen Jackson lead the team in points for the entire game?\n",
      "AI: stephen jackson lead the team in point for less than half the game\n",
      "Human:  If not, who led the team in points for the other half of the game?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "team, high_points, high_rebounds, high_assists\n",
      "SELECT team, high_points, high_rebounds, high_assists\n",
      "FROM DF\n",
      "WHERE team = 'la clippers' AND high_points != 'gerald wallace (32)'\n",
      "UNION\n",
      "SELECT team, high_points, high_rebounds, high_assists\n",
      "FROM DF\n",
      "WHERE team = 'la lakers' AND high_points != 'stephen jackson (30)'\n",
      "UNION\n",
      "SELECT team, high_points, high_rebounds, high_assists\n",
      "FROM DF\n",
      "WHERE team = 'new jersey' AND high_points != 'gerald wallace (21)';\n",
      "False\n",
      "4047\n",
      " What was the record of the Milwaukee Brewers in the 2005 season?; Did the Milwaukee Brewers play against the Padres in the 2005 season?; Did the Milwaukee Brewers win against the Padres in the 2005 season?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
      "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\n",
      "Human:  What was the record of the Milwaukee Brewers in the 2005 season?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "record\n",
      "SELECT record\n",
      "FROM DF\n",
      "WHERE season = 2005\n",
      "AND team = 'Milwaukee Brewers';\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such column: season\n[SQL: SELECT record\nFROM DF\nWHERE season = 2005\nAND team = 'Milwaukee Brewers';]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1965\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/default.py:921\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 921\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: season",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m choice \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     26\u001b[0m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent_history\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     29\u001b[0m )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m choice:\n\u001b[0;32m---> 31\u001b[0m     Agent_history\u001b[38;5;241m.\u001b[39madd_ai_message(\u001b[43mscene_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     res \u001b[38;5;241m=\u001b[39m scene_B()\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36mscene_A\u001b[0;34m(query, sample)\u001b[0m\n\u001b[1;32m     27\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mmodel, prompt\u001b[38;5;241m=\u001b[39manswer_instruction, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m formatter\u001b[38;5;241m.\u001b[39mnormalize_schema(schema_information\u001b[38;5;241m.\u001b[39mloc[sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m formatter\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_2_batch_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m stage_3_batch_pred \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mbatch([\u001b[38;5;28mdict\u001b[39m({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m: formatter\u001b[38;5;241m.\u001b[39mformat_html(table_caption\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m'\u001b[39m: sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     32\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSQL\u001b[39m\u001b[38;5;124m'\u001b[39m:  stage_2_batch_pred,\n\u001b[1;32m     33\u001b[0m                 })])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(stage_3_batch_pred)\n",
      "File \u001b[0;32m~/zh/tabular_data/executor/executor.py:80\u001b[0m, in \u001b[0;36mSQLManager.execute_from_df\u001b[0;34m(self, command, data, table_name)\u001b[0m\n\u001b[1;32m     78\u001b[0m db_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_col_name(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     79\u001b[0m db_data\u001b[38;5;241m.\u001b[39mto_sql(table_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 80\u001b[0m subtable \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subtable\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/pandas/io/sql.py:682\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    673\u001b[0m         sql,\n\u001b[1;32m    674\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    680\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/pandas/io/sql.py:1776\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1721\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1729\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/pandas/io/sql.py:1599\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   1597\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1774\u001b[0m, in \u001b[0;36mConnection.exec_driver_sql\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1769\u001b[0m execution_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_options\u001b[38;5;241m.\u001b[39mmerge_with(\n\u001b[1;32m   1770\u001b[0m     execution_options\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[0;32m-> 1774\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1844\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(\n\u001b[1;32m   1840\u001b[0m         dialect,\n\u001b[1;32m   1841\u001b[0m         context,\n\u001b[1;32m   1842\u001b[0m     )\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1984\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1984\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2339\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1965\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1971\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1972\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1976\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1977\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/sqlboy/lib/python3.10/site-packages/sqlalchemy/engine/default.py:921\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 921\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such column: season\n[SQL: SELECT record\nFROM DF\nWHERE season = 2005\nAND team = 'Milwaukee Brewers';]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "result = [ ]\n",
    "for i in range(len(table_loader.dataset)):\n",
    "    sample = table_loader.normalize_table(table_loader.dataset[i])\n",
    "    llm_chain = LLMChain(llm=model, prompt=stage_0_prompt, verbose=False)\n",
    "    stage_0_batch_pred = llm_chain.batch([{\"query\": sample['query']}], return_only_outputs=True)[0]['text'].split(':')[-1]\n",
    "    print(stage_0_batch_pred)\n",
    "    sub_queries = stage_0_batch_pred.split(';')\n",
    "\n",
    "    from langchain_community.callbacks import get_openai_callback\n",
    "    Agent_history = ChatMessageHistory()\n",
    "    agent_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"Based on the history information, your task is to determine whether the information is enough to answer the user query.\n",
    "                return 'A' if you think you need more information, else return 'B'. You should only use information during the conversation.\"\"\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = LLMChain(llm=model, prompt=agent_prompt, verbose=True)\n",
    "    with get_openai_callback() as cb:\n",
    "        for sub_query in sub_queries:\n",
    "            Agent_history.add_user_message(sub_query)\n",
    "            choice = chain.invoke(\n",
    "            {\n",
    "                \"chat_history\": Agent_history.messages,\n",
    "            }\n",
    "            )['text']\n",
    "            if 'A' in choice:\n",
    "                Agent_history.add_ai_message(scene_A(sub_query, sample))\n",
    "            else:\n",
    "                res = scene_B()\n",
    "                print(res)\n",
    "                result.append(res)\n",
    "                \n",
    "    print(cb.total_tokens)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlboy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
